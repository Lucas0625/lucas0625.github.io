<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    <title>深度学习基础知识 | Mr.chen</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="介绍深度学习中常用的一些概念">
<meta name="keywords" content="基础概念">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习基础知识">
<meta property="og:url" content="http://lucas0625.github.io/blog/2019/05/09/deeplearning-base/index.html">
<meta property="og:site_name" content="Mr.chen">
<meta property="og:description" content="介绍深度学习中常用的一些概念">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.15.46.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.16.50.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.17.21.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.20.02.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.20.18.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_4_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_8_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_16_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_18_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_22_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_25_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_30_1-1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_32_1.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.16.14.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.21.35.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.22.35.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.23.13.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-06-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%20%E4%B8%8A%E5%8D%8811.19.26.png">
<meta property="og:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-06-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%20%E4%B8%8A%E5%8D%8811.26.22.png">
<meta property="og:updated_time" content="2019-06-10T07:59:10.799Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习基础知识">
<meta name="twitter:description" content="介绍深度学习中常用的一些概念">
<meta name="twitter:image" content="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.15.46.png">
    

    

    

    <link rel="stylesheet" href="/lucas0625.github.io/libs/font-awesome5/css/fontawesome.min.css">
    <link rel="stylesheet" href="/lucas0625.github.io/libs/font-awesome5/css/fa-brands.min.css">
    <link rel="stylesheet" href="/lucas0625.github.io/libs/font-awesome5/css/fa-solid.min.css">
    <link rel="stylesheet" href="/lucas0625.github.io/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/lucas0625.github.io/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/lucas0625.github.io/css/style.css">

    <script src="/lucas0625.github.io/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/lucas0625.github.io/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/lucas0625.github.io/libs/justified-gallery/justifiedGallery.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
    
    


</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/lucas0625.github.io/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Mr.chen</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/lucas0625.github.io/.">Home</a>
                
                    <a class="main-nav-link" href="/lucas0625.github.io/archives">Archives</a>
                
                    <a class="main-nav-link" href="/lucas0625.github.io/categories">Categories</a>
                
                    <a class="main-nav-link" href="/lucas0625.github.io/tags">Tags</a>
                
                    <a class="main-nav-link" href="/lucas0625.github.io/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/lucas0625.github.io/css/images/avatar.png" />
                            <i class="fas fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fas fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/lucas0625.github.io/',
        CONTENT_URL: '/lucas0625.github.io/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/lucas0625.github.io/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/lucas0625.github.io/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/lucas0625.github.io/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/lucas0625.github.io/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/lucas0625.github.io/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/lucas0625.github.io/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile" class="">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/lucas0625.github.io/css/images/avatar.png" />
            <h2 id="name">琛</h2>
            <h3 id="title">学习记录</h3>
            <span id="location"><i class="fas fa-map-marker-alt" style="padding-right: 5px"></i>Beijing, China</span>
            <a id="follow" target="_blank" href="https://github.com/Lucas0625/">关注我</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                40
                <span>文章</span>
            </div>
            <div class="article-info-block">
                13
                <span>标签</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="https://github.com/Lucas0625" target="_blank" title="github" class=tooltip>
                            <i class="fab fa-github"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main"><article id="post-deeplearning-base" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
            深度学习基础知识
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/lucas0625.github.io/2019/05/09/deeplearning-base/">
            <time datetime="2019-05-09T05:52:55.739Z" itemprop="datePublished">2019-05-09</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/lucas0625.github.io/categories/深度学习/">深度学习</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/lucas0625.github.io/tags/基础概念/">基础概念</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">文章目录</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降"><span class="toc-number">1.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降-1"><span class="toc-number">1.1.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降法"><span class="toc-number">1.2.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降代码-PyTorch"><span class="toc-number">1.3.</span> <span class="toc-text">梯度下降代码(PyTorch)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降在线性模型中的应用"><span class="toc-number">1.3.1.</span> <span class="toc-text">梯度下降在线性模型中的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降在多项式模型中的应用"><span class="toc-number">1.3.2.</span> <span class="toc-text">梯度下降在多项式模型中的应用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#激活函数"><span class="toc-number">2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义"><span class="toc-number">2.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常见的激活函数"><span class="toc-number">2.2.</span> <span class="toc-text">常见的激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid-函数"><span class="toc-number">2.2.1.</span> <span class="toc-text">sigmoid 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tanh-函数"><span class="toc-number">2.2.2.</span> <span class="toc-text">tanh 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU-函数"><span class="toc-number">2.2.3.</span> <span class="toc-text">ReLU 函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#激活函数的选择"><span class="toc-number">2.3.</span> <span class="toc-text">激活函数的选择</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#反向传播算法"><span class="toc-number">3.</span> <span class="toc-text">反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义-1"><span class="toc-number">3.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#求解过程"><span class="toc-number">3.2.</span> <span class="toc-text">求解过程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#优化算法"><span class="toc-number">4.</span> <span class="toc-text">优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义-2"><span class="toc-number">4.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常用的优化算法"><span class="toc-number">4.2.</span> <span class="toc-text">常用的优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD-随机梯度下降"><span class="toc-number">4.2.1.</span> <span class="toc-text">SGD 随机梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-3"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实现"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum-基于动量的梯度下降"><span class="toc-number">4.2.2.</span> <span class="toc-text">Momentum 基于动量的梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-4"><span class="toc-number">4.2.2.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实现-1"><span class="toc-number">4.2.2.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adagrad-自适应梯度下降"><span class="toc-number">4.2.3.</span> <span class="toc-text">Adagrad 自适应梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-5"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实现-2"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSProp-RMSprop方法"><span class="toc-number">4.2.4.</span> <span class="toc-text">RMSProp RMSprop方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-6"><span class="toc-number">4.2.4.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实现-3"><span class="toc-number">4.2.4.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adadelta-Adaedlta方法"><span class="toc-number">4.2.5.</span> <span class="toc-text">Adadelta Adaedlta方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-7"><span class="toc-number">4.2.5.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实现-4"><span class="toc-number">4.2.5.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam-Adam方法"><span class="toc-number">4.2.6.</span> <span class="toc-text">Adam Adam方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-8"><span class="toc-number">4.2.6.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实现-5"><span class="toc-number">4.2.6.2.</span> <span class="toc-text">实现</span></a></li></ol></li></ol></li></ol></li></ol>
                </div>
            
            <p>介绍深度学习中常用的一些概念</p>
<a id="more"></a>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><h2 id="梯度下降-1"><a href="#梯度下降-1" class="headerlink" title="梯度下降"></a>梯度下降</h2><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.15.46.png" alt=""><br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.16.50.png" alt=""><br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.17.21.png" alt=""></p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.20.02.png" alt=""><br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-09%20%E4%B8%8B%E5%8D%882.20.18.png" alt=""></p>
<h2 id="梯度下降代码-PyTorch"><a href="#梯度下降代码-PyTorch" class="headerlink" title="梯度下降代码(PyTorch)"></a>梯度下降代码(PyTorch)</h2><h3 id="梯度下降在线性模型中的应用"><a href="#梯度下降在线性模型中的应用" class="headerlink" title="梯度下降在线性模型中的应用"></a>梯度下降在线性模型中的应用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">2019</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;torch._C.Generator at 0x10c9b39f0&gt;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取 x 和 y</span></span><br><span class="line">x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>],</span><br><span class="line">                    [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>], </span><br><span class="line">                    [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>],</span><br><span class="line">                    [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>], </span><br><span class="line">                    [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画出图像</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.plot(x_train, y_train, <span class="string">'bo'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x11ca8cfd0&gt;]
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_4_1-1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 转换成 Tensor</span></span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义参数 w 和 b</span></span><br><span class="line">w = Variable(torch.randn(<span class="number">1</span>), requires_grad=<span class="keyword">True</span>) <span class="comment"># 随机初始化 w</span></span><br><span class="line">b = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="keyword">True</span>) <span class="comment"># 使用 0 初始化 b</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建线性回归模型</span></span><br><span class="line">x_train = Variable(x_train)</span><br><span class="line">y_train = Variable(y_train)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">liner_model</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_ = liner_model(x_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在更新参数之前，模型输出结果</span></span><br><span class="line">plt.plot(x_train.data.numpy(), y_train.data.numpy(), <span class="string">'bo'</span>, label=<span class="string">'real'</span>)</span><br><span class="line">plt.plot(x_train.data.numpy(), y_.data.numpy(), <span class="string">'ro'</span>, label=<span class="string">'estimated'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11cc6beb8&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_8_1-1.png" alt=""></p>
<p>计算误差函数：</p>
<script type="math/tex; mode=display">\frac{1}{n} \sum_{i=1}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算误差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(y_, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mean((y_-y) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">loss = get_loss(y_, y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看 loss 的大小</span></span><br><span class="line">loss</span><br></pre></td></tr></table></figure>
<pre><code>tensor(10.2335, grad_fn=&lt;MeanBackward1&gt;)
</code></pre><p>计算 w 和 b 的梯度，采用PyTorch的自动求导，不需要手动取计算梯度。w 和 b 的梯度分别是：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial w}=\frac{2}{n} \sum_{i=1}^{n} x_{i}\left(w x_{i}+b-y_{i}\right)</script><script type="math/tex; mode=display">\frac{\partial}{\partial b}=\frac{2}{n} \sum_{i=1}^{n}\left(w x_{i}+b-y_{i}\right)</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自动求导</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看 w 和 b 的梯度</span></span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([-41.1289])
tensor([-6.0890])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新一次参数</span></span><br><span class="line">w.data = w.data - <span class="number">1e-2</span> * w.grad.data</span><br><span class="line">b.data = b.data - <span class="number">1e-2</span> * b.grad.data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新一次参数后，模型输出结果</span></span><br><span class="line">y_ = liner_model(x_train)</span><br><span class="line">plt.plot(x_train.data.numpy(), y_train.data.numpy(), <span class="string">'bo'</span>, label=<span class="string">'real'</span>)</span><br><span class="line">plt.plot(x_train.data.numpy(), y_.data.numpy(), <span class="string">'ro'</span>, label=<span class="string">'estimated'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11cbd84e0&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_16_1-1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多次更新</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    y_ = liner_model(x_train)</span><br><span class="line">    loss = get_loss(y_, y_train)</span><br><span class="line">    </span><br><span class="line">    w.grad.zero_() <span class="comment"># 梯度归零</span></span><br><span class="line">    b.grad.zero_() <span class="comment"># 梯度归零</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    w.data = w.data - <span class="number">1e-2</span> * w.grad.data <span class="comment"># 更新 w</span></span><br><span class="line">    b.data = b.data - <span class="number">1e-2</span> * b.grad.data <span class="comment"># 更新 b</span></span><br><span class="line">    print(<span class="string">'epoch: &#123;&#125;, loss: &#123;&#125;'</span>.format(e, loss.data.item()))</span><br></pre></td></tr></table></figure>
<pre><code>epoch: 0, loss: 0.23505009710788727
epoch: 1, loss: 0.23023782670497894
epoch: 2, loss: 0.2298405021429062
epoch: 3, loss: 0.22952641546726227
epoch: 4, loss: 0.22921547293663025
epoch: 5, loss: 0.22890615463256836
epoch: 6, loss: 0.22859837114810944
epoch: 7, loss: 0.22829222679138184
epoch: 8, loss: 0.227987602353096
epoch: 9, loss: 0.2276846319437027
epoch: 10, loss: 0.2273831069469452
epoch: 11, loss: 0.22708319127559662
epoch: 12, loss: 0.22678478062152863
epoch: 13, loss: 0.2264879196882248
epoch: 14, loss: 0.2261926382780075
epoch: 15, loss: 0.22589880228042603
epoch: 16, loss: 0.22560644149780273
epoch: 17, loss: 0.225315660238266
epoch: 18, loss: 0.22502633929252625
epoch: 19, loss: 0.2247384935617447
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数更新20次之后的结果</span></span><br><span class="line">y_ = liner_model(x_train)</span><br><span class="line">plt.plot(x_train.data.numpy(), y_train.data.numpy(), <span class="string">'bo'</span>, label=<span class="string">'real'</span>)</span><br><span class="line">plt.plot(x_train.data.numpy(), y_.data.numpy(), <span class="string">'ro'</span>, label=<span class="string">'estimated'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11cb5fc18&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_18_1-1.png" alt=""></p>
<p>经过 20 次更新， 红色的预测结果已经比较好的拟合了蓝色的真实值</p>
<h3 id="梯度下降在多项式模型中的应用"><a href="#梯度下降在多项式模型中的应用" class="headerlink" title="梯度下降在多项式模型中的应用"></a>梯度下降在多项式模型中的应用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个多变量函数</span></span><br><span class="line">w_target = np.array([<span class="number">0.5</span>, <span class="number">3</span>, <span class="number">2.4</span>]) <span class="comment"># 定义参数</span></span><br><span class="line">b_target = np.array([<span class="number">0.9</span>]) <span class="comment"># 定义参数</span></span><br><span class="line"></span><br><span class="line">f_des = <span class="string">'y = &#123;:.2f&#125; + &#123;:.2f&#125; * x + &#123;:.2f&#125; * x^2 + &#123;:.2f&#125; * x^3'</span>.format( </span><br><span class="line">                b_target[<span class="number">0</span>], w_target[<span class="number">0</span>], w_target[<span class="number">1</span>], w_target[<span class="number">2</span>]) <span class="comment"># 打印出函数的样式</span></span><br><span class="line">print(f_des)</span><br></pre></td></tr></table></figure>
<pre><code>y = 0.90 + 0.50 * x + 3.00 * x^2 + 2.40 * x^3
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先画出这个函数的图像</span></span><br><span class="line">x_sample = np.arange(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">y_sample = b_target[<span class="number">0</span>] + w_target[<span class="number">0</span>] * x_sample + w_target[<span class="number">1</span>] * x_sample **<span class="number">2</span> + w_target[<span class="number">2</span>] * x_sample ** <span class="number">3</span></span><br><span class="line"></span><br><span class="line">plt.plot(x_sample, y_sample, label=<span class="string">'real curve'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11c8255f8&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_22_1-1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建训练数据 x 和 y</span></span><br><span class="line"><span class="comment"># x 是一个如下矩阵 [x, x^2, x^3]</span></span><br><span class="line"><span class="comment"># y 是函数的结果 [y]</span></span><br><span class="line"></span><br><span class="line">x_train = np.stack([x_sample **i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">4</span>)], axis=<span class="number">1</span>)</span><br><span class="line">x_train = torch.from_numpy(x_train).float() <span class="comment"># 转换成 float tensor</span></span><br><span class="line"></span><br><span class="line">y_train = torch.from_numpy(y_sample).float().unsqueeze(<span class="number">1</span>) <span class="comment"># 转换为 列向量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义要优化的参数 w 和 b</span></span><br><span class="line">w = Variable(torch.randn(<span class="number">3</span>,<span class="number">1</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 x 和 y 转换成 Variabel</span></span><br><span class="line">x_train = Variable(x_train)</span><br><span class="line">y_train = Variable(y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义多元线性模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_model</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(x, w) + b <span class="comment"># torch.mm 矩阵相乘</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画出没有参数没有更新之前的模型与真实模型之间的对比</span></span><br><span class="line">y_pred = multi_model(x_train)</span><br><span class="line"></span><br><span class="line">plt.plot(x_train.data.numpy()[:, <span class="number">0</span>], y_pred.data.numpy(), color=<span class="string">'r'</span>, label=<span class="string">'fitting curve'</span>)</span><br><span class="line">plt.plot(x_train.data.numpy()[:, <span class="number">0</span>], y_sample, color=<span class="string">'b'</span>, label=<span class="string">'real curve'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11cf3f390&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_25_1-1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = get_loss(y_pred, y_train)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(142.2003, grad_fn=&lt;MeanBackward1&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自动求导</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看 w 和 b 的梯度</span></span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ -36.4598],
        [ -13.2381],
        [-236.6262]])
tensor([-3.9543])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新一次参数</span></span><br><span class="line">w.data = w.data - <span class="number">0.001</span> * w.grad.data</span><br><span class="line">b.data = b.data - <span class="number">0.001</span> * b.grad.data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画出更新一次参数之后模型</span></span><br><span class="line">y_pred = multi_model(x_train)</span><br><span class="line"></span><br><span class="line">plt.plot(x_train.data.numpy()[:, <span class="number">0</span>], y_pred.data.numpy(), color=<span class="string">'r'</span>, label=<span class="string">'fitting curve'</span>)</span><br><span class="line">plt.plot(x_train.data.numpy()[:, <span class="number">0</span>], y_sample, color=<span class="string">'b'</span>, label=<span class="string">'real curve'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11ccc1b70&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_30_1-1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新 100 次</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    y_pred = multi_model(x_train)</span><br><span class="line">    loss = get_loss(y_pred, y_train)</span><br><span class="line">    </span><br><span class="line">    w.grad.data.zero_()</span><br><span class="line">    b.grad.data.zero_()</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w.data = w.data - <span class="number">0.001</span> * w.grad.data</span><br><span class="line">    b.data = b.data - <span class="number">0.001</span> * b.grad.data</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'epoch &#123;&#125;, Loss:&#123;:.5f&#125;'</span>.format(e+<span class="number">1</span>, loss.data.item()))</span><br></pre></td></tr></table></figure>
<pre><code>epoch 20, Loss:2.84467
epoch 40, Loss:1.08338
epoch 60, Loss:0.61722
epoch 80, Loss:0.48439
epoch 100, Loss:0.43824
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新 100 次之后的模型结果</span></span><br><span class="line">y_pred = multi_model(x_train)</span><br><span class="line"></span><br><span class="line">plt.plot(x_train.data.numpy()[:, <span class="number">0</span>], y_pred.data.numpy(), label=<span class="string">'fitting curve'</span>, color=<span class="string">'r'</span>) </span><br><span class="line">plt.plot(x_train.data.numpy()[:, <span class="number">0</span>], y_sample, label=<span class="string">'real curve'</span>, color=<span class="string">'b'</span>) </span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.legend.Legend at 0x11cf335c0&gt;
</code></pre><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-09-output_32_1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输出此时的参数 w 和 b</span></span><br><span class="line">print(w.data)</span><br><span class="line">print(b.data)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1.1381],
        [3.1028],
        [2.3013]])
tensor([0.1931])
</code></pre><p>可以看到模型迭代100次之后已经非常接近原始数据了</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>类比人脑神经元，解释如下<br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.16.14.png" alt=""><br>激活函数在神经网络中非常重要，只有通过激活函数才会进入下一层继续传播，如果不使用激活函数，无论多少层神经网络，最后都会变成单层神经网络，所以每一层必须使用激活函数</p>
<h2 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h2><h3 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h3><script type="math/tex; mode=display">f(z)=\frac{1}{1+\exp (-z)}</script><p>导函数:</p>
<script type="math/tex; mode=display">f^{\prime}(z)=f(z)(1-f(z))</script><p>图像：<br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.21.35.png" alt=""></p>
<h3 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h3><script type="math/tex; mode=display">f(z)=\tanh (z)=\frac{\mathrm{e}^{z}-\mathrm{e}^{-z}}{\mathrm{e}^{z}+\mathrm{e}^{-z}}</script><p>导函数：</p>
<script type="math/tex; mode=display">f^{\prime}(z)=1-(f(z))^{2}</script><p>图像:<br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.22.35.png" alt=""></p>
<h3 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h3><script type="math/tex; mode=display">f(z)=\max (0, z)</script><p>导函数:</p>
<script type="math/tex; mode=display">f^{\prime}(z)=\left\{\begin{array}{l}{1, z>0} \\ {0, z \leqslant 0}\end{array}\right.</script><p>图像:<br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-05-13-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-05-13%20%E4%B8%8B%E5%8D%883.23.13.png" alt=""></p>
<h2 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h2><ol>
<li>现在的神经网络中，90%都使用<strong>ReLU</strong>激活函数，该激活函数能够加快梯度下降的收敛速度，对比其他激活函数计算更加简单。</li>
<li>Sigmoid和Tanh会导致梯度消失，当z很大时，sigmoid函数的导函数会=趋近于0，造成梯度消失，Tanh相当于Sigmoid函数的平移，原理类似。</li>
<li>RuLU的优势，以及局限性：<ul>
<li>优点：计算简单，避免梯度消失，单侧抑制提供了网络的稀疏性表达能力</li>
<li>局限性：在训练过程中会导致神经元死亡问题，这是由于函数负梯度在经过ReLU单元时被置为0，且之后也不被任何数据激活，即改神经元梯度永远为0，不对任何数据产生效应。在实习训练过程中，如果学习率设置较大，会导致一定比例的神经元不可逆死亡。导致参数梯度无法更新，整个训练过程失败。</li>
<li>优化：采用Leaky ReLU 表达式 <script type="math/tex">f(z)=\left\{\begin{array}{cl}{z,} & {z>0} \\ {a z,} & {z \leqslant 0}\end{array}\right.</script></li>
</ul>
</li>
</ol>
<h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>反向传播本质上是链式求导法则的一个应用。反向传播算法是一个优雅的局部过程，每次求导只是对当前的运算求导，求解每层网络的参数都是通过链式法则将前面的结果求出不断迭代到这一层，所以说是一个传播过程。</p>
<h2 id="求解过程"><a href="#求解过程" class="headerlink" title="求解过程"></a>求解过程</h2><ol>
<li><p>求解下图的反向传播过程<br><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-06-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%20%E4%B8%8A%E5%8D%8811.19.26.png" alt=""><br>表达式：</p>
<script type="math/tex; mode=display">\begin{array}{l}{q=x+y=-2+5=3} \\ {f=q z=3(-4)=-12}\end{array}</script><p>求解中间过程：</p>
<script type="math/tex; mode=display">\begin{array}{ll}{\frac{\partial f}{\partial q}=z=-4} & {\frac{\partial f}{\partial z}=q=3} \\ {\frac{\partial q}{\partial x}=1} & {\frac{\partial q}{\partial y}=1}\end{array}</script><p>求解梯度：</p>
<script type="math/tex; mode=display">\begin{aligned} \frac{\partial f}{\partial x} &=\frac{\partial f}{\partial q} \frac{\partial q}{\partial x}=z \times 1=-4 \\ \frac{\partial f}{\partial y} &=\frac{\partial f}{\partial q} \frac{\partial q}{\partial y}=z \times 1=-4 \end{aligned}</script></li>
<li><p>Sigmoid 函数求解反向传播<br>表达式：</p>
<script type="math/tex; mode=display">f(w, x)=\frac{1}{1+e^{-\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\right)}}</script><p>求解：</p>
<script type="math/tex; mode=display">\frac{\partial f}{\partial w_{0}}, \frac{\partial f}{\partial w_{1}}, \frac{\partial f}{\partial w_{2}}</script><p>构建计算图：</p>
<script type="math/tex; mode=display">\begin{array}{l}{f(x)=\frac{1}{x}} \\ {f_{c}(x)=1+x} \\ {f_{e}(x)=e^{x}} \\ {f_{w}(x)=-\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\right)}\end{array}</script><p><img src="http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-06-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-06-10%20%E4%B8%8A%E5%8D%8811.26.22.png" alt=""><br>求解过程：<br>上图中绿色数字表示数值，红色数字表示梯度。从后向前计算各个参数的梯度。</p>
<ol>
<li>最后面梯度为 1</li>
<li>经过 $\frac{1}{x}$ 这个函数，梯度是 $-\frac{1}{x^{2}}$ ， 所以往前传播的梯度是 $1 \times-\frac{1}{1.37^{2}}=-0.53$。</li>
<li>经过 +1 操作，梯度不变</li>
<li>经过 $e^{x}$, 梯度变为 $-0.53 \times e^{-1}=-0.2$。</li>
<li>经过 *-1， 梯度不变。</li>
<li>不断向后传播就能够求得每一个参数的梯度。</li>
</ol>
</li>
</ol>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h2><p>基于反向传播算法计算出梯度，接下来就会利用各种优化算法对参数进行优化。</p>
<h2 id="常用的优化算法"><a href="#常用的优化算法" class="headerlink" title="常用的优化算法"></a>常用的优化算法</h2><h3 id="SGD-随机梯度下降"><a href="#SGD-随机梯度下降" class="headerlink" title="SGD 随机梯度下降"></a>SGD 随机梯度下降</h3><h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\eta \nabla L(\theta)</script><p>$\eta$ 代表学习率，$\nabla L(\theta)$ 代表梯度</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_update</span><span class="params">(parameters, lr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> parameters:</span><br><span class="line">        param.data = param.data - lr * param.grad.data</span><br><span class="line">        </span><br><span class="line"><span class="comment"># PyTorch 实现</span></span><br><span class="line">optimzier = torch.optim.SGD(net.parameters(), <span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Momentum-基于动量的梯度下降"><a href="#Momentum-基于动量的梯度下降" class="headerlink" title="Momentum 基于动量的梯度下降"></a>Momentum 基于动量的梯度下降</h3><h4 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h4><script type="math/tex; mode=display">\begin{array}{c}{v_{i}=\gamma v_{i-1}+\eta \nabla L(\theta)} \\ {\theta_{i}=\theta_{i-1}-v_{i}}\end{array}</script><p>$v_{i}$ 是当前速度，$\gamma$ 是动量参数，是一个小于 1 的正数， $\eta$ 是学习率<br>相当于每次在进行参数更新的时候，都会将之前的速度考虑进来，每个参数在各方向上的移动幅度不仅取决于当前的梯度，还取决于过去各个梯度在各个方向上是否一致，如果一个梯度一致沿着当前方向进行更新，那么每次更新的幅度就越来越大，如果一个梯度在一个方向上不断变化，那么其更新幅度就会被衰减，这样我们就可以使用一个较大的学习率，使得收敛更快，同时梯度比较大的方向就会因为动量的关系每次更新的幅度减少。</p>
<h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(parameters, vs, lr, gamma)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param, v <span class="keyword">in</span> zip(parameters, vs):</span><br><span class="line">    v[:] = gamma * v + lr * param.grad.data </span><br><span class="line">    param.data = param.data - v</span><br><span class="line">    </span><br><span class="line"><span class="comment"># PyTorch 实现</span></span><br><span class="line"><span class="comment"># 加动量</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Adagrad-自适应梯度下降"><a href="#Adagrad-自适应梯度下降" class="headerlink" title="Adagrad 自适应梯度下降"></a>Adagrad 自适应梯度下降</h3><h4 id="定义-5"><a href="#定义-5" class="headerlink" title="定义"></a>定义</h4><p>学习率在更新过程中不断改变。学习率为：</p>
<script type="math/tex; mode=display">\frac{\eta}{\sqrt{s+\epsilon}}</script><p>每次使用一个 batch size的数据进行参数更新的时候，我们需要计算所有参数的梯度，对于每个参数，初始化一个变量 s 为 0，然后每次将该参数的梯度平方和累加到这个变量 s 上，然后再更新参数的时候，学习率变为上式的样子。<br>$\epsilon$ 是为了数值稳定性而加上的，因为有可能 s 为 0，那么 0 出现在分母就会出现无穷大的情况，通常取 $10^{-10}$ ,这样不同的参数由于梯度不同，它们对应的 s 大小也就不同，得到的学习率也就不同，实现了自适应的学习率。<br>Adagrad 的 核心想法是：如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡。而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够快速的更新。<br>缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。</p>
<h4 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_adagrad</span><span class="params">(parameters, sqrs, lr)</span>:</span></span><br><span class="line">    ‘’‘</span><br><span class="line">    sqrs: 梯度平方和</span><br><span class="line">    ’‘’</span><br><span class="line">    eps = <span class="number">1e-10</span> </span><br><span class="line">    <span class="keyword">for</span> param, sqr <span class="keyword">in</span> zip(parameters, sqrs):</span><br><span class="line">        sqr[:] = sqr + param.grad.data ** <span class="number">2</span> </span><br><span class="line">        div = lr / torch.sqrt(sqr + eps) * param.grad.data </span><br><span class="line">        param.data = param.data - div</span><br><span class="line">        </span><br><span class="line"><span class="comment"># PyTorch 实现</span></span><br><span class="line">optimizer = torch.optim.Adagrad(net.parameters(), lr=<span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="RMSProp-RMSprop方法"><a href="#RMSProp-RMSprop方法" class="headerlink" title="RMSProp RMSprop方法"></a>RMSProp RMSprop方法</h3><h4 id="定义-6"><a href="#定义-6" class="headerlink" title="定义"></a>定义</h4><p>RMSProp的提出，为了解决Adagrad训练后期，学习率过小的问的。<br>RMSProp 使用一个指数加权移动平均来计算 s，如下式：</p>
<script type="math/tex; mode=display">s_{i}=\alpha s_{i-1}+(1-\alpha) g^{2}</script><p>学习率更新公式：</p>
<script type="math/tex; mode=display">\frac{\eta}{\sqrt{s+\epsilon}}</script><p>$g$ 表示当前求出的参数梯度，$\alpha$ 是一个移动平均的系数，这个系数使用 RMSProp 更新到后期累加的梯度平方较小，从而保证 s 不会太大，也就是使得模型后期依然能找到比较优的结果。</p>
<h4 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(parameters, sqrs, lr, alpha)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-10</span> </span><br><span class="line">    <span class="keyword">for</span> param, sqr <span class="keyword">in</span> zip(parameters, sqrs):</span><br><span class="line">        sqr[:] = alpha * sqr + (<span class="number">1</span> - alpha) * param.grad.data ** <span class="number">2</span> </span><br><span class="line">        div = lr / torch.sqrt(sqr + eps) * param.grad.data </span><br><span class="line">        param.data = param.data - div</span><br><span class="line">        </span><br><span class="line"><span class="comment"># PyTorch 实现</span></span><br><span class="line">optimizer = torch.optim.RMSprop(net.parameters(), lr=<span class="number">1e-3</span>, alpha=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Adadelta-Adaedlta方法"><a href="#Adadelta-Adaedlta方法" class="headerlink" title="Adadelta Adaedlta方法"></a>Adadelta Adaedlta方法</h3><h4 id="定义-7"><a href="#定义-7" class="headerlink" title="定义"></a>定义</h4><p>也是为了解决Adagrad中学习率不断减小的问题。<br>先使用移动平均来计算 s</p>
<script type="math/tex; mode=display">s=\rho s+(1-\rho) g^{2}</script><p>$\rho$ 移动平均系数，$g$ 参数的梯度，<br>计算需要更新的参数的变化量：</p>
<script type="math/tex; mode=display">g^{\prime}=\frac{\sqrt{\Delta \theta+\epsilon}}{\sqrt{s+\epsilon}} g</script><p>$\Delta \theta$ 初始为 0 张量，每一步做如下指数加权移动平均更新：</p>
<script type="math/tex; mode=display">\Delta \theta=\rho \Delta \theta+(1-\rho) g^{\prime 2}</script><p>最后参数更新为：</p>
<script type="math/tex; mode=display">\theta=\theta-g^{\prime}</script><h4 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(parameters, sqrs, deltas, rho)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span> </span><br><span class="line">    <span class="keyword">for</span> param, sqr, delta <span class="keyword">in</span> zip(parameters, sqrs, deltas):</span><br><span class="line">        sqr[:] = rho * sqr + (<span class="number">1</span> - rho) * param.grad.data ** <span class="number">2</span> </span><br><span class="line">        cur_delta = torch.sqrt(delta + eps) / torch.sqrt(sqr + eps) * param.grad.data </span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * cur_delta ** <span class="number">2</span> </span><br><span class="line">        param.data = param.data - cur_delta</span><br><span class="line">        </span><br><span class="line"><span class="comment"># PyTorch 实现</span></span><br><span class="line">optimizer = torch.optim.Adadelta(net.parameters(), rho=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Adam-Adam方法"><a href="#Adam-Adam方法" class="headerlink" title="Adam Adam方法"></a>Adam Adam方法</h3><h4 id="定义-8"><a href="#定义-8" class="headerlink" title="定义"></a>定义</h4><p>Adam 是一个结合了动量法和RMSProp的优化算法，结合了两者的优点。<br>使用一个动量变量 v 和一个 RMSProp 中的梯度元素平方的移动指数加权平均 s，优先将他们全部初始化为 0，然后再每次迭代中，计算他们的移动加权平均进行更新。</p>
<script type="math/tex; mode=display">\begin{aligned} v &=\beta_{1} v+\left(1-\beta_{1}\right) g \\ s &=\beta_{2} s+\left(1-\beta_{2}\right) g^{2} \end{aligned}</script><p>为了减轻 v 和 s 被初始化为 0 的初期对计算指数加权移动平均的影响，每次 v 和 s 都做下面的修正。</p>
<script type="math/tex; mode=display">\begin{aligned} \hat{v} &=\frac{v}{1-\beta_{1}^{t}} \\ \hat{s} &=\frac{s}{1-\beta_{2}^{t}} \end{aligned}</script><p>$t$迭代次数，当$0\leq\beta<em>{1},\beta</em>{2}\leq1$时，迭代到后期$t$比较大，那么$\beta<em>{1}^{t}$和$\beta</em>{2}^{t}$就几乎为 0，不会对 v 和 s 有任何影响了，算法作者建议$\beta<em>{1}=0.9,\beta</em>{2}=0.999$。<br>使用修正之后的$\hat{v}$和$\hat{s}$进行学习率的重新计算。</p>
<script type="math/tex; mode=display">g^{\prime}=\frac{\eta\hat{v}}{\sqrt{\hat{s}+\epsilon}}</script><p>$\eta$是学习率， $\epsilon$是为了数值稳定而添加的常数。<br>更新参数：</p>
<script type="math/tex; mode=display">\theta_{i}=\theta_{i-1}-g^{\prime}</script><h4 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自定义实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(parameters, vs, sqrs, lr, t, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-8</span> </span><br><span class="line">    <span class="keyword">for</span> param, v, sqr <span class="keyword">in</span> zip(parameters, vs, sqrs):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * param.grad.data </span><br><span class="line">        sqr[:] = beta2 * sqr + (<span class="number">1</span> - beta2) * param.grad.data ** <span class="number">2</span> </span><br><span class="line">        v_hat = v / (<span class="number">1</span> - beta1 ** t) </span><br><span class="line">        s_hat = sqr / (<span class="number">1</span> - beta2 ** t) </span><br><span class="line">        param.data = param.data - lr * v_hat / torch.sqrt(s_hat + eps)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># PyTorch 实现</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="http://lucas0625.github.io/blog/2019/05/09/deeplearning-base/" data-id="cjxl716nv003qja2uplmu05hw" class="article-share-link"><i class="fas fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/lucas0625.github.io/2019/05/14/multilayer-neural-network/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">上一篇</strong>
            <div class="article-nav-title">
                
                    多层神经网络
                
            </div>
        </a>
    
    
        <a href="/lucas0625.github.io/2019/05/05/interviewCode/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">下一篇</strong>
            <div class="article-nav-title">算法面试编程试题集锦</div>
        </a>
    
</nav>


    
</article>


    
    

</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">最新文章</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/lucas0625.github.io/2019/06/14/email-category/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/lucas0625.github.io/categories/小型项目/">小型项目</a></p>
                            <p class="item-title"><a href="/lucas0625.github.io/2019/06/14/email-category/" class="title">垃圾邮件分类</a></p>
                            <p class="item-date"><time datetime="2019-06-14T06:41:26.939Z" itemprop="datePublished">2019-06-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/lucas0625.github.io/2019/05/31/classic-RNN-LSTM-GRU/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/lucas0625.github.io/categories/深度学习/">深度学习</a></p>
                            <p class="item-title"><a href="/lucas0625.github.io/2019/05/31/classic-RNN-LSTM-GRU/" class="title">基本循环神经网路-RNN，LSTM，GRU</a></p>
                            <p class="item-date"><time datetime="2019-05-31T08:32:59.359Z" itemprop="datePublished">2019-05-31</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/lucas0625.github.io/2019/05/31/RNN-base/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/lucas0625.github.io/categories/深度学习/">深度学习</a></p>
                            <p class="item-title"><a href="/lucas0625.github.io/2019/05/31/RNN-base/" class="title">循环神经网络基础概念</a></p>
                            <p class="item-date"><time datetime="2019-05-31T06:46:55.594Z" itemprop="datePublished">2019-05-31</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/lucas0625.github.io/2019/05/29/network-train/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/lucas0625.github.io/categories/深度学习/">深度学习</a></p>
                            <p class="item-title"><a href="/lucas0625.github.io/2019/05/29/network-train/" class="title">神经网络训练技巧</a></p>
                            <p class="item-date"><time datetime="2019-05-29T06:59:17.774Z" itemprop="datePublished">2019-05-29</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/lucas0625.github.io/2019/05/28/classic-DenseNet/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/lucas0625.github.io/categories/深度学习/">深度学习</a></p>
                            <p class="item-title"><a href="/lucas0625.github.io/2019/05/28/classic-DenseNet/" class="title">经典卷积神经网络-DenseNet</a></p>
                            <p class="item-date"><time datetime="2019-05-28T07:16:17.093Z" itemprop="datePublished">2019-05-28</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/其他/">其他</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/小型项目/">小型项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/工具/">工具</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/数据库/">数据库</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/求职面试/">求职面试</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/深度学习/">深度学习</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/爬虫/">爬虫</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/算法/">算法</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/lucas0625.github.io/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">10</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2019/06/">六月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2019/05/">五月 2019</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2019/04/">四月 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2019/02/">二月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2018/12/">十二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2018/10/">十月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/lucas0625.github.io/archives/2018/09/">九月 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">标签</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/Git/">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/PyTorch/">PyTorch</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/es/">es</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/re/">re</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/其他/">其他</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/基础/">基础</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/基础概念/">基础概念</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/基础知识/">基础知识</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/实战/">实战</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/技巧/">技巧</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/算法/">算法</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/经验/">经验</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/lucas0625.github.io/tags/面试/">面试</a><span class="tag-list-count">3</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/lucas0625.github.io/tags/Git/" style="font-size: 10px;">Git</a> <a href="/lucas0625.github.io/tags/PyTorch/" style="font-size: 20px;">PyTorch</a> <a href="/lucas0625.github.io/tags/es/" style="font-size: 10px;">es</a> <a href="/lucas0625.github.io/tags/re/" style="font-size: 10px;">re</a> <a href="/lucas0625.github.io/tags/其他/" style="font-size: 10px;">其他</a> <a href="/lucas0625.github.io/tags/基础/" style="font-size: 16.67px;">基础</a> <a href="/lucas0625.github.io/tags/基础概念/" style="font-size: 10px;">基础概念</a> <a href="/lucas0625.github.io/tags/基础知识/" style="font-size: 10px;">基础知识</a> <a href="/lucas0625.github.io/tags/实战/" style="font-size: 10px;">实战</a> <a href="/lucas0625.github.io/tags/技巧/" style="font-size: 10px;">技巧</a> <a href="/lucas0625.github.io/tags/算法/" style="font-size: 20px;">算法</a> <a href="/lucas0625.github.io/tags/经验/" style="font-size: 10px;">经验</a> <a href="/lucas0625.github.io/tags/面试/" style="font-size: 13.33px;">面试</a>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">链接</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fas fa-angle-up"></div>
</aside>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2019 琛<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
        


    
        <script src="/lucas0625.github.io/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/lucas0625.github.io/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/lucas0625.github.io/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/lucas0625.github.io/js/main.js"></script>

    </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>