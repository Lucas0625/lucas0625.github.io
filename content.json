{"meta":{"title":"Mr.wang","subtitle":null,"description":null,"author":"琛","url":"http://lucas0625.github.io"},"pages":[{"title":"About","date":"2019-02-04T08:28:25.234Z","updated":"2019-02-04T08:28:25.125Z","comments":true,"path":"about/index.html","permalink":"http://lucas0625.github.io/about/index.html","excerpt":"","text":"个人信息 姓名：Lucas 性别：男 电子邮箱：wangchen_jh@outlook.com Wechat：751009328 目前状态：格式化大脑中。。。"},{"title":"Categories","date":"2019-02-05T07:55:36.181Z","updated":"2019-02-04T08:28:25.234Z","comments":true,"path":"categories/index.html","permalink":"http://lucas0625.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-02-04T08:28:25.266Z","updated":"2019-02-04T08:28:25.246Z","comments":true,"path":"tags/index.html","permalink":"http://lucas0625.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Glove 原理与实现","slug":"Glove","date":"2019-04-25T12:15:37.175Z","updated":"2019-04-25T12:15:37.175Z","comments":true,"path":"2019/04/25/Glove/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/Glove/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"TextRNN 原理与实现","slug":"TextRNN","date":"2019-04-25T06:22:25.197Z","updated":"2019-04-25T06:22:25.197Z","comments":true,"path":"2019/04/25/TextRNN/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/TextRNN/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"FastText 原理与实现","slug":"FastText","date":"2019-04-25T02:30:15.713Z","updated":"2019-04-25T02:30:15.713Z","comments":true,"path":"2019/04/25/FastText/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/FastText/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"Word2Vec 原理与实现","slug":"Word2Vec","date":"2019-04-25T02:30:06.908Z","updated":"2019-04-25T02:30:06.909Z","comments":true,"path":"2019/04/25/Word2Vec/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/Word2Vec/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"BERT 原理与实现","slug":"BERT","date":"2019-04-25T02:29:55.578Z","updated":"2019-04-25T02:29:55.578Z","comments":true,"path":"2019/04/25/BERT/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/BERT/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"Transformer 原理与实现","slug":"Transformer","date":"2019-04-25T02:29:27.661Z","updated":"2019-04-25T02:29:27.661Z","comments":true,"path":"2019/04/25/Transformer/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/Transformer/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"Bi-LSTM 原理与实现","slug":"Bi-LSTM","date":"2019-04-25T02:28:40.370Z","updated":"2019-04-25T02:28:40.370Z","comments":true,"path":"2019/04/25/Bi-LSTM/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/Bi-LSTM/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"TextLSTM 原理与实现","slug":"TextLSTM","date":"2019-04-25T02:27:57.789Z","updated":"2019-04-25T02:27:57.790Z","comments":true,"path":"2019/04/25/TextLSTM/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/TextLSTM/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"TextCNN 原理与实现","slug":"TextCNN","date":"2019-04-25T02:27:21.661Z","updated":"2019-04-25T02:27:21.662Z","comments":true,"path":"2019/04/25/TextCNN/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/TextCNN/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"NNLM 原理与实现","slug":"NNLM","date":"2019-04-25T02:25:41.973Z","updated":"2019-04-25T02:25:41.973Z","comments":true,"path":"2019/04/25/NNLM/","link":"","permalink":"http://lucas0625.github.io/2019/04/25/NNLM/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"Git 常用操作指南","slug":"Git常用操作指南","date":"2019-04-24T09:34:03.140Z","updated":"2019-04-25T12:08:17.553Z","comments":true,"path":"2019/04/24/Git常用操作指南/","link":"","permalink":"http://lucas0625.github.io/2019/04/24/Git常用操作指南/","excerpt":"本文介绍自己平时工作中常用的一些Git操作","text":"本文介绍自己平时工作中常用的一些Git操作 Git 常用操作指南.gitignore规范 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。[注]:所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（*）匹配零个或多个任意字符 [abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c） 问号（?）只匹配一个任意字符 如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 example: 1234567891011# 此为注释 – 将被 Git 忽略 # 忽略所有 .a 结尾的文件 *.a # 但 lib.a 除外 !lib.a # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO /TODO # 忽略 build/ 目录下的所有文件 build/ # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt doc/*.txt 查看文件之间的差别 git diff file 查看工作目录中当前文件和暂存区域快照之间的差异 git diff –cached / –staged 查看已经暂存起来的文件和上次提交时的快照之间的差异 提交文件 git commit -m ‘up’ 将暂存区的文件提交 git commit -a -m ‘up’ 跳过add步骤，将工作区的文件直接提交 删除文件 git rm file 从暂存区删除，并连带从工作目录中删除指定的文件 git rm -f file 如何删除之前修改过，并且已加入暂存区，则必须强制删除 git rm –cached file 将文件从git仓库中删除，但是仍保留在工作目录中 修改文件名 git mv file_form file_to 查看提交历史 git log 按照提交历史列出所有更新，最近的更新排在最上面 git log -p -n 显示每次提交的内容差异，显示最近的n次提交 git log –stat 显示每次提交的简要增改行统计 git log –pretty=oneline 将每个提交放在一行显示 git log –pretty=format:”%h - %an, %ar : %s” 定制要显示的记录格式 12345678910111213141516选项 说明 %H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字. 实际作出修改的人 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 -date= 选项定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字. 最后将此工作成果提交到仓库的人 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期，按多久以前的方式显示 %s 提交说明 git log –pretty=format:”%h %s” –graph 增加ASCII字符串表示的简单图形，展示每个提交所在的分支及其分化衍合情况 其他选项 12345678910选项 说明 -p 按补丁格式显示每个更新之间的差异。 --stat 显示每次更新的文件修改统计信息。 --shortstat 只显示 --stat 中最后的行数修改添加移除统计。 --name-only 仅在提交信息后显示已修改的文件清单。 --name-status 显示新增、修改、删除的文件清单。 --abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 --relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 --graph 显示 ASCII 图形表示的分支合并历史。 --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 git log –since=2.weeks 显示最近两周的提交 123456选项 说明 -(n) 仅显示最近的 n 条提交 --since, --after 仅显示指定时间之后的提交。 --until, --before 仅显示指定时间之前的提交。 --author 仅显示指定作者相关的提交。 --committer 仅显示指定提交者相关的提交。 撤销操作 git commit –amend 撤销刚才的提交操作，使用当前的暂存区域快照提交，如果刚才提交完没有作任何改动，相当于有机会重新编辑提交说明 git reset HEAD file 取消已暂存的文件 git checkout – file 取消工作区对文件的修改!!!!!!!!!!(此过程不可逆) 管理远程仓库 git remote 列出远程仓库的名字 git remote -v 显示对应的克隆地址 git pull 拉取远程仓库的更新 git push 推送数据到远程仓库 分支操作 git branch testing 新建testing分支 git branch 显示当前分支 git branch -a 显示所有分支 git checkout testing 转换到testing分支 git checkout -b testing 新建testing分支，并转向testing分支 git branch -d testing 删除testing分支 git merge testing 合并testing分支到当前分支 12345合并方式包括简单的指针前进操作 fast forward 或者多方合并 recursive， 当合并有冲突时，任何包含未解决冲突的文件都会以unmerged状态存在。&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 当前分支============&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;要合并的分支解决完冲突之后 执行add, commit 提交到仓库 git branch -v 查看各个分支最后一个提交的对象的信息 git branch –merged 筛选出与当前分支合并的分支，一般来说列表中没有*的分支通常都可以删除 git branch –no-merged 筛选出尚未与当前分支合并的分支 git rebase master 把当前分支里提交的改变移动到master分支里 git rebase –onto master server client 取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它在 master 上重演一遍 git rebase master server 取出特性分支 server，然后在主分支 master 上重演","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/categories/工具/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://lucas0625.github.io/tags/Git/"}]},{"title":"2.3 Logistic 回归","slug":"Logistic回归","date":"2019-02-04T16:00:00.000Z","updated":"2019-02-05T08:07:07.502Z","comments":true,"path":"2019/02/05/Logistic回归/","link":"","permalink":"http://lucas0625.github.io/2019/02/05/Logistic回归/","excerpt":"本文介绍Pytorch的基本概念。","text":"本文介绍Pytorch的基本概念。 多层全连接神经网络2.3 Logistic回归12345import torchfrom torch.autograd import Variableimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline 12# 设定随机种子torch.manual_seed(2017) &lt;torch._C.Generator at 0x10d916550&gt; 123456789101112131415161718192021# 从 data.txt 中读入点with open('./Logistic_data.txt', 'r') as f: data_list = [i.split('\\n')[0].split(',') for i in f.readlines()] data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]# 标准化x0_max = max([i[0] for i in data])x1_max = max([i[1] for i in data])data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点plot_x0 = [i[0] for i in x0]plot_y0 = [i[1] for i in x0]plot_x1 = [i[0] for i in x1]plot_y1 = [i[1] for i in x1]plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x112a31f98&gt; 123np_data = np.array(data, dtype='float32') # 转换成 numpy arrayx_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1] 实现Sigmoid 的函数，Sigmoid 函数的公式为$$f(x) = \\frac{1}{1 + e^{-x}}$$ 123# 定义 sigmoid 函数def sigmoid(x): return 1 / (1 + np.exp(-x)) 12345# 画出 sigmoid 的图像plot_x = np.arange(-10, 10.01, 0.01)plot_y = sigmoid(plot_x)plt.plot(plot_x, plot_y, 'r') [&lt;matplotlib.lines.Line2D at 0x115df7fd0&gt;] 12x_data = Variable(x_data)y_data = Variable(y_data) 123456# 定义 logistic 回归模型w = Variable(torch.randn(2, 1), requires_grad=True) b = Variable(torch.zeros(1), requires_grad=True)def logistic_regression(x): return torch.sigmoid(torch.mm(x, w) + b) 123456789101112131415# 画出参数更新之前的结果w0 = w[0].data[0]w1 = w[1].data[0]b0 = b.data[0]plot_x = np.arange(0.2, 1, 0.01)plot_x = torch.from_numpy(plot_x) # 转换成Tensorplot_y = (-w0 * plot_x - b0) / w1plot_x = plot_x.numpy() # 转换成numpyplot_y = plot_y.numpy() # 转换成numpyplt.plot(plot_x, plot_y, 'g', label='cutting line')plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x11686cc18&gt; 可以看到分类效果基本是混乱的，我们来计算一下 loss，公式如下 $$loss = -(y log(\\hat{y}) + (1 - y) log(1 - \\hat{y}))$$ 1234# 计算lossdef binary_loss(y_pred, y): logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean() return -logits 123y_pred = logistic_regression(x_data)loss = binary_loss(y_pred, y_data)print(loss) tensor(0.7911, grad_fn=&lt;NegBackward&gt;) 123456789# 自动求导并更新参数loss.backward()w.data = w.data - 0.1 * w.grad.datab.data = b.data - 0.1 * b.grad.data# 算出一次更新之后的lossy_pred = logistic_regression(x_data)loss = binary_loss(y_pred, y_data)print(loss) tensor(0.7801, grad_fn=&lt;NegBackward&gt;) 123456# 使用 torch.optim 更新参数from torch import nnw = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))optimizer = torch.optim.SGD([w, b], lr=1.) 1234567891011121314151617181920# 进行 1000 次更新import timestart = time.time()for e in range(1000): # 前向传播 y_pred = logistic_regression(x_data) loss = binary_loss(y_pred, y_data) # 计算 loss # 反向传播 optimizer.zero_grad() # 使用优化器将梯度归 0 loss.backward() optimizer.step() # 使用优化器来更新参数 # 计算正确率 mask = y_pred.ge(0.5).float() acc = (mask == y_data).sum().data.item() / y_data.shape[0] if (e + 1) % 200 == 0: print('epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;'.format(e+1, loss.data.item(), acc))during = time.time() - startprint()print('During Time: &#123;:.3f&#125; s'.format(during)) epoch: 200, Loss: 0.32431, Acc: 0.91000 epoch: 400, Loss: 0.29052, Acc: 0.91000 epoch: 600, Loss: 0.27069, Acc: 0.91000 epoch: 800, Loss: 0.25759, Acc: 0.90000 epoch: 1000, Loss: 0.24827, Acc: 0.89000 During Time: 0.304 s 123456789101112131415161718# 画出更新之后的结果w0 = w[0].data[0]w1 = w[1].data[0]b0 = b.data[0]plot_x = np.arange(0.2, 1, 0.01)plot_x = torch.from_numpy(plot_x) # 转换成Tensorplot_y = (-w0 * plot_x - b0) / w1plot_x = plot_x.numpy() # 转换成numpyplot_y = plot_y.numpy() # 转换成numpyplt.plot(plot_x, plot_y, 'g', label='cutting line')plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x116526ef0&gt; 前面我们使用了自己写的 loss，其实 PyTorch 已经为我们写好了一些常见的 loss，比如线性回归里面的 loss 是 nn.MSE()，而 Logistic 回归的二分类 loss 在 PyTorch 中是 nn.BCEWithLogitsLoss()，关于更多的 loss，可以查看文档 PyTorch 为我们实现的 loss 函数有两个好处，第一是方便我们使用，不需要重复造轮子，第二就是其实现是在底层 C++ 语言上的，所以速度上和稳定性上都要比我们自己实现的要好 另外，PyTorch 出于稳定性考虑，将模型的 Sigmoid 操作和最后的 loss 都合在了 nn.BCEWithLogitsLoss()，所以我们使用 PyTorch 自带的 loss 就不需要再加上 Sigmoid 操作了 12345678910# 使用自带的losscriterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性w = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))def logistic_reg(x): return torch.mm(x, w) + boptimizer = torch.optim.SGD([w, b], 1.) 123y_pred = logistic_reg(x_data)loss = criterion(y_pred, y_data)print(loss.data) tensor(0.6650) 1234567891011121314151617181920# 同样进行 1000 次更新start = time.time()for e in range(1000): # 前向传播 y_pred = logistic_reg(x_data) loss = criterion(y_pred, y_data) # 反向传播 optimizer.zero_grad() loss.backward() optimizer.step() # 计算正确率 mask = y_pred.ge(0.5).float() acc = (mask == y_data).sum().data.item() / y_data.shape[0] if (e + 1) % 200 == 0: print('epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;'.format(e+1, loss.data.item(), acc))during = time.time() - startprint()print('During Time: &#123;:.3f&#125; s'.format(during)) epoch: 200, Loss: 0.39010, Acc: 0.87000 epoch: 400, Loss: 0.32184, Acc: 0.87000 epoch: 600, Loss: 0.28917, Acc: 0.87000 epoch: 800, Loss: 0.26983, Acc: 0.87000 epoch: 1000, Loss: 0.25700, Acc: 0.88000 During Time: 0.202 s 可以看到，使用了 PyTorch 自带的 loss 之后，速度有了一定的上升，虽然看上去速度的提升并不多，但是这只是一个小网络，对于大网络，使用自带的 loss 不管对于稳定性还是速度而言，都有质的飞跃，同时也避免了重复造轮子的困扰","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/tags/Pytorch/"}]},{"title":"1. Pytorch 基本概念","slug":"基本概念","date":"2019-02-04T16:00:00.000Z","updated":"2019-02-05T08:02:13.708Z","comments":true,"path":"2019/02/05/基本概念/","link":"","permalink":"http://lucas0625.github.io/2019/02/05/基本概念/","excerpt":"本文介绍Pytorch的基本概念。","text":"本文介绍Pytorch的基本概念。 Pytorch 基本概念123456import torchimport numpy as npimport pandas as pdfrom torch import nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoader 1torch.cuda.is_available() False Tensor 张量表示的是一个多维矩阵，零维就是一个点，一维就是向量，二维就是一般的矩阵，多维就相当于一个多维矩阵，Tensor可以和numpy的ndarray相互对应，Tensor可以和ndarray相互转换 123a = torch.Tensor([[2, 3], [4, 8], [7, 9]])print('a is : &#123;&#125;'.format(a))print('a size is &#123;&#125;'.format(a.size())) a is : tensor([[2., 3.], [4., 8.], [7., 9.]]) a size is torch.Size([3, 2]) 12b = torch.LongTensor([[2, 3], [4, 8],[7, 9]])print('b is : &#123;&#125;'.format(b)) b is : tensor([[2, 3], [4, 8], [7, 9]]) 1234c = torch.zeros((3, 2))print('zero tensor : &#123;&#125;'.format(c))d = torch.randn((3, 2))print('randn tensor is : &#123;&#125;'.format(d)) zero tensor : tensor([[0., 0.], [0., 0.], [0., 0.]]) randn tensor is : tensor([[-0.7097, -0.2020], [-0.1451, -0.3853], [-0.9366, 1.1942]]) 12a[0, 1] = 100print('changed a is : &#123;&#125;'.format(a)) changed a is : tensor([[ 2., 100.], [ 4., 8.], [ 7., 9.]]) 12numpy_b = b.numpy()print('conver to numpy is \\n &#123;&#125;'.format(numpy_b)) conver to numpy is [[2 3] [4 8] [7 9]] 12345e = np.array([[2, 3], [4, 5]])torch_e = torch.from_numpy(e)print('from numpy to touch.Tensor is &#123;&#125;'.format(torch_e))f_torch_e = torch_e.float()print('change data type to float tensor : &#123;&#125;'.format(f_torch_e)) from numpy to touch.Tensor is tensor([[2, 3], [4, 5]]) change data type to float tensor : tensor([[2., 3.], [4., 5.]]) Variable 变量variable 提供了自动求导功能，Variable和Tensor没有本质区别，不过Variable会被放入一个计算图中，然后进行前向传播，反向传播，自动求导 Variable有三个比较重要的组成属性：data， grad， grad_fn data： 可以取出Variable中的Tensor数值 grad_fn： 表示的是得到这个Variable的操作，比如加减或者乘除 grad： 是这个Variable的反向传播梯度 1234567891011121314# creat Variablex = Variable(torch.Tensor([1]), requires_grad=True)w = Variable(torch.Tensor([2]), requires_grad=True)b = Variable(torch.Tensor([3]), requires_grad=True)# build a computational graphy = w * x + b# compute gradientsy.backward()print(x.grad)print(w.grad)print(b.grad) tensor([2.]) tensor([1.]) tensor([1.]) 123456789# 矩阵求导x = torch.randn(3)x = Variable(x, requires_grad=True)y = x * 2print(y)y.backward(torch.FloatTensor([1, 0.1, 0.01]))print(x.grad) tensor([-0.6298, 1.7484, -1.1588], grad_fn=&lt;MulBackward0&gt;) tensor([2.0000, 0.2000, 0.0200]) Dataset 数据集torch.utils.data.Dataset 代表这一数据的抽象类吗可以自己定义数据类的继承和重写，只需要定义__len__和 __getitem__ 两个函数 123456789101112131415161718class myDataset(Dataset): def __init__(self, csv_file, txt_file, root_dir, other_file): self.csv_data = pd.read_csv(csv_file) with open(txt_file, 'r') as f: data_list = f.readlines() self.txt_data = data_list self.root_dir = root_dir def __len__(self): return len(self.csv_data) def __getitem__(self, idx): data = (self.csv_file[idx], self.txt_file[idx]) return data # DataLoader 来定义一个新的迭代器# dataiter = DataLoader(myDataset, batch_size=32, shuffle=True, collate_fn=default_collate) nn.Module 模组PyTorch 编写的神经网络，所有层结构和损失函数都来自于torch.nn，所有的模型构建都从这个基类nn.Module继承 12345678910# 构建计算图 也就是模型class net_name(nn.Module): def __init__(self, other_arguments): super(net_name, self).__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size) # other network layer def forward(self, x): x = self.conv1(x) return x 123# 定义损失函数criterion = nn.CrossEntropyLoss()# loss = criterion(output, target) torch.optim 优化在机器学习或者深度学习中，我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略 优化算法分类： 一阶优化算法 梯度下降 二阶优化算法 使用二阶倒数，也叫hessian方法 12# lr 学习率 momentum 动量# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 模型的保存和加载两种保存方式： 保存整个模型的结构信息和参数信息，保存的对象是模型model 保存模型的参数，保存的对象是模型的状态model.state_dict() 两种加载方式： 加载完整的模型结构和参数信息，网络较大的时候记载时间比较长，同时存储空间也比较大 加载模型参数信息，需要先导入模型的结构，然后再导入模型 12345678# 保存# torch.save(model, './model.path')# torch.save(model.state_dict(), './model_state.path')# 加载# load_model = torch.load('model.path')# 先加载模型，再加载参数# model.load_state_dict(torch.load('model_state.path'))","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/tags/Pytorch/"}]},{"title":"2.2 多项式回归","slug":"多项式回归","date":"2019-02-04T16:00:00.000Z","updated":"2019-02-05T08:05:20.172Z","comments":true,"path":"2019/02/05/多项式回归/","link":"","permalink":"http://lucas0625.github.io/2019/02/05/多项式回归/","excerpt":"本文介绍Pytorch的基本概念。","text":"本文介绍Pytorch的基本概念。 多层全链接神经网络多项式回归对于一般的的线性回归，由于该函数拟合出来的是一条直线，所以精度欠佳，我们可以考虑多项式回归，也就是提高每个属性的次数，而不再是只使用一次去回归目标函数 1234567import torchimport numpy as npimport pandas as pdfrom torch import nn, optimfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoaderimport matplotlib.pyplot as plt 123456789# 定义一个待拟合的多变量函数w_target = np.array([0.5, 3, 2.4]) # 定义参数b_target = np.array([0.9]) # 定义参数f_des = 'y = &#123;:.2f&#125; + &#123;:.2f&#125; * x + &#123;:.2f&#125; * x^2 + &#123;:.2f&#125; * x^3'.format( b_target[0], w_target[0], w_target[1], w_target[2]) # 打印出函数的式子print(f_des) y = 0.90 + 0.50 * x + 3.00 * x^2 + 2.40 * x^3 1234567# 画出这个函数的曲线x_sample = np.arange(-3, 3.1, 0.1)y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3plt.plot(x_sample, y_sample, label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x10d3c7d30&gt; 12345678# 构建数据 x 和 y# x 是一个如下矩阵 [x, x^2, x^3]# y 是函数的结果 [y]x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)x_train = torch.from_numpy(x_train).float() # 转换成 float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # 转化成 float tensor 12345678910# 定义参数和模型w = Variable(torch.randn(3, 1), requires_grad=True)b = Variable(torch.zeros(1), requires_grad=True)# 将 x 和 y 转换成 Variablex_train = Variable(x_train)y_train = Variable(y_train)def multi_linear(x): return torch.mm(x, w) + b 123456# 画出更新之前的模型y_pred = multi_linear(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r')plt.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')plt.legend() &lt;matplotlib.legend.Legend at 0x11dec42e8&gt; 12345# 定义损失函数def get_loss(y_, y): return torch.mean((y_ - y_train) ** 2)# or 这两个函数是等价的，一个是pytorch内置的，一个是自定义的criterion = nn.MSELoss() 123# 计算误差，这里的误差和一元的线性模型的误差是相同的，前面已经定义过了 get_lossloss = get_loss(y_pred, y_train)print(loss) tensor(929.5853, grad_fn=&lt;MeanBackward1&gt;) 1print(criterion(y_pred, y_train)) tensor(929.5853, grad_fn=&lt;MseLossBackward&gt;) 12# 自动求导loss.backward() 123# 查看一下 w 和 b 的梯度print(w.grad)print(b.grad) tensor([[ -79.6964], [-155.8812], [-513.9532]]) tensor([-28.7454]) 123# 更新一下参数w.data = w.data - 0.001 * w.grad.datab.data = b.data - 0.001 * b.grad.data 123456# 画出更新一次之后的模型y_pred = multi_linear(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r')plt.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')plt.legend() &lt;matplotlib.legend.Legend at 0x11e171cf8&gt; 1234567891011121314# 进行 100 次参数更新for e in range(100): y_pred = multi_linear(x_train) loss = get_loss(y_pred, y_train) w.grad.data.zero_() b.grad.data.zero_() loss.backward() # 更新参数 w.data = w.data - 0.001 * w.grad.data b.data = b.data - 0.001 * b.grad.data if (e + 1) % 20 == 0: print('epoch &#123;&#125;, Loss: &#123;:.5f&#125;'.format(e+1, loss.data)) epoch 20, Loss: 20.92323 epoch 40, Loss: 6.17233 epoch 60, Loss: 2.68077 epoch 80, Loss: 1.81879 epoch 100, Loss: 1.57229 123456# 画出更新之后的结果y_pred = multi_linear(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r')plt.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')plt.legend() &lt;matplotlib.legend.Legend at 0x11debf4e0&gt;","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/tags/Pytorch/"}]},{"title":"2.1 线性模型","slug":"线性模型","date":"2019-02-04T16:00:00.000Z","updated":"2019-02-05T08:03:01.258Z","comments":true,"path":"2019/02/05/线性模型/","link":"","permalink":"http://lucas0625.github.io/2019/02/05/线性模型/","excerpt":"本文介绍Pytorch的基本概念。","text":"本文介绍Pytorch的基本概念。 多层全链接神经网络线性模型1234567import torchimport numpy as npimport pandas as pdfrom torch import nn, optimfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoaderimport matplotlib.pyplot as plt 1%matplotlib inline 一维线性回归基于均方误差最小化来进行模型求解的办法称为“最小二乘法” 123456# 一维线性回归模型x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)plt.scatter(x_train, y_train, ) &lt;matplotlib.collections.PathCollection at 0x1137c2160&gt; 123# 将numpy.array转为Tensorx_train = torch.from_numpy(x_train)y_train = torch.from_numpy(y_train) 123456789101112131415# 定义模型class LinearRegression(nn.Module): def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) # 输入和输出为一维 def forward(self, x): out = self.linear(x) return out# 判断是否支持GPU加速if torch.cuda.is_available(): model = LinearRegression().cuda()else: model = LinearRegression() 123# 定义损失函数和优化函数criterion = nn.MSELoss()optimizer = optim.SGD(model.parameters(), lr=1e-3) 123456789101112131415161718192021# 开始训练模型num_epochs = 1000for epoch in range(num_epochs): if torch.cuda.is_available(): inputs = Variable(x_train).cuda() target = Variable(y_train).cuda() else: inputs = Variable(x_train) target = Variable(y_train) # 前向传播 out = model(inputs) # 得到前向传播的结果 loss = criterion(out, target) # 得到损失函数 # 反向传播 optimizer.zero_grad() # 归零梯度，不归零梯度的话梯度会累加在一起，造成结果不收敛 loss.backward() optimizer.step() if (epoch + 1) % 20 == 0: print('Epoch&#123;&#125;/&#123;&#125;, loss:&#123;:.6f&#125;'.format(epoch+1, num_epochs, loss.data)) Epoch20/1000, loss:0.216174 Epoch40/1000, loss:0.215691 Epoch60/1000, loss:0.215213 Epoch80/1000, loss:0.214740 Epoch100/1000, loss:0.214271 Epoch120/1000, loss:0.213808 Epoch140/1000, loss:0.213349 Epoch160/1000, loss:0.212895 Epoch180/1000, loss:0.212445 Epoch200/1000, loss:0.212000 Epoch220/1000, loss:0.211560 Epoch240/1000, loss:0.211124 Epoch260/1000, loss:0.210692 Epoch280/1000, loss:0.210265 Epoch300/1000, loss:0.209843 Epoch320/1000, loss:0.209424 Epoch340/1000, loss:0.209010 Epoch360/1000, loss:0.208600 Epoch380/1000, loss:0.208195 Epoch400/1000, loss:0.207793 Epoch420/1000, loss:0.207396 Epoch440/1000, loss:0.207003 Epoch460/1000, loss:0.206613 Epoch480/1000, loss:0.206228 Epoch500/1000, loss:0.205847 Epoch520/1000, loss:0.205469 Epoch540/1000, loss:0.205095 Epoch560/1000, loss:0.204726 Epoch580/1000, loss:0.204360 Epoch600/1000, loss:0.203997 Epoch620/1000, loss:0.203639 Epoch640/1000, loss:0.203284 Epoch660/1000, loss:0.202932 Epoch680/1000, loss:0.202585 Epoch700/1000, loss:0.202241 Epoch720/1000, loss:0.201900 Epoch740/1000, loss:0.201563 Epoch760/1000, loss:0.201229 Epoch780/1000, loss:0.200899 Epoch800/1000, loss:0.200572 Epoch820/1000, loss:0.200248 Epoch840/1000, loss:0.199928 Epoch860/1000, loss:0.199611 Epoch880/1000, loss:0.199297 Epoch900/1000, loss:0.198987 Epoch920/1000, loss:0.198679 Epoch940/1000, loss:0.198375 Epoch960/1000, loss:0.198074 Epoch980/1000, loss:0.197776 Epoch1000/1000, loss:0.197481 123456# 训练之后，预测结果model.eval() # 将模型变成测试模式predict = model(Variable(x_train))predict = predict.data.numpy()plt.plot(x_train.numpy(), y_train.numpy(), 'ro', label='Original data')plt.plot(x_train.numpy(), predict, label='Fitting Line') [&lt;matplotlib.lines.Line2D at 0x117416d30&gt;]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/tags/Pytorch/"}]},{"title":"爬虫基础","slug":"爬虫基础","date":"2019-02-03T16:00:00.000Z","updated":"2019-02-04T09:17:45.054Z","comments":true,"path":"2019/02/04/爬虫基础/","link":"","permalink":"http://lucas0625.github.io/2019/02/04/爬虫基础/","excerpt":"本文介绍爬虫的一些基础知识。","text":"本文介绍爬虫的一些基础知识。 爬虫基本原理讲解什么是爬虫自动获取网络资源机器人。请求网站并提取数据的自动化程序。获取的是html代码，所需要的数据保存在html代码中，接下来就是从html中提取出想要的信息，然后将信息存储在数据库。 爬虫的基本流程 发起请求通过HTTP库像目标站点发起请求，即发送一个Request，请求包含额外的headers等信息，等待服务器响应。 获取响应内容如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。 解析内容得到的内容可能是HTML，可以用正则表达式，网页解析库进行解析，可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。 保存数据保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。 Request中包含的内容 请求方式主要有GET，POST两种类型，另外还有HEAD，PUT，DELETE，OPTIONS等。GET请求： 请求的参数都包含在请求网址里面，即包含在Request url中；可以直接输入url然后回车直接访问。POST请求：请求的参数包含在Form Data中，不包含在请求网址中；必须构建表单，点击表单提交，才可以构造一个POST请求。 请求URLURL全称是统一资源定位符，如一个网页文档，一张图片，一个视频等都可以用URL唯一来确定。 请求头包含请求时的头部信息（配置信息），如User-Agent，Host，Cookies等信息。 请求体请求时额外携带的数据，如表单提交时的表单数据。GET请求时一般不包含，POST请求时加入Form Data信息 Response中包含的内容 响应状态Status Code，有多种响应状态，如200代表成功，301跳转，404找不到页面，502服务器错误。 响应头如内容类型，内容长度，服务器信息，设置Cookie等等。 响应体最主要的部分，包含了请求资源的内容，如网页HTML，图片，二进制数据等。 爬虫可以抓取什么样的数据 网页文本如HTML文档，Json格式文本等。 图片获取到的是二进制文件，保存为图片格式。 视频同为二进制文件，保存为视频格式即可。4.其他只要是能请求到的，都能获取。 怎么进行解析 直接处理返回最简单的字符串，可以对字符串直接进行处理。这种处理的网站比较简单。 Json解析XHR标签，解析json 正则表达式 Beautifulsoup解析 PyQuery解析 XPath解析 为什么我抓到的数据和浏览器看到的不一样抓到的只是网页源代码，浏览器看得到代码是通过JS渲染过的 解决JavaScript渲染问题 分析Ajax请求返回的结果是Json格式字符串 使用Selenium/Webdriver驱动一个浏览器 Splash 模拟JavaScript PyV8，Ghost.py进行模拟加载 怎样保存数据 文本存文本，Json，Xml等。 关系型数据库如MySQL，Oracle，SQL Server，等具有结构化表结构形式存储。 非关系型数据库如MongoDB，Redis等key-value形式存储。 环境配置所需环境python3.6, anacondaMongoDB 非关系型数据库。key,value型数据库 环境安装首先安装homebrew,然后利用homebrew安装mongodb1brew install mongodb Redis 非关系型数据库。key,value型数据库，分布式爬虫需要用到。 1brew install redis MySQL 关系型数据库，体积小，使用方便，做数据存储时简便易用1brew install mysql Python多版本共存 1234windows:环境变量：where python 可以查看各版本Pythonmac:echo $PATH 输出环境变量 爬虫常用库安装 请求库 urllib urllib.request re requests selenium 驱动浏览器，用来做自动化测试 chromedriver 驱动chrome浏览器 phantomjs 无界面浏览器 解析库 lxml 提供了xpath解析方式 beautifulsoup4 (bs4)网页解析库，依赖于lxml pyquery 网页解析库，相对于bs4更加方便 存储库 pymysql 用Python操作MySQL数据库 pymongo 用Python操作MongoDB数据库 redis 用Python操作Redis数据库 flask web库，做web代理的时候会用到 django web服务器框架，提供了完整的后台管理。做分布式爬虫管理的时候会用到 jupyter 运行在网页上的记事本Urllibk库基本使用Urllib库组成 urllib.request 请求模块,用来请求url urllib.error 异常处理模块，捕捉请求时出现的错误 urllib.parse url解析模块 urllib.robotparse robot.txt解析模块 urlopenurllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 1234import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com')print(response.read().decode('utf-8')) 123456import urllib.parseimport urllib.requestdata = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')response = urllib.request.urlopen('http://httpbin.org/post', data=data)print(response.read()) b&apos;{\\n &quot;args&quot;: {}, \\n &quot;data&quot;: &quot;&quot;, \\n &quot;files&quot;: {}, \\n &quot;form&quot;: {\\n &quot;word&quot;: &quot;hello&quot;\\n }, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;1&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Content-Length&quot;: &quot;10&quot;, \\n &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;89667a57-c909-475a-9870-f01181e8c85d&quot;\\n }, \\n &quot;json&quot;: null, \\n &quot;origin&quot;: &quot;219.238.82.169&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/post&quot;\\n}\\n&apos; 1234import urllib.requestresponse = urllib.request.urlopen('http://httpbin.org/get', timeout=1)print(response.read()) b&apos;{\\n &quot;args&quot;: {}, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;0&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;40948f0e-e4b2-4b5f-9d84-aeb77595ca52&quot;\\n }, \\n &quot;origin&quot;: &quot;219.238.82.169&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\\n}\\n&apos; 123456789import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)except urllib.error.URLError as e: if isinstance(e.reason, socket.timeout): print('TIME OUT') TIME OUT 响应响应类型1234import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(type(response)) &lt;class &apos;http.client.HTTPResponse&apos;&gt; 状态码、响应头123456import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(response.status)print(response.getheaders())print(response.getheader('Server')) 200 [(&apos;Server&apos;, &apos;nginx&apos;), (&apos;Content-Type&apos;, &apos;text/html; charset=utf-8&apos;), (&apos;X-Frame-Options&apos;, &apos;SAMEORIGIN&apos;), (&apos;X-Clacks-Overhead&apos;, &apos;GNU Terry Pratchett&apos;), (&apos;Content-Length&apos;, &apos;47436&apos;), (&apos;Accept-Ranges&apos;, &apos;bytes&apos;), (&apos;Date&apos;, &apos;Wed, 22 Mar 2017 15:40:16 GMT&apos;), (&apos;Via&apos;, &apos;1.1 varnish&apos;), (&apos;Age&apos;, &apos;3417&apos;), (&apos;Connection&apos;, &apos;close&apos;), (&apos;X-Served-By&apos;, &apos;cache-itm7426-ITM&apos;), (&apos;X-Cache&apos;, &apos;HIT&apos;), (&apos;X-Cache-Hits&apos;, &apos;16&apos;), (&apos;X-Timer&apos;, &apos;S1490197216.605863,VS0,VE0&apos;), (&apos;Vary&apos;, &apos;Cookie&apos;), (&apos;Public-Key-Pins&apos;, &apos;max-age=600; includeSubDomains; pin-sha256=&quot;WoiWRyIOVNa9ihaBciRSC7XHjliYS9VwUGOIud4PB18=&quot;; pin-sha256=&quot;5C8kvU039KouVrl52D0eZSGf4Onjo4Khs8tmyTlV3nU=&quot;; pin-sha256=&quot;5C8kvU039KouVrl52D0eZSGf4Onjo4Khs8tmyTlV3nU=&quot;; pin-sha256=&quot;lCppFqbkrlJ3EcVFAkeip0+44VaoJUymbnOaEUk7tEU=&quot;; pin-sha256=&quot;TUDnr0MEoJ3of7+YliBMBVFB4/gJsv5zO7IxD9+YoWI=&quot;; pin-sha256=&quot;x4QzPSC810K5/cMjb05Qm4k3Bw5zBn4lTdO/nEW/Td4=&quot;;&apos;), (&apos;Strict-Transport-Security&apos;, &apos;max-age=63072000; includeSubDomains&apos;)] nginx 1234import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(response.read().decode('utf-8')) Request12345import urllib.requestrequest = urllib.request.Request('https://python.org')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) 1234567891011121314from urllib import request, parseurl = 'http://httpbin.org/post'headers = &#123; 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) { &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: { &quot;name&quot;: &quot;Germey&quot; }, &quot;headers&quot;: { &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connect-Time&quot;: &quot;1&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Total-Route-Time&quot;: &quot;0&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;, &quot;Via&quot;: &quot;1.1 vegur&quot;, &quot;X-Request-Id&quot;: &quot;f96e736e-0b8a-4ab4-9dcc-a970fcd2fbbf&quot; }, &quot;json&quot;: null, &quot;origin&quot;: &quot;219.238.82.169&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot; } 1234567891011from urllib import request, parseurl = 'http://httpbin.org/post'dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, method='POST')req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')response = request.urlopen(req)print(response.read().decode('utf-8')) { &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: { &quot;name&quot;: &quot;Germey&quot; }, &quot;headers&quot;: { &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connect-Time&quot;: &quot;0&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Total-Route-Time&quot;: &quot;0&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;, &quot;Via&quot;: &quot;1.1 vegur&quot;, &quot;X-Request-Id&quot;: &quot;a624bcaa-3581-4b93-84b0-037940338e71&quot; }, &quot;json&quot;: null, &quot;origin&quot;: &quot;219.238.82.169&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot; } Handler代理123456789import urllib.requestproxy_handler = urllib.request.ProxyHandler(&#123; 'http': 'http://127.0.0.1:9743', 'https': 'https://127.0.0.1:9743'&#125;)opener = urllib.request.build_opener(proxy_handler)response = opener.open('http://httpbin.org/get')print(response.read()) b&apos;{\\n &quot;args&quot;: {}, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;2&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;b0e2272d-1663-4192-ac45-eb958279afd8&quot;\\n }, \\n &quot;origin&quot;: &quot;110.10.176.224&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\\n}\\n&apos; Cookie12345678import http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+\"=\"+item.value) BAIDUID=E77BF84491E332F6F8F1D451AD0063D3:FG=1 BIDUPSID=E77BF84491E332F6F8F1D451AD0063D3 H_PS_PSSID=1466_21127_22075 PSTM=1490198051 BDSVRTM=0 BD_HOME=0 1234567import http.cookiejar, urllib.requestfilename = \"cookie.txt\"cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) 1234567import http.cookiejar, urllib.requestfilename = 'cookie.txt'cookie = http.cookiejar.LWPCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) 1234567import http.cookiejar, urllib.requestcookie = http.cookiejar.LWPCookieJar()cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 异常处理12345from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.URLError as e: print(e.reason) Not Found 12345678910from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.HTTPError as e: print(e.reason, e.code, e.headers, sep='\\n')except error.URLError as e: print(e.reason)else: print('Request Successfully') Not Found 404 Server: nginx/1.10.1 Date: Wed, 22 Mar 2017 15:59:55 GMT Content-Type: text/html; charset=UTF-8 Transfer-Encoding: chunked Connection: close Vary: Cookie Expires: Wed, 11 Jan 1984 05:00:00 GMT Cache-Control: no-cache, must-revalidate, max-age=0 Link: &lt;http://cuiqingcai.com/wp-json/&gt;; rel=&quot;https://api.w.org/&quot; 12345678910import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)except urllib.error.URLError as e: print(type(e.reason)) if isinstance(e.reason, socket.timeout): print('TIME OUT') &lt;class &apos;socket.timeout&apos;&gt; TIME OUT URL解析urlparseurllib.parse.urlparse(urlstring, scheme=’’, allow_fragments=True) 1234from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment')print(type(result), result) &lt;class &apos;urllib.parse.ParseResult&apos;&gt; ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 1234from urllib.parse import urlparseresult = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) ParseResult(scheme=&apos;https&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 1234from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 1234from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment', allow_fragments=False)print(result) ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5#comment&apos;, fragment=&apos;&apos;) 1234from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html#comment', allow_fragments=False)print(result) ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html#comment&apos;, params=&apos;&apos;, query=&apos;&apos;, fragment=&apos;&apos;) urlunparse1234from urllib.parse import urlunparsedata = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']print(urlunparse(data)) http://www.baidu.com/index.html;user?a=6#comment urljoin12345678910from urllib.parse import urljoinprint(urljoin('http://www.baidu.com', 'FAQ.html'))print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))print(urljoin('http://www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com#comment', '?category=2')) http://www.baidu.com/FAQ.html https://cuiqingcai.com/FAQ.html https://cuiqingcai.com/FAQ.html https://cuiqingcai.com/FAQ.html?question=2 https://cuiqingcai.com/index.php http://www.baidu.com?category=2#comment www.baidu.com?category=2#comment www.baidu.com?category=2 urlencode123456789from urllib.parse import urlencodeparams = &#123; 'name': 'germey', 'age': 22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) http://www.baidu.com?name=germey&amp;age=22 request库使用requestsRequests是Python语言编写，基于urllib，采用Apache2 Licensed开源协议的HTTP库。它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。 实例引入12345678import requestsresponse = requests.get('https://www.baidu.com/')print(type(response))print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 各种请求方式123456import requestsrequests.post('http://httpbin.org/post')requests.put('http://httpbin.org/put')requests.delete('http://httpbin.org/delete')requests.head('http://httpbin.org/get')requests.options('http://httpbin.org/get') 请求基本GET请求基本写法1234import requestsresponse = requests.get('http://httpbin.org/get')print(response.text) 带参数GET请求123import requestsresponse = requests.get(\"http://httpbin.org/get?name=germey&amp;age=22\")print(response.text) 12345678import requestsdata = &#123; 'name': 'germey', 'age': 22&#125;response = requests.get(\"http://httpbin.org/get\", params=data)print(response.text) 解析json12345678import requestsimport jsonresponse = requests.get(\"http://httpbin.org/get\")print(type(response.text))print(response.json())print(json.loads(response.text))print(type(response.json())) 获取二进制数据123456import requestsresponse = requests.get(\"https://github.com/favicon.ico\")print(type(response.text), type(response.content))print(response.text)print(response.content) 123456import requestsresponse = requests.get(\"https://github.com/favicon.ico\")with open('favicon.ico', 'wb') as f: f.write(response.content) f.close() 添加headers1234import requestsresponse = requests.get(\"https://www.zhihu.com/explore\")print(response.text) 1234567import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;response = requests.get(\"https://www.zhihu.com/explore\", headers=headers)print(response.text) 基本POST请求12345import requestsdata = &#123;'name': 'germey', 'age': '22'&#125;response = requests.post(\"http://httpbin.org/post\", data=data)print(response.text) 12345678import requestsdata = &#123;'name': 'germey', 'age': '22'&#125;headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;response = requests.post(\"http://httpbin.org/post\", data=data, headers=headers)print(response.json()) 响应response属性12345678import requestsresponse = requests.get('http://www.jianshu.com')print(type(response.status_code), response.status_code)print(type(response.headers), response.headers)print(type(response.cookies), response.cookies)print(type(response.url), response.url)print(type(response.history), response.history) 状态码判断1234import requestsresponse = requests.get('http://www.jianshu.com/hello.html')exit() if not response.status_code == requests.codes.not_found else print('404 Not Found') 1234import requestsresponse = requests.get('http://www.jianshu.com')exit() if not response.status_code == 200 else print('Request Successfully') 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475100: ('continue',),101: ('switching_protocols',),102: ('processing',),103: ('checkpoint',),122: ('uri_too_long', 'request_uri_too_long'),200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\\\o/', '✓'),201: ('created',),202: ('accepted',),203: ('non_authoritative_info', 'non_authoritative_information'),204: ('no_content',),205: ('reset_content', 'reset'),206: ('partial_content', 'partial'),207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),208: ('already_reported',),226: ('im_used',),# Redirection.300: ('multiple_choices',),301: ('moved_permanently', 'moved', '\\\\o-'),302: ('found',),303: ('see_other', 'other'),304: ('not_modified',),305: ('use_proxy',),306: ('switch_proxy',),307: ('temporary_redirect', 'temporary_moved', 'temporary'),308: ('permanent_redirect', 'resume_incomplete', 'resume',), # These 2 to be removed in 3.0# Client Error.400: ('bad_request', 'bad'),401: ('unauthorized',),402: ('payment_required', 'payment'),403: ('forbidden',),404: ('not_found', '-o-'),405: ('method_not_allowed', 'not_allowed'),406: ('not_acceptable',),407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),408: ('request_timeout', 'timeout'),409: ('conflict',),410: ('gone',),411: ('length_required',),412: ('precondition_failed', 'precondition'),413: ('request_entity_too_large',),414: ('request_uri_too_large',),415: ('unsupported_media_type', 'unsupported_media', 'media_type'),416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),417: ('expectation_failed',),418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),421: ('misdirected_request',),422: ('unprocessable_entity', 'unprocessable'),423: ('locked',),424: ('failed_dependency', 'dependency'),425: ('unordered_collection', 'unordered'),426: ('upgrade_required', 'upgrade'),428: ('precondition_required', 'precondition'),429: ('too_many_requests', 'too_many'),431: ('header_fields_too_large', 'fields_too_large'),444: ('no_response', 'none'),449: ('retry_with', 'retry'),450: ('blocked_by_windows_parental_controls', 'parental_controls'),451: ('unavailable_for_legal_reasons', 'legal_reasons'),499: ('client_closed_request',),# Server Error.500: ('internal_server_error', 'server_error', '/o\\\\', '✗'),501: ('not_implemented',),502: ('bad_gateway',),503: ('service_unavailable', 'unavailable'),504: ('gateway_timeout',),505: ('http_version_not_supported', 'http_version'),506: ('variant_also_negotiates',),507: ('insufficient_storage',),509: ('bandwidth_limit_exceeded', 'bandwidth'),510: ('not_extended',),511: ('network_authentication_required', 'network_auth', 'network_authentication'), 高级操作文件上传12345import requestsfiles = &#123;'file': open('favicon.ico', 'rb')&#125;response = requests.post(\"http://httpbin.org/post\", files=files)print(response.text) 获取cookie123456import requestsresponse = requests.get(\"https://www.baidu.com\")print(response.cookies)for key, value in response.cookies.items(): print(key + '=' + value) 会话维持模拟登录 12345import requestsrequests.get('http://httpbin.org/cookies/set/number/123456789')response = requests.get('http://httpbin.org/cookies')print(response.text) 123456import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/number/123456789')response = s.get('http://httpbin.org/cookies')print(response.text) 证书验证1234import requestsresponse = requests.get('https://www.12306.cn')print(response.status_code) 12345import requestsfrom requests.packages import urllib3urllib3.disable_warnings()response = requests.get('https://www.12306.cn', verify=False)print(response.status_code) 1234import requestsresponse = requests.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))print(response.status_code) 代理设置123456789import requestsproxies = &#123; \"http\": \"http://127.0.0.1:9743\", \"https\": \"https://127.0.0.1:9743\",&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) 1234567import requestsproxies = &#123; \"http\": \"http://user:password@127.0.0.1:9743/\",&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) 1pip3 install 'requests[socks]' 12345678import requestsproxies = &#123; 'http': 'socks5://127.0.0.1:9742', 'https': 'socks5://127.0.0.1:9742'&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) 超时设置1234567import requestsfrom requests.exceptions import ReadTimeouttry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout') 认证设置12345import requestsfrom requests.auth import HTTPBasicAuthr = requests.get('http://120.27.34.24:9001', auth=HTTPBasicAuth('user', '123'))print(r.status_code) 1234import requestsr = requests.get('http://120.27.34.24:9001', auth=('user', '123'))print(r.status_code) 异常处理1234567891011import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout')except ConnectionError: print('Connection error')except RequestException: print('Error') Connection error selenium使用Selenium自动化测试工具，支持多种浏览器。爬虫中主要来解决JavaScript渲染问题。 基本使用12345678910111213141516171819from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com') input = browser.find_element_by_id('kw') input.send_keys('Python') input.send_keys(Keys.ENTER) wait = WebDriverWait(browser, 10) wait.until(EC.presence_of_element_located((By.ID, 'content_left'))) print(browser.current_url) print(browser.get_cookies()) print(browser.page_source)finally: browser.close() 声明浏览器对象1234567from selenium import webdriverbrowser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.Edge()browser = webdriver.PhantomJS()browser = webdriver.Safari() 访问页面123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')print(browser.page_source)browser.close() 查找元素单个元素123456789from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input_first = browser.find_element_by_id('q')input_second = browser.find_element_by_css_selector('#q')input_third = browser.find_element_by_xpath('//*[@id=\"q\"]')print(input_first, input_second, input_third)browser.close() &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 12345678from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input_first = browser.find_element(By.ID, 'q')print(input_first)browser.close() &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;1f209c0d11551c40d9d20ad964fef244&quot;, element=&quot;0.07914603542731591-1&quot;)&gt; 多个元素1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')lis = browser.find_elements_by_css_selector('.service-bd li')print(lis)browser.close() [&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-16&quot;)&gt;] 12345678from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get('https://www.taobao.com')lis = browser.find_elements(By.CSS_SELECTOR, '.service-bd li')print(lis)browser.close() [&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-16&quot;)&gt;] find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 元素交互操作对获取的元素调用交互方法 123456789101112from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input = browser.find_element_by_id('q')input.send_keys('iPhone')time.sleep(1)input.clear()input.send_keys('iPad')button = browser.find_element_by_class_name('btn-search')button.click() 更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement 交互动作将动作附加到动作链中串行执行 123456789101112from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform() 更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains 执行JavaScript123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert(\"To Bottom\")') 获取元素信息获取属性123456789from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)logo = browser.find_element_by_id('zh-top-link-logo')print(logo)print(logo.get_attribute('class')) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;e08c0f28d7f44d75ccd50df6bb676104&quot;, element=&quot;0.7236390660048155-1&quot;)&gt; zu-top-link-logo 获取文本值1234567from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.text) 提问 获取ID、位置、标签名、大小12345678910from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.id)print(input.location)print(input.tag_name)print(input.size) 0.6822924344980397-1 {&apos;y&apos;: 7, &apos;x&apos;: 774} button {&apos;height&apos;: 32, &apos;width&apos;: 66} Frame123456789101112131415161718import timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionbrowser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')print(source)try: logo = browser.find_element_by_class_name('logo')except NoSuchElementException: print('NO LOGO')browser.switch_to.parent_frame()logo = browser.find_element_by_class_name('logo')print(logo)print(logo.text) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.44746093888932004-1&quot;)&gt; NO LOGO &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.13792611320464965-2&quot;)&gt; RUNOOB.COM 等待隐式等待当使用了隐式等待执行测试的时候，如果 WebDriver没有在 DOM中找到元素，将继续等待，超出设定时间后则抛出找不到元素的异常, 换句话说，当查找元素或元素并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是0 1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.implicitly_wait(10)browser.get('https://www.zhihu.com/explore')input = browser.find_element_by_class_name('zu-top-add-question')print(input) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;b29214772d59e912f1ac52e96ed29abe&quot;, element=&quot;0.12886805191194894-1&quot;)&gt; 显式等待1234567891011from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECbrowser = webdriver.Chrome()browser.get('https://www.taobao.com/')wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, 'q')))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))print(input, button) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-2&quot;)&gt; title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’) visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现Alert 详细内容：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions 前进后退1234567891011import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com/')browser.get('https://www.taobao.com/')browser.get('https://www.python.org/')browser.back()time.sleep(1)browser.forward()browser.close() Cookies123456789from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')print(browser.get_cookies())browser.add_cookie(&#123;'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'&#125;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies()) [{&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_cap_id&apos;, &apos;expiry&apos;: 1494196091.403418}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;YWEyOGY4MmI1MzQ2NGY5MmFiMjgzZGUzZWJjYTgwYjY=|1491604091|ff946847ddb5881245bdb7a5e6401b70dc61013f&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;cap_id&apos;, &apos;expiry&apos;: 1494196091.402855}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;MjcxMDE3YzU1YjI4NDljZjljNTQ4ZDIyOWJjZTBhNmY=|1491604091|8da4722b56a1545c2020dba97394a220c0eca8d9&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;r_cap_id&apos;, &apos;expiry&apos;: 1494196091.402525}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmc&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;AADCo7e1kguPTqvEOMieRUzwkA7ZUBhV-VY=|1491604091&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;d_c0&apos;, &apos;expiry&apos;: 1586212091.344773}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.1491604091.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmz&apos;, &apos;expiry&apos;: 1507372091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;3cc99fc5-8706-43fc-90ac-3ad991bd1a25&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_zap&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;97cb00128ccb46659728f7c69cc191b0|1491604091000|1491604091000&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;q_c1&apos;, &apos;expiry&apos;: 1586212091.401644}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.2.10.1491604091&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmb&apos;, &apos;expiry&apos;: 1491605891}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.000--|3=entry_date=20170408=1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmv&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.669300758.1491604091.1491604091.1491604091.1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utma&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmt&apos;, &apos;expiry&apos;: 1491604691}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;AQAAALCZuwh+dgIAeu3PPHA+csDPnXvT&apos;, &apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: True, &apos;name&apos;: &apos;aliyungf_tc&apos;}] [{&apos;secure&apos;: False, &apos;value&apos;: &apos;germey&apos;, &apos;domain&apos;: &apos;.www.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;name&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_cap_id&apos;, &apos;expiry&apos;: 1494196091.403418}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;YWEyOGY4MmI1MzQ2NGY5MmFiMjgzZGUzZWJjYTgwYjY=|1491604091|ff946847ddb5881245bdb7a5e6401b70dc61013f&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;cap_id&apos;, &apos;expiry&apos;: 1494196091.402855}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;MjcxMDE3YzU1YjI4NDljZjljNTQ4ZDIyOWJjZTBhNmY=|1491604091|8da4722b56a1545c2020dba97394a220c0eca8d9&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;r_cap_id&apos;, &apos;expiry&apos;: 1494196091.402525}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmc&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;AADCo7e1kguPTqvEOMieRUzwkA7ZUBhV-VY=|1491604091&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;d_c0&apos;, &apos;expiry&apos;: 1586212091.344773}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.1491604091.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmz&apos;, &apos;expiry&apos;: 1507372091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;3cc99fc5-8706-43fc-90ac-3ad991bd1a25&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_zap&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;97cb00128ccb46659728f7c69cc191b0|1491604091000|1491604091000&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;q_c1&apos;, &apos;expiry&apos;: 1586212091.401644}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.2.10.1491604091&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmb&apos;, &apos;expiry&apos;: 1491605891}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.000--|3=entry_date=20170408=1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmv&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.669300758.1491604091.1491604091.1491604091.1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utma&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmt&apos;, &apos;expiry&apos;: 1491604691}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;AQAAALCZuwh+dgIAeu3PPHA+csDPnXvT&apos;, &apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: True, &apos;name&apos;: &apos;aliyungf_tc&apos;}] [] 选项卡管理123456789101112import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.execute_script('window.open()')print(browser.window_handles)browser.switch_to_window(browser.window_handles[1])browser.get('https://www.taobao.com')time.sleep(1)browser.switch_to_window(browser.window_handles[0])browser.get('https://python.org') [&apos;CDwindow-4f58e3a7-7167-4587-bedf-9cd8c867f435&apos;, &apos;CDwindow-6e05f076-6d77-453a-a36c-32baacc447df&apos;] 异常处理12345from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.find_element_by_id('hello') --------------------------------------------------------------------------- NoSuchElementException Traceback (most recent call last) &lt;ipython-input-23-978945848a1b&gt; in &lt;module&gt;() 3 browser = webdriver.Chrome() 4 browser.get(&apos;https://www.baidu.com&apos;) ----&gt; 5 browser.find_element_by_id(&apos;hello&apos;) /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in find_element_by_id(self, id_) 267 driver.find_element_by_id(&apos;foo&apos;) 268 &quot;&quot;&quot; --&gt; 269 return self.find_element(by=By.ID, value=id_) 270 271 def find_elements_by_id(self, id_): /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in find_element(self, by, value) 750 return self.execute(Command.FIND_ELEMENT, { 751 &apos;using&apos;: by, --&gt; 752 &apos;value&apos;: value})[&apos;value&apos;] 753 754 def find_elements(self, by=By.ID, value=None): /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in execute(self, driver_command, params) 234 response = self.command_executor.execute(driver_command, params) 235 if response: --&gt; 236 self.error_handler.check_response(response) 237 response[&apos;value&apos;] = self._unwrap_value( 238 response.get(&apos;value&apos;, None)) /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/errorhandler.py in check_response(self, response) 190 elif exception_class == UnexpectedAlertPresentException and &apos;alert&apos; in value: 191 raise exception_class(message, screen, stacktrace, value[&apos;alert&apos;].get(&apos;text&apos;)) --&gt; 192 raise exception_class(message, screen, stacktrace) 193 194 def _value_or_default(self, obj, key, default): NoSuchElementException: Message: no such element: Unable to locate element: {&quot;method&quot;:&quot;id&quot;,&quot;selector&quot;:&quot;hello&quot;} (Session info: chrome=57.0.2987.133) (Driver info: chromedriver=2.27.440174 (e97a722caafc2d3a8b807ee115bfb307f7d2cfd9),platform=Mac OS X 10.12.3 x86_64) 1234567891011121314from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: print('No Element')finally: browser.close() No Element 详细文档：http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions JsonJson 是一种轻量级的数据交换格式字符串是Json的表现形式符合Json格式的字符串叫Json字符串 Json相比与XML的优点 易于阅读 易于解析 网络传输效率高同时也可以跨语言交换数据 反序列化1json.loads() # 将Json字符串格式转换成对应的Python相对应数据格式 序列化1json.dumps() #将Python数据格式转换成Json相对应的格式 数据类型转换对应关系 Json Python object dict array kist string str number int number float true True false False null None 相关名词辨析 Json对象：是Javascript中的一中叫法，相对于其他语言其实是没有的。 Json：数据交换的标准格式。 Json字符串：符合Json格式的字符串。 正则表达式正则表达式正则表达式是对字符串操作的一中逻辑公示，就是用事先定义好的一些特定字符，以及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一中过滤逻辑。 常见匹配模式 模式 描述 \\w 匹配字母数字及下划线 \\W 匹配非字母数字下划线 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f]. \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0-9] \\D 匹配任意非数字 \\A 匹配字符串开始 \\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结束 \\G 匹配最后匹配完成的位置 \\n 匹配一个换行符 \\t 匹配一个制表符 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ [^…] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。 * 匹配0个或多个的表达式。 + 匹配1个或多个的表达式。 ? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 {n} 精确匹配n个前面表达式。 {n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a&#124;b 匹配a或b ( ) 匹配括号内的表达式，也表示一个组 re.matchre.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。re.match(pattern, string, flags=0) 最常规的匹配12345678import recontent = 'Hello 123 4567 World_This is a Regex Demo'print(len(content))result = re.match('^Hello\\s\\d\\d\\d\\s\\d&#123;4&#125;\\s\\w&#123;10&#125;.*Demo$', content)print(result)print(result.group())print(result.span()) 41 &lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt; Hello 123 4567 World_This is a Regex Demo (0, 41) 泛匹配1234567import recontent = 'Hello 123 4567 World_This is a Regex Demo'result = re.match('^Hello.*Demo$', content)print(result)print(result.group())print(result.span()) &lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt; Hello 123 4567 World_This is a Regex Demo (0, 41) 匹配目标1234567import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^Hello\\s(\\d+)\\sWorld.*Demo$', content)print(result)print(result.group(1))print(result.span()) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 1234567 (0, 40) 贪婪匹配123456import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^He.*(\\d+).*Demo$', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 7 非贪婪匹配123456import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^He.*?(\\d+).*Demo$', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 1234567 匹配模式1234567import recontent = '''Hello 1234567 World_Thisis a Regex Demo'''result = re.match('^He.*?(\\d+).*?Demo$', content, re.S)print(result.group(1)) 1234567 转义12345import recontent = 'price is $5.00'result = re.match('price is $5.00', content)print(result) None 12345import recontent = 'price is $5.00'result = re.match('price is \\$5\\.00', content)print(result) &lt;_sre.SRE_Match object; span=(0, 14), match=&apos;price is $5.00&apos;&gt; 总结：尽量使用泛匹配、使用括号得到匹配目标、尽量使用非贪婪模式、有换行符就用re.S re.searchre.search 扫描整个字符串并返回第一个成功的匹配。 12345import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'result = re.match('Hello.*?(\\d+).*?Demo', content)print(result) None 123456import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'result = re.search('Hello.*?(\\d+).*?Demo', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(13, 53), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 1234567 总结：为匹配方便，能用search就不用match 匹配演练12345678910111213141516171819202122232425import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;&lt;i class=\"fa fa-user\"&gt;&lt;/i&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?active.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)if result: print(result.group(1), result.group(2)) 齐秦 往事随风 12345678910111213141516171819202122232425import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)if result: print(result.group(1), result.group(2)) 任贤齐 沧海一声笑 12345678910111213141516171819202122232425import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html)if result: print(result.group(1), result.group(2)) beyond 光辉岁月 re.findall搜索字符串，以列表形式返回全部能匹配的子串。 12345678910111213141516171819202122232425262728import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''results = re.findall('&lt;li.*?href=\"(.*?)\".*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)print(results)print(type(results))for result in results: print(result) print(result[0], result[1], result[2]) [(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;), (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;), (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;), (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;), (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)] &lt;class &apos;list&apos;&gt; (&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;) /2.mp3 任贤齐 沧海一声笑 (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;) /3.mp3 齐秦 往事随风 (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;) /4.mp3 beyond 光辉岁月 (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;) /5.mp3 陈慧琳 记事本 (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;) /6.mp3 邓丽君 但愿人长久 1234567891011121314151617181920212223242526import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''results = re.findall('&lt;li.*?&gt;\\s*?(&lt;a.*?&gt;)?(\\w+)(&lt;/a&gt;)?\\s*?&lt;/li&gt;', html, re.S)print(results)for result in results: print(result[1]) [(&apos;&apos;, &apos;一路上有你&apos;, &apos;&apos;), (&apos;&lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;&apos;, &apos;沧海一声笑&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;&apos;, &apos;往事随风&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;&apos;, &apos;光辉岁月&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;&apos;, &apos;记事本&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&apos;, &apos;但愿人长久&apos;, &apos;&lt;/a&gt;&apos;)] 一路上有你 沧海一声笑 往事随风 光辉岁月 记事本 但愿人长久 re.sub替换字符串中每一个匹配的子串后返回替换后的字符串。注意：sub函数第二个参数可以传入一个函数。 12345import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('\\d+', '', content)print(content) Extra stings Hello World_This is a Regex Demo Extra stings 12345import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('\\d+', 'Replacement', content)print(content) Extra stings Hello Replacement World_This is a Regex Demo Extra stings 12345import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('(\\d+)', r'\\1 8910', content)print(content) Extra stings Hello 1234567 8910 World_This is a Regex Demo Extra stings 12345678910111213141516171819202122import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;''' 12345678910111213141516171819202122232425262728import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''html = re.sub('&lt;a.*?&gt;|&lt;/a&gt;', '', html)print(html)results = re.findall('&lt;li.*?&gt;(.*?)&lt;/li&gt;', html, re.S)print(results)for result in results: print(result.strip()) &lt;div id=&quot;songs-list&quot;&gt; &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; &lt;p class=&quot;introduction&quot;&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; &lt;li data-view=&quot;7&quot;&gt; 沧海一声笑 &lt;/li&gt; &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; 往事随风 &lt;/li&gt; &lt;li data-view=&quot;6&quot;&gt;光辉岁月&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt;记事本&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt; 但愿人长久 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; [&apos;一路上有你&apos;, &apos;\\n 沧海一声笑\\n &apos;, &apos;\\n 往事随风\\n &apos;, &apos;光辉岁月&apos;, &apos;记事本&apos;, &apos;\\n 但愿人长久\\n &apos;] 一路上有你 沧海一声笑 往事随风 光辉岁月 记事本 但愿人长久 re.compile将正则字符串编译成正则表达式对象 1将一个正则表达式串编译成正则对象，以便于复用该匹配模式 12345678import recontent = '''Hello 1234567 World_Thisis a Regex Demo'''pattern = re.compile('Hello.*Demo', re.S)result = re.match(pattern, content)#result = re.match('Hello.*Demo', content, re.S)print(result) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This\\nis a Regex Demo&apos;&gt; 实战练习12345678910import requestsimport recontent = requests.get('https://book.douban.com/').textpattern = re.compile('&lt;li.*?cover.*?href=\"(.*?)\".*?title=\"(.*?)\".*?more-meta.*?author\"&gt;(.*?)&lt;/span&gt;.*?year\"&gt;(.*?)&lt;/span&gt;.*?&lt;/li&gt;', re.S)results = re.findall(pattern, content)for result in results: url, name, author, date = result author = re.sub('\\s', '', author) date = re.sub('\\s', '', date) print(url, name, author, date) https://book.douban.com/subject/26925834/?icn=index-editionrecommend 别走出这一步 [英]S.J.沃森 2017-1 https://book.douban.com/subject/26953532/?icn=index-editionrecommend 白先勇细说红楼梦 白先勇 2017-2-1 https://book.douban.com/subject/26959159/?icn=index-editionrecommend 岁月凶猛 冯仑 2017-2 https://book.douban.com/subject/26949210/?icn=index-editionrecommend 如果没有今天，明天会不会有昨天？ [瑞士]伊夫·博萨尔特（YvesBossart） 2017-1 https://book.douban.com/subject/27001447/?icn=index-editionrecommend 人类这100年 阿夏 2017-2 https://book.douban.com/subject/26864566/?icn=index-latestbook-subject 眼泪的化学 [澳]彼得·凯里 2017-2 https://book.douban.com/subject/26991064/?icn=index-latestbook-subject 青年斯大林 [英]西蒙·蒙蒂菲奥里 2017-3 https://book.douban.com/subject/26938056/?icn=index-latestbook-subject 带艾伯特回家 [美]霍默·希卡姆 2017-3 https://book.douban.com/subject/26954757/?icn=index-latestbook-subject 乳房 [美]弗洛伦斯·威廉姆斯 2017-2 https://book.douban.com/subject/26956479/?icn=index-latestbook-subject 草原动物园 马伯庸 2017-3 https://book.douban.com/subject/26956018/?icn=index-latestbook-subject 贩卖音乐 [美]大卫·伊斯曼 2017-3-1 https://book.douban.com/subject/26703649/?icn=index-latestbook-subject 被占的宅子 [阿根廷]胡利奥·科塔萨尔 2017-3 https://book.douban.com/subject/26578402/?icn=index-latestbook-subject 信仰与观看 [法]罗兰·雷希特(RolandRecht) 2017-2-17 https://book.douban.com/subject/26939171/?icn=index-latestbook-subject 妹妹的坟墓 [美]罗伯特·杜格尼(RobertDugoni) 2017-3-1 https://book.douban.com/subject/26972465/?icn=index-latestbook-subject 全栈市场人 Lydia 2017-2-1 https://book.douban.com/subject/26986928/?icn=index-latestbook-subject 终极X战警2 [英]马克·米勒&amp;nbsp;/&amp;nbsp;[美]亚当·库伯特 2017-3-15 https://book.douban.com/subject/26948144/?icn=index-latestbook-subject 格调（修订第3版） [美]保罗·福塞尔（PaulFussell） 2017-2 https://book.douban.com/subject/26945792/?icn=index-latestbook-subject 原谅石 [美]洛里·斯皮尔曼 2017-2 https://book.douban.com/subject/26974207/?icn=index-latestbook-subject 庇护二世闻见录 [意]皮科洛米尼 2017-2 https://book.douban.com/subject/26983143/?icn=index-latestbook-subject 遇见野兔的那一年 [芬]阿托·帕西林纳 2017-3-1 https://book.douban.com/subject/26976429/?icn=index-latestbook-subject 鲍勃·迪伦：诗人之歌 [法]让-多米尼克·布里埃 2017-4 https://book.douban.com/subject/26962860/?icn=index-latestbook-subject 牙医谋杀案 [英]阿加莎·克里斯蒂 2017-3 https://book.douban.com/subject/26923022/?icn=index-latestbook-subject 石挥谈艺录：把生命交给舞台 石挥 2017-2 https://book.douban.com/subject/26897190/?icn=index-latestbook-subject 理想 [美]安·兰德 2017-2 https://book.douban.com/subject/26985981/?icn=index-latestbook-subject 青苔不会消失 袁凌 2017-4 https://book.douban.com/subject/26984949/?icn=index-latestbook-subject 地下铁道 [美]科尔森·怀特黑德（ColsonWhitehead） 2017-3 https://book.douban.com/subject/26944012/?icn=index-latestbook-subject 极简进步史 [英]罗纳德·赖特 2017-4-1 https://book.douban.com/subject/26969002/?icn=index-latestbook-subject 驻马店伤心故事集 郑在欢 2017-2 https://book.douban.com/subject/26854223/?icn=index-latestbook-subject 致薇拉 [美]弗拉基米尔·纳博科夫 2017-3 https://book.douban.com/subject/26841616/?icn=index-latestbook-subject 北方档案 [法]玛格丽特·尤瑟纳尔 2017-2 https://book.douban.com/subject/26980391/?icn=index-latestbook-subject 食帖15：便当灵感集 林江 2017-2 https://book.douban.com/subject/26958882/?icn=index-latestbook-subject 生火 [法]克里斯多夫·夏布特（ChristopheChabouté）编绘 2017-3 https://book.douban.com/subject/26989163/?icn=index-latestbook-subject 文明之光（第四册） 吴军 2017-3-1 https://book.douban.com/subject/26878906/?icn=index-latestbook-subject 公牛山 [美]布赖恩·帕诺威奇 2017-2 https://book.douban.com/subject/26989534/?icn=index-latestbook-subject 几乎消失的偷闲艺术 [加拿大]达尼·拉费里埃 2017-4 https://book.douban.com/subject/26939973/?icn=index-latestbook-subject 散步去 [日]谷口治郎 2017-3 https://book.douban.com/subject/26865333/?icn=index-latestbook-subject 中国1945 [美]理查德·伯恩斯坦(RichardBernstein) 2017-3-1 https://book.douban.com/subject/26989242/?icn=index-latestbook-subject 有匪2：离恨楼 Priest 2017-3 https://book.douban.com/subject/26985790/?icn=index-latestbook-subject 女人、火与危险事物 [美]乔治·莱考夫 2017-3 https://book.douban.com/subject/26972277/?icn=index-latestbook-subject 寻找时间的人 [爱尔兰]凯特·汤普森 2017-3 https://www.douban.com/note/610758170/ 白先勇细说红楼梦【全二册】 白先勇 2017-2-1 https://read.douban.com/ebook/31540864/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 奇爱博士 [英]彼得·乔治 2016-8-1 https://read.douban.com/ebook/31433872/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 在时光中盛开的女子 李筱懿 2017-3 https://read.douban.com/ebook/31178635/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 如何高效记忆（原书第2版） [美]肯尼思•希格比（KennethL.Higbee） 2017-3-5 https://read.douban.com/ebook/31358183/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 愿无岁月可回头 回忆专用小马甲 2016-9 https://read.douban.com/ebook/31341636/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 走神的艺术与科学 [新西兰]迈克尔·C.科尔巴里斯 2017-3-1 https://read.douban.com/ebook/27621094/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 神秘的量子生命 [英]吉姆•艾尔－哈利利/约翰乔•麦克法登 2016-8 https://read.douban.com/ebook/31221966/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 寻找时间的人 [爱尔兰]凯特·汤普森 2017-3 https://read.douban.com/ebook/31481323/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 山之四季 [日]高村光太郎 2017-1 https://read.douban.com/ebook/31154855/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 东北游记 [美]迈克尔·麦尔 2017-1 BeautifulSoup 解析库BeautifulSoup灵活又方便的网页解析库，处理高效，支持多种解析器，利用它不用编写正则表达式即可方便的实现网页信息的提取。 解析库 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, “html.parser”) Python的内置标准库、执行速度适中 、文档容错能力强 Python 2.7.3 or 3.2.2)前的版本中文容错能力差 lxml HTML 解析器 BeautifulSoup(markup, “lxml”) 速度快、文档容错能力强 需要安装C语言库 lxml XML 解析器 BeautifulSoup(markup, “xml”) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, “html5lib”) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢、不依赖外部扩展 基本使用123456789101112131415html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string) &lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse&apos;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt; &lt;b&gt; The Dormouse&apos;s story &lt;/b&gt; &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;!-- Elsie --&gt; &lt;/a&gt; , &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt; Lacie &lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt; Tillie &lt;/a&gt; ; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; ... &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; The Dormouse&apos;s story 标签选择器选择元素1234567891011121314151617html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.head)print(soup.p) &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;class &apos;bs4.element.Tag&apos;&gt; &lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt; 获取名称1234567891011121314html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title.name) title 获取属性123456789101112131415html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.attrs['name'])print(soup.p['name']) dromouse dromouse 获取内容1234567891011121314html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p clss=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.string) The Dormouse&apos;s story 嵌套选择1234567891011121314html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.head.title.string) The Dormouse&apos;s story 子节点和子孙节点123456789101112131415161718192021html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.contents) [&apos;\\n Once upon a time there were three little sisters; and their names were\\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt;, &apos;\\n&apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &apos; \\n and\\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;, &apos;\\n and they lived at the bottom of a well.\\n &apos;] 1234567891011121314151617181920212223html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.children)for i, child in enumerate(soup.p.children): print(i, child) &lt;list_iterator object at 0x1064f7dd8&gt; 0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; 2 3 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; 4 and 5 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; 6 and they lived at the bottom of a well. 1234567891011121314151617181920212223html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.descendants)for i, child in enumerate(soup.p.descendants): print(i, child) &lt;generator object descendants at 0x10650e678&gt; 0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; 2 3 &lt;span&gt;Elsie&lt;/span&gt; 4 Elsie 5 6 7 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; 8 Lacie 9 and 10 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; 11 Tillie 12 and they lived at the bottom of a well. 父节点和祖先节点123456789101112131415161718192021html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.a.parent) &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; 123456789101112131415161718192021html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(list(enumerate(soup.a.parents))) [(0, &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;), (1, &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;), (2, &lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;), (3, &lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;)] 兄弟节点12345678910111213141516171819202122html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(list(enumerate(soup.a.next_siblings)))print(list(enumerate(soup.a.previous_siblings))) [(0, &apos;\\n&apos;), (1, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;), (2, &apos; \\n and\\n &apos;), (3, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;), (4, &apos;\\n and they lived at the bottom of a well.\\n &apos;)] [(0, &apos;\\n Once upon a time there were three little sisters; and their names were\\n &apos;)] 标准选择器find_all( name , attrs , recursive , text , **kwargs )可根据标签名、属性、内容查找文档 name12345678910111213141516171819202122html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all('ul'))print(type(soup.find_all('ul')[0])) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;, &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt;] &lt;class &apos;bs4.element.Tag&apos;&gt; 12345678910111213141516171819202122html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.find_all('ul'): print(ul.find_all('li')) [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] attrs12345678910111213141516171819202122html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\" name=\"elements\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(attrs=&#123;'id': 'list-1'&#125;))print(soup.find_all(attrs=&#123;'name': 'elements'&#125;)) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] 12345678910111213141516171819202122html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(id='list-1'))print(soup.find_all(class_='element')) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] text123456789101112131415161718192021html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(text='Foo')) [&apos;Foo&apos;, &apos;Foo&apos;] find( name , attrs , recursive , text , **kwargs )find返回单个元素，find_all返回所有元素 1234567891011121314151617181920212223html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find('ul'))print(type(soup.find('ul')))print(soup.find('page')) &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;class &apos;bs4.element.Tag&apos;&gt; None find_parents() find_parent()find_parents()返回所有祖先节点，find_parent()返回直接父节点。 find_next_siblings() find_next_sibling()find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。 find_previous_siblings() find_previous_sibling()find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。 find_all_next() find_next()find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点 find_all_previous() 和 find_previous()find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点 CSS选择器通过select()直接传入CSS选择器即可完成选择CSS选择器语法： 123456789101112131415161718192021222324html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.select('.panel .panel-heading'))print(soup.select('ul li'))print(soup.select('#list-2 .element'))print(type(soup.select('ul')[0])) [&lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] &lt;class &apos;bs4.element.Tag&apos;&gt; 12345678910111213141516171819202122html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul.select('li')) [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] 获取属性1234567891011121314151617181920212223html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul['id']) print(ul.attrs['id']) list-1 list-1 list-2 list-2 获取内容12345678910111213141516171819202122html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for li in soup.select('li'): print(li.get_text()) Foo Bar Jay Foo Bar 总结 推荐使用lxml解析库，必要时使用html.parser 标签选择筛选功能弱但是速度快 建议使用find()、find_all() 查询匹配单个结果或者多个结果 如果对CSS选择器熟悉建议使用select() 记住常用的获取属性和文本值的方法 pyquerypyquery强大又灵活的网页解析库。如果你觉得正则写起来太麻烦，如果你觉得BeautifulSoup语法太难记，如果你熟悉jQuery的语法，那么PyQuery就是你的绝佳选择。主要是使用CSS选择器进行解析。比BeautifulSoup中的select()方法更加强大。 初始化 字符串初始化，直接传入html即可 URL初始化，将URL以参数url传入即可。 文件初始化，将demo.html传入filename即可。 字符串初始化1234567891011121314html = '''&lt;div&gt; &lt;ul&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; URL初始化123from pyquery import PyQuery as pqdoc = pq(url='http://www.baidu.com')print(doc('head')) &lt;head&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&quot;/&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; 文件初始化123from pyquery import PyQuery as pqdoc = pq(filename='demo.html')print(doc('li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 基本CSS选择器 find() 返回符合条件的所有节点。 children() 返回所有子节点。 parent() 返回某个节点的父节点 parents() 返回某个节点的祖先节点。 siblings() 返回某个节点的兄弟节点。 获取到单个节点，可以直接打印或者转换成字符串。获取到多个节点，需要调用items()方法，将结果变成生成器，然后对其遍历，去除所有结果。 attr() 获取属性。 text() 获取文本。 1234567891011121314html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('#container .list li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 查找元素子元素12345678910111213141516171819html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 123lis = items.children()print(type(lis))print(lis) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 12lis = items.children('.active')print(lis) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 父元素1234567891011121314151617html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 12345678910111213141516171819html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')parents = items.parents()print(type(parents))print(parents) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 12parent = items.parents('.wrap')print(parent) &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 兄弟元素1234567891011121314151617html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings()) &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 1234567891011121314151617html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings('.active')) &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 遍历单个元素1234567891011121314151617html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 12345678910111213141516171819html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li) &lt;class &apos;generator&apos;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 获取信息获取属性12345678910111213141516171819html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.attr('href'))print(a.attr.href) &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; link3.html link3.html 获取文本123456789101112131415161718html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text()) &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; third item 获取HTML123456789101112131415161718html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(li.html()) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; DOM操作addClass、removeClass123456789101112131415161718192021html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.removeClass('active')print(li)li.addClass('active')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; attr、css123456789101112131415161718192021html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.attr('name', 'link')print(li)li.css('font-size', '14px')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot; name=&quot;link&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot; name=&quot;link&quot; style=&quot;font-size: 14px&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; remove123456789101112html = '''&lt;div class=\"wrap\"&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)wrap = doc('.wrap')print(wrap.text())wrap.find('p').remove()print(wrap.text()) Hello, World This is a paragraph. Hello, World 其他DOM方法http://pyquery.readthedocs.io/en/latest/api.html 伪类选择器123456789101112131415161718192021222324252627html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('li:first-child')print(li)li = doc('li:last-child')print(li)li = doc('li:nth-child(2)')print(li)li = doc('li:gt(2)')print(li)li = doc('li:nth-child(2n)')print(li)li = doc('li:contains(second)')print(li) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; 更多CSS选择器可以查看http://www.w3school.com.cn/css/index.asp 官方文档http://pyquery.readthedocs.io/ 数据库Python操作三大主流数据库数据库分类 关系型数据库e.g: MariaDB, SQLite, SQLServer, MySQL, PostgreSQL, ORACLE 非关系型数据库e.g: mongoDB, HBASE, redis, CouchDB, Cassandre, Neo4j分类： 文档型 key-value型 列式数据库 图形数据库 MySQL 数据库","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://lucas0625.github.io/categories/爬虫/"}],"tags":[{"name":"基础知识","slug":"基础知识","permalink":"http://lucas0625.github.io/tags/基础知识/"}]},{"title":"常见排序算法","slug":"常见排序算法","date":"2018-12-10T16:00:00.000Z","updated":"2018-12-11T06:08:52.309Z","comments":true,"path":"2018/12/11/常见排序算法/","link":"","permalink":"http://lucas0625.github.io/2018/12/11/常见排序算法/","excerpt":"本文介绍常见的排序算法。包括冒泡排序，插入排序，选择排序。","text":"本文介绍常见的排序算法。包括冒泡排序，插入排序，选择排序。 冒泡排序12345678910111213from typing import List# 冒泡排序 稳定排序算法def bubble_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(len(a)): made_swap = False for j in range(len(a) - i - 1): if a[j] &gt; a[j + 1]: a[j], a[j + 1] = a[j + 1], a[j] made_swap = True if not made_swap: break 注： 空间复杂度O(1)，原地排序算法。 稳定排序算法。 最优时间复杂度O(n), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 插入排序1234567891011121314from typing import List# 插入排序 稳定排序算法def insertion_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(1, len(a)): value = a[i] j = i - 1 while j &gt;= 0 and a[j] &gt; value: a[j + 1] = a[j] j -= 1 a[j + 1] = value 注： 空间复杂度O(1)，原地排序算法。 稳定排序算法。 最优时间复杂度O(n), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 选择排序123456789101112131415from typing import List# 选择排序 非稳定排序算法def selection_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(len(a)): min_index = i min_val = a[i] for j in range(i, len(a)): if a[j] &lt; min_val: min_val = a[j] min_index = j a[i], a[min_index] = a[min_index], a[i] 注： 空间复杂度O(1)，原地排序算法。 非稳定排序算法。 最优时间复杂度O(n^2), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2)","categories":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]},{"title":"Python 编码规范","slug":"Python 编码规范","date":"2018-10-08T16:00:00.000Z","updated":"2018-10-09T13:42:43.892Z","comments":true,"path":"2018/10/09/Python 编码规范/","link":"","permalink":"http://lucas0625.github.io/2018/10/09/Python 编码规范/","excerpt":"遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。在团队开发中，使用（尽量）统一的编码风格，还可以降低沟通成本。","text":"遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。在团队开发中，使用（尽量）统一的编码风格，还可以降低沟通成本。 缩紧 不要使用 tab 缩进 使用任何编辑器写 Python，请把一个 tab 展开为 4 个空格 绝对不要混用 tab 和空格，否则容易出现 IndentationError 空格 在 list, dict, tuple, set, 参数列表的 , 后面加一个空格 在 dict 的 : 后面加一个空格 在注释符号 # 后面加一个空格，但是 #!/usr/bin/python 的 # 后不能有空格 操作符两端加一个空格，如 +, -, *, /, |, &amp;, = 接上一条，在参数列表里的 =两端不需要空格 括号((), {}, [])内的两端不需要空格 空行 function 和 class 顶上两个空行 class 的 method 之间一个空行 函数内逻辑无关的段落之间空一行，不要过度使用空行 不要把多个语句写在一行，然后用 ; 隔开 if/for/while 语句中，即使执行语句只有一句，也要另起一行 换行 每一行代码控制在80字符以内 使用\\或()控制换行，举例： 12345678def foo(first, second, third, fourth, fifth, sixth, and_some_other_very_long_param): user = User.objects.filter_by(first=first, second=second, third=third) \\ .skip(100).limit(100) \\ .all()text = ('Long strings can be made up ' 'of several shorter strings.') 命名 使用有意义的，英文单词或词组，绝对不要使用汉语拼音 package/module 名中不要出现 - 各种类型的命名规范： import 所有 import 尽量放在文件开头，在 docstring 下面，其他变量定义的上面 不要使用 from foo imort * import 需要分组，每组之间一个空行，每个分组内的顺序尽量采用字典序，分组顺序是： 1.标准库 2.第三方库 3.本项目的 package 和 module 不要使用隐式的相对导入（implicit relative imports），可是使用显示的相对导入（explicit relative imports），如 from ..utils import validator，最好使用全路径导入（absolute imports） 对于不同的 package，一个 import 单独一行，同一个 package/module 下的内容可以写一起 为了避免可能出现的命名冲突，可以使用 as 或导入上一级命名空间 不要出现循环导入(cyclic import) 注释 文档字符串 docstring, 是 package, module, class, method, function 级别的注释，可以通过 __doc__ 成员访问到，注释内容在一对 &quot;&quot;&quot; 符号之间 function, method 的文档字符串应当描述其功能、输入参数、返回值，如果有复杂的算法和实现，也需要写清楚 不要写错误的注释，不要无谓的注释 优先使用英文写注释，英文不好全部写中文，否则更加看不懂 异常 不要轻易使用 try/except except 后面需要指定捕捉的异常，裸露的 except 会捕捉所有异常，意味着会隐藏潜在的问题 可以有多个 except 语句，捕捉多种异常，分别做异常处理 使用 finally 子句来处理一些收尾操作 try/except 里的内容不要太多，只在可能抛出异常的地方使用 从 Exception 而不是 BaseException 继承自定义的异常类 Class(类) 显示的写明父类，如果不是继承自别的类，就继承自 object 类 使用 super 调用父类的方法 支持多继承，即同时有多个父类，建议使用 Mixin 编码建议字符串 使用字符串的 join 方法拼接字符串 使用字符串类型的方法，而不是 string 模块的方法 使用 startswith 和 endswith 方法比较前缀和后缀 使用 format 方法格式化字符串 比较 空的 list, str, tuple, set, dict 和 0, 0.0, None 都是 False 使用 if some_list 而不是 if len(some_list) 判断某个 list 是否为空，其他类型同理 使用 is 和 is not 与单例（如 None）进行比较，而不是用 == 和 != 使用 if a is not None 而不是 if not a is None 用 isinstance 而不是 type 判断类型 不要用 == 和 != 与 True 和 False 比较（除非有特殊情况，如在 sqlalchemy 中可能用到） 使用 in 操作: 用 key in dict 而不是 dict.has_key() 用 set 加速 “存在性” 检查，list 的查找是线性的，复杂度 O(n)，set 底层是 hash table, 复杂度 O(1)，但用 set需要比 list 更多内存空间 其他 使用列表表达式（list comprehension），字典表达式(dict comprehension, Python 2.7+) 和生成器(generator) dict 的 get 方法可以指定默认值，但有些时候应该用 [] 操作，使得可以抛出 KeyError 使用 for item in list 迭代 list, for index, item in enumerate(list) 迭代 list 并获取下标 使用内建函数 sorted 和 list.sort 进行排序 适量使用map, reduce, filter 和 lambda，使用内建的 all, any 处理多个条件的判断 使用 defaultdict (Python 2.5+), Counter(Python 2.7+) 等 “冷门” 但好用的标准库算法和数据结构 使用装饰器(decorator) 使用 with 语句处理上下文 有些时候不要对类型做太过严格的限制，利用 Python 的鸭子类型（Duck Type）特性 使用 logging 记录日志，配置好格式和级别 了解 Python 的 Magic Method：A Guide to Python’s Magic Methods, Python 魔术方法指南 阅读优秀的开源代码，如 Flask 框架, Requests for Humans 不要重复造轮子，查看标准库、PyPi、Github、Google 等使用现有的优秀的解决方案","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/categories/编程/"}],"tags":[{"name":"经验","slug":"经验","permalink":"http://lucas0625.github.io/tags/经验/"}]},{"title":"深入理解 Python package","slug":"深入理解Python package","date":"2018-10-07T16:00:00.000Z","updated":"2018-10-09T13:42:17.241Z","comments":true,"path":"2018/10/08/深入理解Python package/","link":"","permalink":"http://lucas0625.github.io/2018/10/08/深入理解Python package/","excerpt":"Python 是通过 module 组织代码的，module 即一个 py 文件，module 又是通过 package 来组织的，package 是一个包含 __init__.py 的文件夹，代码，module，package 它们三者的关系就是：module 包含代码，package 至少包含一个为 __init__.py 的 module。 12345package├── __init__.py├── submodule.py└── subpackage └── __init__.py","text":"Python 是通过 module 组织代码的，module 即一个 py 文件，module 又是通过 package 来组织的，package 是一个包含 __init__.py 的文件夹，代码，module，package 它们三者的关系就是：module 包含代码，package 至少包含一个为 __init__.py 的 module。 12345package├── __init__.py├── submodule.py└── subpackage └── __init__.py 空的 __init__.py不包含任何代码的 __init__.py 只用来标识一个文件夹是一个 package，而 package 是可以被导出的。 1from package import item 此处的 item 可以是 package 中包含的 submodule 或 subpackage。 12from package import submodulefrom package import subpackage 不为空 __init__.py如果 __init__.py 不为空，其中包含的任何变量，包括 function、class、variable 以及 任何被导入的 module 都可以通过 package 导出。 1from package import item 此处的 item 可以是 __init__.py中的任何变量 package的初始化工作一个 package 被导入，不管在什么时候 __init__.py 中的代码只执行一次。 12345&gt;&gt;&gt; import packagehello world&gt;&gt;&gt; import package&gt;&gt;&gt; import package&gt;&gt;&gt; 由于 package 被导入时__init__.py 中的可执行代码会被执行，所以小心在 package 中放置你的代码，尽可能消除它们产生的副作用，比如把代码尽可能的进行封装成函数或类。 从package中倒入变量的顺序1from package import item import 语句首先检查 item 是否是 __init__.py 中定义的变量，然后检查其是不是一个 subpackage，如果不是再去检查其是不是一个 module，都不是将抛出 ImportError。 在 import item.subitem.subsubitem 语句时，除了最后一个 subsubitem 之外其他 item 都必须是 package，而最后一个 subsubitem 必须是一个 package 或者 module，不能是他前一个 item 定义的 function、class、variable。 使用*导入在 from package import *语句中，如果 __init__.py 中定义了 __all__ 变量，一个 list，仅仅只有这个 list 中定义的 submodule 或者变量将会被导出。 如果__init__.py中没有__all__变量，导出将按照一下规则执行： 此 package 被导入，并且执行 __init__.py 中可被执行的代码 __init__.py 中定义的 variable 被导入 __init__.py 中被显式导入的 module 被导入","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/categories/编程/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/tags/基础/"}]},{"title":"python 格式化显示字符串","slug":"json","date":"2018-09-16T08:30:18.409Z","updated":"2018-10-09T13:42:33.245Z","comments":true,"path":"2018/09/16/json/","link":"","permalink":"http://lucas0625.github.io/2018/09/16/json/","excerpt":"","text":"12import jsonjson.dumps(dict, indent=4)","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/categories/编程/"}],"tags":[{"name":"技巧","slug":"技巧","permalink":"http://lucas0625.github.io/tags/技巧/"}]},{"title":"算法和数据结构","slug":"Algorithms and Data Structures","date":"2018-09-11T13:17:13.800Z","updated":"2018-09-16T08:35:39.377Z","comments":true,"path":"2018/09/11/Algorithms and Data Structures/","link":"","permalink":"http://lucas0625.github.io/2018/09/11/Algorithms and Data Structures/","excerpt":"数据结构数据结构就是关系，数据元素相互之间存在的一种或多种特定关系的集合","text":"数据结构数据结构就是关系，数据元素相互之间存在的一种或多种特定关系的集合 逻辑结构和物理结构逻辑结构： 指数据对象中数据元素之间的相互关系，也是今后最需要关注和讨论的问题 逻辑结构指数据的逻辑结构在计算机中的存储形式 四大逻辑结构： 集合结构： 同属一个集合 线性结构： 一对一 树形结构： 一对多 图形结构： 多对多 物理结构 存储器： 主要针对内存而言，想赢盘，光盘 数据存储结构形式： 顺序存储，链式存储 顺序存储： 数据元素存放在地址连续的存储单元里，其数据的逻辑关系和物理关系是一致的链式存储： 把数据元素放在任意的存储单元里，这组存储单元可以是连续的，也可以是不连续的 算法算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作 算法的五个基本特征： 输入：零个或多个输入 输出： 至少有一个或多个输出 有穷行： 算法在执行有限的步骤之后，自动结束而不会出现无限循环 确定性： 每一个步骤都具有确定的含义 可行性： 每一步都必须是可行的 算法设计的要求 正确性 可读性 健壮性 时间效率高和存储量低 算法效率的度量方法运行时间主要取决因素 算法采用的策略，方案 编译产生的代码质量 问题的输入规模 机器执行指令的速度 分析算法的运行时间时，重要的是把基本操作的数量和输入模式关联起来可以忽略的项 常数可以忽略 与最高次项相乘的常数可以忽略 其他次项（除去最高项）时间复杂度定义：在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随n的变化情况并确定T(n)的数量级。算法的时间复杂度，也就是算法的时间度量，记作：T(n)=O(f(n))。它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐进时间复杂度，简称为时间复杂度。其中f(n)是问题规模n的某个函数。记法：O()，大O记法推导大O阶： 用常数1取代运行时间中的所有假发常数 在修改后的运行次数函数中，只保留最高阶项 如果最高阶存在且不是1，则去除与这个项相乘的常数 得到的最后结果就是大O阶最常用的大O阶1.O(1) 常数阶2.O(n) 线形阶3.O(n^2 ) 平方阶4.O(logn) 对数阶5.O(nlogn) nlogn阶6.O(n^3 ) 立方阶7.O(2^n ) 指数阶空间复杂度算法的空间复杂度通过计算算法所需的存储空间实现，算法的空间复杂度的计算公式：S(n)=O(f(n))，其中，n为问题规模，f(n)为语句关于n所占存储空间的函数通常，我们都是用时间复杂度来指运行时间的需求，用空间复杂度来指空间需求","categories":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/tags/算法/"}]}]}