{"meta":{"title":"Mr.wang","subtitle":null,"description":null,"author":"琛","url":"http://lucas0625.github.io/blog"},"pages":[{"title":"About","date":"2019-02-04T08:28:25.234Z","updated":"2019-02-04T08:28:25.125Z","comments":true,"path":"about/index.html","permalink":"http://lucas0625.github.io/blog/about/index.html","excerpt":"","text":"个人信息 姓名：Lucas 性别：男 电子邮箱：wangchen_jh@outlook.com Wechat：751009328 目前状态：格式化大脑中。。。"},{"title":"Categories","date":"2019-02-05T07:55:36.181Z","updated":"2019-02-04T08:28:25.234Z","comments":true,"path":"categories/index.html","permalink":"http://lucas0625.github.io/blog/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-02-04T08:28:25.266Z","updated":"2019-02-04T08:28:25.246Z","comments":true,"path":"tags/index.html","permalink":"http://lucas0625.github.io/blog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"神经网络训练技巧","slug":"network-train","date":"2019-05-29T06:59:17.774Z","updated":"2019-05-29T07:00:48.903Z","comments":true,"path":"2019/05/29/network-train/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/29/network-train/","excerpt":"神经网络训练过程中的一些技巧","text":"神经网络训练过程中的一些技巧 卷积神经网络训练技巧数据增强回译法 数据读取Dataset灵活的数据读入，用到 torch.utils.data.Dataset()。可以先看看 Dataset 的文档 如果我们希望定义自己的数据读入函数，我们只需要定义一个子类继承于 Dataset，然后重新定义 __getitem__() 和 __len__() 这两个函数就可以了，__getitem__() 表示按照下标取出其中一个数据，__len__ 表示所有数据的总数/ 数据格式如下： from torch.utils.data import Dataset # 定义一个子类叫 custom_dataset，继承于 Datasetclass Custom_dataset(Dataset): def __init__(self, txt_path, transform=None): self.transform = transform # 传入数据预处理 with open(txt_path, 'r') as f: lines = f.readlines() self.img_list = [i.split()[0] for i in lines] # 得到所有的图像名字 self.label_list = [i.split()[1] for i in lines] # 得到所有的label def __getitem__(self, idx): # 根据 idx 取出一个 img = self.img_list[idx] label = self.label_list[idx] if self.transform is not None: img = self.transform(img) return img, label def __len__(self): # 总数据是多少 return len(self.label_list) # 测试读取函数txt_dataset = Custom_dataset('./train.txt') # 读入 txt 文件# 取得其中一个数据data, label = txt_dataset[0]print(data)print(label) 1009_2.png YOU 通过上诉方式我们也能够非常方便的定义一个数据读入，同时也能够方便的定义数据预处理 DataLoader实现了一个 batch 的处理数据，DataLoader 是一个多线程迭代器，能够帮助我们一个 batch 的读入模型，同时使用多线程速度更快。DataLoader 的文档 DataLoader 中有几个使用最多的参数，第一个是 dataset，就是我们前面定义的数据读入，可以使用 ImageFolder，可以使用自己定义的数据读入子类，第二个是 batch_size，这就是一批多少个数据，第三个是 shuffle，表示是否打乱数据，第四个是 num_workers，表示使用几个线程，默认使用主线程，第五个是 drop_last，表示是否扔掉最后无法构成一个批次的数据。 除了这些参数之外，还有一个参数叫 collate_fn , 能够方便的处理一个 batch 中的数据。这个是在 DataLoader 中已经有默认定义了，可以看源码如果希望将 batch 输出的 label 补成相同的长度，短的 label 用 0 填充，这其实是在机器翻译中的一个需求，这个时候我们就需要使用 collate_fn 来自定义我们 batch 的处理方式， from torch.utils.data import DataLoader train_data1 = DataLoader(txt_dataset, batch_size=100, shuffle=True) # 将 2 个数据作为一个batchfor im, label in train_data1: print(label) break (&apos;JOHN&apos;, &apos;2009&apos;, &apos;7&apos;, &apos;WRIGHT&apos;, &apos;NELSON&apos;, &apos;IS&apos;, &apos;HOUSE&apos;, &apos;TOM&apos;, &apos;8275&apos;, &apos;MAID&apos;, &apos;AH&apos;, &apos;RESIST&apos;, &apos;SALESDCMOBILEADSCOM&apos;, &apos;ONGC&apos;, &apos;AUR&apos;, &apos;BIG&apos;, &apos;THE&apos;, &apos;OF&apos;, &apos;YOU&apos;, &apos;UP&apos;, &apos;BUTTE&apos;, &apos;YOU&apos;, &apos;CROFT&apos;, &apos;SHIPPING&apos;, &apos;PINE&apos;, &apos;ENEMIES&apos;, &apos;816527&apos;, &apos;18&apos;, &apos;ASSOCIATE&apos;, &apos;DEPOSITPHOTOS&apos;, &apos;KANTOR&apos;, &apos;IS&apos;, &apos;2008&apos;, &apos;APPLE&apos;, &apos;GEBRUIK&apos;, &apos;SWAYAMVARA&apos;, &apos;IN&apos;, &apos;SHORT&apos;, &apos;COFFEE&apos;, &apos;2010&apos;, &apos;55&apos;, &apos;ROGEN&apos;, &apos;TOO&apos;, &apos;RENTON&apos;, &apos;SHORT&apos;, &apos;IS&apos;, &apos;FINAL&apos;, &apos;AWARD&apos;, &apos;AND&apos;, &apos;KURT&apos;, &apos;VS&apos;, &apos;HUDGENS&apos;, &apos;CROFT&apos;, &apos;DAY&apos;, &apos;22&apos;, &apos;SWAN&apos;, &apos;WE&apos;, &apos;SEPTEMBER&apos;, &apos;COM&apos;, &apos;YOUR&apos;, &apos;EN&apos;, &apos;STRATEGIC&apos;, &apos;TOTAL&apos;, &apos;SPOTLIGHT&apos;, &apos;GUILDHALL&apos;, &apos;PERFORMANCE&apos;, &apos;FREE&apos;, &apos;CARL&apos;, &apos;FAST&apos;, &apos;103&apos;, &apos;OPENING&apos;, &apos;BANKS&apos;, &apos;150&apos;, &apos;MEDIA&apos;, &apos;ZAPORIZHIA&apos;, &apos;AC&apos;, &apos;VAL&apos;, &apos;STATE&apos;, &apos;SHOAH&apos;, &apos;SRI&apos;, &apos;GREAT&apos;, &apos;CLEARANCE&apos;, &apos;ONE&apos;, &apos;THE&apos;, &apos;HOSTING&apos;, &apos;REWARDS&apos;, &apos;LIFE&apos;, &apos;SBI&apos;, &apos;SCALES&apos;, &apos;SPACE&apos;, &apos;YOU&apos;, &apos;THERE&apos;, &apos;THE&apos;, &apos;FOUND&apos;, &apos;70304&apos;, &apos;SO&apos;, &apos;HELT&apos;, &apos;7749&apos;, &apos;PRICES&apos;, &apos;UFO&apos;) def collate_fn(batch): batch.sort(key=lambda x: len(x[1]), reverse=True) # 将数据集按照 label 的长度从大到小排序 img, label = zip(*batch) # 将数据和 label 配对取出 pad_label = [] lens = [] max_len = len(label[0]) for i in range(len(label)): temp_label = label[i] temp_label += '0' * (max_len - len(label[i])) pad_label.append(temp_label) lens.append(len(label[i])) return img, pad_label, lens # 输出 label 的真实长度 # 使用自定义的 collate_fntrain_data3 = DataLoader(txt_dataset, 10, True, collate_fn=collate_fn)im, label, lens = next(iter(train_data3)) im (&apos;2292_1.png&apos;, &apos;857_23.png&apos;, &apos;2433_6.png&apos;, &apos;2539_1.png&apos;, &apos;2314_9.png&apos;, &apos;5142_4.png&apos;, &apos;2497_6.png&apos;, &apos;325_16.png&apos;, &apos;2202_1.png&apos;, &apos;940_1.png&apos;) label [&apos;HAZELMERR&apos;, &apos;PORTABLE0&apos;, &apos;PRIVATE00&apos;, &apos;827500000&apos;, &apos;HERE00000&apos;, &apos;BALE00000&apos;, &apos;VIEW00000&apos;, &apos;HOW000000&apos;, &apos;FOR000000&apos;, &apos;300000000&apos;] lens [9, 8, 7, 4, 4, 4, 4, 3, 3, 2] 可以看到一个 batch 中所有的 label 都从长到短进行排列，同时短的 label 都被补长了，所以使用 collate_fn 能够非常方便的处理一个 batch 中的数据，一般情况下，没有特别的要求，使用 pytorch 中内置的 collate_fn 就可以满足要求了 学习率衰减在 pytorch 中学习率衰减非常方便，使用 torch.optim.lr_scheduler，更多的信息可以直接查看文档 也可以用下面这种方式来做学习率衰减，更加直观 import numpy as npimport torchfrom torch import nnimport torch.nn.functional as Ffrom torch.autograd import Variable class Net(nn.Module): def __init__(self): pass def forward(self,): pass net = Net() # optimizer = torch.optim.SGD(net,lr=0.01, weight_decay=1e-4) 可以通过 optimizer.param_groups来得到所有参数组和其对应的属性, 参数组是什么意思呢？就是我们可以将模型的参数分成几个组，每个组定义一个学习率，这里比较复杂，一般来讲如果不做特别修改，就只有一个参数组.这个参数组是一个字典，里面有很多属性，比如学习率，权重衰减等等，我们可以访问以下 # print('learning rate: &#123;&#125;'.format(optimizer.param_groups[0]['lr']))# print('weight decay: &#123;&#125;'.format(optimizer.param_groups[0]['weight_decay'])) # learning rate: 0.01# weight decay: 0.0001 # 可以通过修改这个属性来改变我们训练过程中的学习率# optimizer.param_groups[0]['lr'] = 1e-5# 为了防止有多个参数组，我们可以使用一个循环# for param_group in optimizer.param_groups: # param_group['lr'] = 1e-1 # 在任意位置改变学习率def set_learning_rate(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lr 批标准化# 实现一维的情况import torchdef simple_batch_norm_1d(x, gamma, beta): eps = 1e-5 x_mean = torch.mean(x, dim=0, keepdim=True) # 保留纬度进行 broadcast x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True) x_hat = (x - x_mean) / torch.sqrt(x_var + eps) return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean) # 验证对于任何输入会不会被标准化x = torch.arange(15, dtype=torch.float).view(5, 3)gamma = torch.ones(x.shape[1])beta = torch.zeros(x.shape[1])print('before bn:')print(x)y = simple_batch_norm_1d(x, gamma, beta)print('after bn:')print(y) before bn: tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.], [12., 13., 14.]]) after bn: tensor([[-1.4142, -1.4142, -1.4142], [-0.7071, -0.7071, -0.7071], [ 0.0000, 0.0000, 0.0000], [ 0.7071, 0.7071, 0.7071], [ 1.4142, 1.4142, 1.4142]]) 可以看到这里一共是 5 个数据点，三个特征，每一列表示一个特征的不同数据点，使用批标准化之后，每一列都变成了标准的正态分布 这个时候会出现一个问题，就是测试的时候该使用批标准化吗？ 答案是肯定的，因为训练的时候使用了，而测试的时候不使用肯定会导致结果出现偏差，但是测试的时候如果只有一个数据集，那么均值不就是这个值，方差为 0 吗？这显然是随机的，所以测试的时候不能用测试的数据集去算均值和方差，而是用训练的时候算出的移动平均均值和方差去代替 下面我们实现以下能够区分训练状态和测试状态的批标准化方法 def batch_norm_1d(x, gamma, beta, is_training, moving_mean, moving_var, moving_momentum=0.1): eps = 1e-5 x_mean = torch.mean(x, dim=0, keepdim=True) # 保留维度进行 broadcast x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True) if is_training: x_hat = (x - x_mean) / torch.sqrt(x_var + eps) moving_mean[:] = moving_momentum * moving_mean + (1. - moving_momentum) * x_mean moving_var[:] = moving_momentum * moving_var + (1. - moving_momentum) * x_var else: x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps) return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean) 从上面可以看到，我们自己实现了 2 维情况的批标准化，对应于卷积的 4 维情况的标准化是类似的，只需要沿着通道的维度进行均值和方差的计算，但是我们自己实现批标准化是很累的，pytorch 当然也为我们内置了批标准化的函数，一维和二维分别是 torch.nn.BatchNorm1d() 和 torch.nn.BatchNorm2d()，不同于我们的实现，pytorch 不仅将 $\\gamma$ 和 $\\beta$ 作为训练的参数，也将 moving_mean 和 moving_var 也作为参数进行训练 正则化使用正则化之后，更新参数变为：$$p_j \\rightarrow p_j - \\eta (\\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j) = p_j - \\eta \\frac{\\partial loss}{\\partial p_j} - 2 \\eta \\lambda p_j$$如果想在随机梯度下降法中使用正则项，或者说权重衰减，torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4) 就可以了，这个 weight_decay 系数就是上面公式中的 $\\lambda$，非常方便 TensorBoard 可视化tensorboard 是 tensorflow 中非常好用的可视化工具，使用这个工具github 可以在 pytorch 中实现可视化. 需要安装以下模块 pip install tensorflow pip install tensorboradX from tensorboardX import SummaryWriter writer = SummaryWriter() # ================ ֵ使用 tensorboard =============== # writer.add_scalars('Loss', &#123;'train': train_loss / len(train_data), 'valid': valid_loss / len(valid_data)&#125;, epoch) # writer.add_scalars('Acc', &#123;'train': train_acc / len(train_data), 'valid': valid_acc / len(valid_data)&#125;, epoch) # =================================================","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"经典卷积神经网络-DenseNet","slug":"classic-DenseNet","date":"2019-05-28T07:16:17.093Z","updated":"2019-05-28T07:17:45.988Z","comments":true,"path":"2019/05/28/classic-DenseNet/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/28/classic-DenseNet/","excerpt":"经典卷积神经网络 DenseNet 结构与实现","text":"经典卷积神经网络 DenseNet 结构与实现 DenseNet网络结构 代码实现# DenseNet 主要由 dense block 构成， 实现 dense blockimport numpy as npimport torchfrom torch import nnfrom torch.autograd import Variablefrom torchvision.datasets import CIFAR10 # 先定义卷积块def conv_block(in_channel, out_channel): layer = nn.Sequential( nn.BatchNorm2d(in_channel), nn.ReLU(True), nn.Conv2d(in_channel, out_channel, 3, padding=1, bias=False) ) return layer # 定义 dense block, dense block 将每次的卷积的输出称为 growth_rate, 因为如果输入是 in_channel, 有 n 层， 那么输出就是# in_channel + n * growth_rateclass Dense_block(nn.Module): def __init__(self, in_channel, growth_rate, num_layers): super(Dense_block, self).__init__() block = [] channel = in_channel for i in range(num_layers): block.append(conv_block(channel, growth_rate)) channel += growth_rate self.net = nn.Sequential(*block) def forward(self, x): for layer in self.net: out = layer(x) x = torch.cat((out, x), dim=1) return x # 验证输出的 channel 是否正确test_net = Dense_block(3, 12, 3)test_x = Variable(torch.zeros(1, 3, 96, 96))print('input shape: &#123;&#125; x &#123;&#125; x &#123;&#125;'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))test_y = test_net(test_x)print('output shape: &#123;&#125; x &#123;&#125; x &#123;&#125;'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3])) input shape: 3 x 96 x 96 output shape: 39 x 96 x 96 除了 dense block 之外， DenseNet 还有一个模块叫过渡层(transition block)，因为 DenseNet 会不断地对纬度进行拼接， 所以当层数很高的时候，输出的通道数就会越变越大，参数和计算量也会越来越大，为了避免这个问题，需要引入过渡层将输出通道降低下来，同时也将输入的长宽减半，这个过渡层可以使用 1x1 的卷积 # 定义 transition blockdef transition(in_channel, out_channel): trans_layer = nn.Sequential( nn.BatchNorm2d(in_channel), nn.ReLU(True), nn.Conv2d(in_channel, out_channel, 1), nn.AvgPool2d(2, 2) ) return trans_layer # 验证过渡层 transition block 是否正确test_net = transition(3, 12)test_x = Variable(torch.zeros(1, 3, 96, 96))print('input shape: &#123;&#125; x &#123;&#125; x &#123;&#125;'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))test_y = test_net(test_x)print('output shape: &#123;&#125; x &#123;&#125; x &#123;&#125;'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3])) input shape: 3 x 96 x 96 output shape: 12 x 48 x 48 # 定义 DenseNetclass DenseNet(nn.Module): def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16]): super(DenseNet, self).__init__() self.block1 = nn.Sequential( nn.Conv2d(in_channel, 64, 7, 2, 3), nn.BatchNorm2d(64), nn.ReLU(True), nn.MaxPool2d(3, 2, padding=1) ) channels = 64 block = [] for i, layers in enumerate(block_layers): block.append(Dense_block(channels, growth_rate, layers)) channels += layers * growth_rate if i != len(block_layers) - 1: block.append(transition(channels, channels // 2)) # 通过 transiton 层数大小减半，通道数减半 channels = channels // 2 self.block2 = nn.Sequential(*block) self.block2.add_module('bn', nn.BatchNorm2d(channels)) self.block2.add_module('relu', nn.ReLU(True)) self.block2.add_module('avg_pool', nn.AvgPool2d(3)) self.classifier = nn.Linear(channels, num_classes) def forward(self, x): x = self.block1(x) x = self.block2(x) x = x.view(x.shape[0], -1) x = self.classifier(x) return x # 测试 DenseNettest_net = DenseNet(3, 10)test_x = Variable(torch.zeros(1, 3, 96, 96))test_y = test_net(test_x)print('output: &#123;&#125;'.format(test_y.shape)) output: torch.Size([1, 10])","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"经典卷积神经网络-ResNet","slug":"classic-ResNet","date":"2019-05-27T08:23:35.439Z","updated":"2019-05-27T08:24:38.492Z","comments":true,"path":"2019/05/27/classic-ResNet/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/27/classic-ResNet/","excerpt":"经典卷积神经网络 ResNet 结构与实现","text":"经典卷积神经网络 ResNet 结构与实现 ResNet网络结构ResNet 通过引入跨层连接，解决了梯度回传消失的问题。在参数几乎一样的情况下，增加模型通道数，采用 1x1 卷积核 代码实现参差网络的结构就是上面这种参差块的堆叠，实现一个 residual block import numpy as npimport torchfrom torch import nnimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torchvision.datasets import CIFAR10 def conv3x3(in_channel, out_channel, stride=1): return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False) class Residual_block(nn.Module): def __init__(self, in_channel, out_channel, same_shape=True): super(Residual_block, self).__init__() self.same_shape = same_shape stride = 1 if self.same_shape else 2 self.conv1 = conv3x3(in_channel, out_channel, stride=stride) self.bn1 = nn.BatchNorm2d(out_channel) self.conv2 = conv3x3(out_channel, out_channel) self.bn2 = nn.BatchNorm2d(out_channel) if not self.same_shape: self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride) def forward(self, x): out = self.conv1(x) out = F.relu(self.bn1(out), True) out = self.conv2(out) out = F.relu(self.bn2(out), True) if not self.same_shape: x = self.conv3(x) return F.relu(x+out, True) # 测试 redidual block 的输出# 输入输出形状相同test_net = Residual_block(32, 32)test_x = Variable(torch.zeros(1, 32, 96, 96))print('input: &#123;&#125;'.format(test_x.shape))test_y = test_net(test_x) print('output: &#123;&#125;'.format(test_y.shape)) input: torch.Size([1, 32, 96, 96]) output: torch.Size([1, 32, 96, 96]) # 输入输出形状不同test_net = Residual_block(3, 32, False) test_x = Variable(torch.zeros(1, 3, 96, 96)) print('input: &#123;&#125;'.format(test_x.shape)) test_y = test_net(test_x) print('output: &#123;&#125;'.format(test_y.shape)) input: torch.Size([1, 3, 96, 96]) output: torch.Size([1, 32, 48, 48]) # 实现 ResNet class ResNet(nn.Module): def __init__(self, in_channels, num_classes, verbose=False): super(ResNet, self).__init__() self.verbose = verbose self.block1 = nn.Conv2d(in_channels, 64, 7, 2) self.block2 = nn.Sequential( nn.MaxPool2d(3, 2), Residual_block(64, 64), Residual_block(64,64) ) self.block3 = nn.Sequential( Residual_block(64, 128, False), Residual_block(128, 128) ) self.block4 = nn.Sequential( Residual_block(128, 256, False), Residual_block(256, 256) ) self.block5 = nn.Sequential( Residual_block(256, 512, False), Residual_block(512, 512), nn.AvgPool2d(3) ) self.classifier = nn.Linear(512, num_classes) def forward(self, x): x = self.block1(x) if self.verbose: print('block 1 output: &#123;&#125;'.format(x.shape)) x = self.block2(x) if self.verbose: print('block 2 output: &#123;&#125;'.format(x.shape)) x = self.block3(x) if self.verbose: print('block 3 output: &#123;&#125;'.format(x.shape)) x = self.block4(x) if self.verbose: print('block 4 output: &#123;&#125;'.format(x.shape)) x = self.block5(x) if self.verbose: print('block 5 output: &#123;&#125;'.format(x.shape)) x = x.view(x.shape[0], -1) x = self.classifier(x) return x # 模型测试test_net = ResNet(3, 10, True) test_x = Variable(torch.zeros(1, 3, 96, 96)) test_y = test_net(test_x) print('output: &#123;&#125;'.format(test_y.shape)) block 1 output: torch.Size([1, 64, 45, 45]) block 2 output: torch.Size([1, 64, 22, 22]) block 3 output: torch.Size([1, 128, 11, 11]) block 4 output: torch.Size([1, 256, 6, 6]) block 5 output: torch.Size([1, 512, 1, 1]) output: torch.Size([1, 10])","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"经典卷积神经网络-GoogleNet","slug":"classic-GoogleNet","date":"2019-05-24T07:21:10.833Z","updated":"2019-05-24T11:22:12.426Z","comments":true,"path":"2019/05/24/classic-GoogleNet/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/24/classic-GoogleNet/","excerpt":"经典卷积神经网络 GoogleNet 结构与实现","text":"经典卷积神经网络 GoogleNet 结构与实现 GoogleNet网络结构采用了非常有效的 inception 模块， 得到了比 VGG 更深的网络结构， 但是却比 VGG 的参数少，因为去掉了后面的全连接层，参数大大减少，提高了计算效率 Inception 模块一个 Inception 模块有四条并行线路。 一个 1x1 的卷积，一个小的感受野进行卷积特征提取。 一个 1x1 的卷积，加上一个 3x3 的卷积， 1x1 的卷积降低输入的特征通道，减少参数计算量，然后接一个 3x3 的卷积做一个较大感受野的卷积。 一个 1x1 的卷积加上一个 5x5 的卷积， 作用和第二个通道一样。 一个 3x3 的最大值池化，加上 1x1 的卷积，最大值池化改变输入的特征排列， 1x1 的卷积进行特征提取。最后将四个并行线路得到的特征在通道这个纬度上拼接在一起 代码实现import numpy as npimport torchfrom torch import nnfrom torch.autograd import Variablefrom torchvision.datasets import CIFAR10 # 定义一个卷积加一个 relu 激活函数和一个 batchnorm 作为一个基本的层结构def conv_relu(in_channel, out_channel, kernel, stride=1, padding=0): layer = nn.Sequential( nn.Conv2d(in_channel, out_channel, kernel, stride, padding), nn.BatchNorm2d(out_channel, eps=1e-3), nn.ReLU(True) ) return layer # 定义 inception 机构class inception(nn.Module): def __init__(self, in_channel, out1_1, out2_1, out2_3, out3_1, out3_5, out4_1): super(inception, self).__init__() # 定义第一条线路 self.branch1x1 = conv_relu(in_channel, out1_1, 1) # 定义第二条线路 self.branch3x3 = nn.Sequential( conv_relu(in_channel, out2_1, 1), conv_relu(out2_1, out2_3, 3, padding=1) ) # 定义第三条线路 self.branch5x5 = nn.Sequential( conv_relu(in_channel, out3_1, 1), conv_relu(out3_1, out3_5, 5, padding=2) ) # 定义第四条线路 self.branch_pool = nn.Sequential( nn.MaxPool2d(3, stride=1, padding=1), conv_relu(in_channel, out4_1, 1) ) def forward(self, x): f1 = self.branch1x1(x) f2 = self.branch3x3(x) f3 = self.branch5x5(x) f4 = self.branch_pool(x) output = torch.cat((f1, f2, f3, f4),dim=1) return output # 测试 inception 模块test_net = inception(3, 64, 48, 64, 64, 96, 32)test_x = Variable(torch.zeros(1, 3, 96, 96))print('input shape: &#123;&#125; x &#123;&#125; x &#123;&#125;'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))test_y = test_net(test_x) print('output shape: &#123;&#125; x &#123;&#125; x &#123;&#125;'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3])) input shape: 3 x 96 x 96 output shape: 256 x 96 x 96 可以看到，输入经过 inception 模块之后， 大小没有变化，通道的纬度变多了， 下面定义 GoogleNet， GoogleNet相当于多个 inception 模块的串联, 原论文中使用多个输出来解决梯度消失的问题， 这里我们只定义一个简单版本的 GoogleNet，简化为一个输出 class GoogleNet(nn.Module): def __init__(self, in_channel, num_classes, verbose=False): super(GoogleNet, self).__init__() self.verbose = verbose self.block1 = nn.Sequential( conv_relu(in_channel, out_channel=64, kernel=7, stride=2, padding=3), nn.MaxPool2d(3, 2) ) self.block2 = nn.Sequential( conv_relu(64, 64, kernel=1), conv_relu(64, 192, kernel=3, padding=1), nn.MaxPool2d(3, 2) ) self.block3 = nn.Sequential( inception(192, 64, 96, 128, 16, 32, 32), inception(256, 128, 128, 192, 32, 96, 64), nn.MaxPool2d(3, 2) ) self.block4 = nn.Sequential( inception(480, 192, 96, 208, 16, 48, 64), inception(512, 160, 112, 224, 24, 64, 64), inception(512, 128, 128, 256, 24, 64, 64), inception(512, 112, 144, 288, 32, 64, 64), inception(528, 256, 160, 320, 32, 128, 128), nn.MaxPool2d(3, 2) ) self.block5 = nn.Sequential( inception(832, 256, 160, 320, 32, 128, 128), inception(832, 384, 182, 384, 48, 128, 128), nn.AvgPool2d(2) ) self.classifer = nn.Linear(1024, num_classes) def forward(self, x): x = self.block1(x) if self.verbose: print('block 1 output: &#123;&#125;'.format(x.shape)) x = self.block2(x) if self.verbose: print('block 2 output: &#123;&#125;'.format(x.shape)) x = self.block3(x) if self.verbose: print('block 3 output: &#123;&#125;'.format(x.shape)) x = self.block4(x) if self.verbose: print('block 4 output: &#123;&#125;'.format(x.shape)) x = self.block5(x) if self.verbose: print('block 5 output: &#123;&#125;'.format(x.shape)) x = x.view(x.shape[0], -1) x = self.classifer(x) return x # 验证 GoogleNettest_net = GoogleNet(3, 10, True) test_x = Variable(torch.zeros(1, 3, 96, 96)) test_y = test_net(test_x) print('output: &#123;&#125;'.format(test_y.shape)) block 1 output: torch.Size([1, 64, 23, 23]) block 2 output: torch.Size([1, 192, 11, 11]) block 3 output: torch.Size([1, 480, 5, 5]) block 4 output: torch.Size([1, 832, 2, 2]) block 5 output: torch.Size([1, 1024, 1, 1]) output: torch.Size([1, 10]) 可以看到输出的尺寸不断减小， 通道纬度不断增加, 实现了简易版的 GoogleNet","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"经典卷积神经网络-VGGNet","slug":"classic-VGGNet","date":"2019-05-21T08:12:43.468Z","updated":"2019-05-21T08:14:14.978Z","comments":true,"path":"2019/05/21/classic-VGGNet/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/21/classic-VGGNet/","excerpt":"经典卷积网络 VGGNet 结构与实现","text":"经典卷积网络 VGGNet 结构与实现 VGGNet网络结构 代码实现import numpy as npimport torchfrom torch import nnfrom torch.autograd import Variablefrom torchvision.datasets import CIFAR10 # VGG 使用了很多层 3*3 的卷积 然后再使用一个最大池化层，这个模块被使用了很多次，定义一个 vgg 的 block 来实现这个结构def vgg_block(num_convs, in_channels, out_channels): # 定义第一层 net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # 定义后面很多层 for i in range(num_convs - 1): net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)) net.append(nn.ReLU(True)) net.append(nn.MaxPool2d(2, 2)) # 定义池化层 return nn.Sequential(*net) # 打印模型，查看结构block_demo = vgg_block(3, 64, 128)block_demo Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (5): ReLU(inplace) (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) # 定义输入为 (1， 64， 300， 300)input_demo = Variable(torch.zeros(1, 64, 300, 300))output_demo = block_demo(input_demo)print(output_demo.shape) torch.Size([1, 128, 150, 150]) 可以看到输出变成了(1, 128, 150, 150)， 可以看到经过这个vgg block， 输入大小减半， 通道数变成了128 # 定义一个函数对这个 vgg block 进行堆叠def vgg_stack(nums_convs, channels): net = [] for n, c in zip(nums_convs, channels): in_c = c[0] out_c = c[1] net.append(vgg_block(n, in_c, out_c)) return nn.Sequential(*net)# 定义一个简单的 VGG 结构vgg_net = vgg_stack((1, 1, 2, 2, 2), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))vgg_net Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (1): Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (2): Sequential( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (3): Sequential( (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (4): Sequential( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace) (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) ) 上诉网络结构中有 5 个最大值池化，说明图片大小会减少 5 倍， 验证如下 test_x = Variable(torch.zeros(1, 3, 256, 256))test_y = vgg_net(test_x)print(test_y.shape) torch.Size([1, 512, 8, 8]) 可以看到图片大小减少了 $2^{5}$ 倍, 最后再加上几层全连接， 就能够得到我们想要的分类输出 # 定义 VGGNet 模型class VGGNet(nn.Module): def __init__(self): super(vgg, self).__init__() self.feature = vgg_net self.fc = nn.Sequential( nn.Linear(512, 100), nn.ReLU(True), nn.Linear(100, 10) ) def forward(self, x): x = self.feature(x) x = x.view(x.shape[0], -1) x = self.fc(x) return x","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"经典卷积神经网络-AlexNet","slug":"classic-AlexNet","date":"2019-05-20T08:02:07.825Z","updated":"2019-05-21T08:18:55.194Z","comments":true,"path":"2019/05/20/classic-AlexNet/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/20/classic-AlexNet/","excerpt":"经典卷积网络 AlexNet 结构与实现","text":"经典卷积网络 AlexNet 结构与实现 AlexNet网络结构几个卷积池化堆叠后连接介个全链接层，如下图所示 代码实现import torchfrom torch import nnimport numpy as npfrom torch.autograd import Variablefrom torchvision.datasets import CIFAR10 # 用小的数据集进行训练，卷积层设计与模型不完全相同class AlexNet(nn.Module): def __init__(self): super().__init__() # 第一层是 5*5 的卷积 ，输入的 channels 是 3， 输出的 channels 是64， 步长为 1， 没有 padding self.conv1 = nn.Sequential( nn.Conv2d(3, 64, 5), nn.ReLU(True), ) # 第二层是 3*3 的池化， 步长为 2， 没有 padding self.max_pool1 = nn.MaxPool2d(3, 2) #第三层是 5*5 的卷积， 输入的 channels 是 64，输出的 channels 是 64， 步长为 1， 没有 padding self.conv2 = nn.Sequential( nn.Conv2d(64, 64, 5, 1), nn.ReLU(True), ) # 第四层是 3*3 的池化， 步长为 2， 没有 padding self.max_pool2 = nn.MaxPool2d(3, 2) # 第五层是全连接层, 输入是 1204， 输出是 384 self.fc1 = nn.Sequential( nn.Linear(1024, 384), nn.ReLU(True) ) # 第六层是全连接层, 输入是 384, 输出是 192 self.fc2 = nn.Linear(384, 192) # 第七层是全连接层， 输入是 192， 输出是 10 self.fc3 = nn.Linear(192, 10) def forward(self, x): x = self.conv1(x) x = self.max_pool1(x) x = self.conv2(x) x = self.max_pool2(x) # 将矩阵拉平 x = x.view(x.shape[0], -1) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x alexnet = AlexNet() # 查看网络结构alexnet AlexNet( (conv1): Sequential( (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU(inplace) ) (max_pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Sequential( (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU(inplace) ) (max_pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Sequential( (0): Linear(in_features=1024, out_features=384, bias=True) (1): ReLU(inplace) ) (fc2): Linear(in_features=384, out_features=192, bias=True) (fc3): Linear(in_features=192, out_features=10, bias=True) ) # 验证网络结构是否正确， 输入一张 32*32 的图片 # 定义输入为(1, 3, 32, 32)input_demo = Variable(torch.zeros(1, 3, 32, 32))output_demo = alexnet(input_demo)print(output_demo.shape) torch.Size([1, 10])","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"百宝箱","slug":"kinds","date":"2019-05-17T10:55:02.081Z","updated":"2019-05-17T10:58:32.364Z","comments":true,"path":"2019/05/17/kinds/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/17/kinds/","excerpt":"记录自己平时用到的一些小知识","text":"记录自己平时用到的一些小知识 返回本机IP地址 https://utool.fun/ip/ https://api.ip.la/ https://httpbin.org/ip","categories":[{"name":"其他","slug":"其他","permalink":"http://lucas0625.github.io/blog/categories/其他/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://lucas0625.github.io/blog/tags/其他/"}]},{"title":"Linux 常用操作","slug":"linux","date":"2019-05-17T01:26:51.213Z","updated":"2019-05-17T10:58:50.758Z","comments":true,"path":"2019/05/17/linux/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/17/linux/","excerpt":"记录平时用到的一些linux命令","text":"记录平时用到的一些linux命令 查看当前目录下文件夹的大小du -h –max-depth=1 分割，合并文件 分割: sed -n ‘1, 100p’ file &gt; file_part1 合并: cat file1 file2 &gt; file 替换sed -i “s/ //g” test 替换空格为空 文件编码 查看文件编码: file filename 转换文件编码: iconv -f 原编码 -t 新编码 原文件 -o 新文件 文件查找grep -rn ‘words’ * : 表示当前目录所有文件，也可以是某个文件名-r 是递归查找-n 是显示行号-R 查找所有文件包含子目录-i 忽略大小写 统计文件大小 wc -l file 计算文件行数 wc -w file 计算文件中的单词数 wc -c file 计算文件中的字符数 文件操作 cp sourcefile destfile 文件拷贝 mv oldname newname 重命名或者文件移动 rm file 删除文件 grep ‘pattern’ file 在文件内搜索字符串 cut -b colnum file 指定欲显示的文件内容范围 cat file 输出文件内容 file somefile 得到文件类型","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"vim 常用技巧","slug":"vim","date":"2019-05-17T01:26:29.091Z","updated":"2019-05-17T01:33:37.304Z","comments":true,"path":"2019/05/17/vim/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/17/vim/","excerpt":"记录平时用到的一些vim命令","text":"记录平时用到的一些vim命令 视图模式下常用设置 set invlist 显示不可见字符 set nu / nonu 显示行号，不显示行号 s/old/new/g 替换old为new, 1, 100s/old/new/g 将1-100行之间的old替换为new set hlsearch 查找高亮 set nohlsearch 取消查找高亮 打开多个编辑器vim -O2 file1 file2 打开两个vim编辑器ctrl+w+l/h 左右切换光标 强制写入tabcrtl + v + i 强制写入\\t，即^I","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"CNN 基础","slug":"CNN-base","date":"2019-05-16T06:21:13.849Z","updated":"2019-05-20T07:58:23.684Z","comments":true,"path":"2019/05/16/CNN-base/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/16/CNN-base/","excerpt":"本文介绍了CNN基础。","text":"本文介绍了CNN基础。 卷积概念 卷积模块卷积卷积在 PyTorch 中有两种方式， 一种是torch.nn.Conv2d(), 一种是torch.nn.functional.conv2d()， 这两种形式本质上都是使用一个卷积操作。这两种形式的卷积对于输入要求是一样的，都是输入torch.autograd.Variable()，大小是(batch, channel, H, W), 其中 batch 表示输入的一批数据的数目 channel 表示输入的通道数，一般一张彩色图片的通道数为3，灰度图是1 H 表示输入图片的高度 W 表示输入图片的宽度 # 实际操作import numpy as np import torch from torch import nn from torch.autograd import Variable import torch.nn.functional as F from PIL import Image import matplotlib.pyplot as plt %matplotlib inline im = Image.open('./cat.png').convert('L') # 读入一张灰度图的图片im = np.array(im, dtype='float32') # 将其转换为一个矩阵im array([[122., 122., 121., ..., 93., 90., 88.], [124., 124., 123., ..., 96., 93., 91.], [126., 126., 125., ..., 100., 97., 95.], ..., [161., 161., 161., ..., 0., 0., 0.], [160., 160., 161., ..., 0., 0., 0.], [160., 160., 160., ..., 0., 0., 0.]], dtype=float32) # 可视化图片plt.imshow(im.astype('uint8'), cmap='gray') &lt;matplotlib.image.AxesImage at 0x1183e2518&gt; im.shape (224, 224) # 将图片矩阵转化为 pytorh tensor， 并适配卷积输入的要求im = torch.from_numpy(im.reshape((1, 1, im.shape[0], im.shape[1]))) im tensor([[[[122., 122., 121., ..., 93., 90., 88.], [124., 124., 123., ..., 96., 93., 91.], [126., 126., 125., ..., 100., 97., 95.], ..., [161., 161., 161., ..., 0., 0., 0.], [160., 160., 161., ..., 0., 0., 0.], [160., 160., 160., ..., 0., 0., 0.]]]]) # 定义一个算子，对其进行轮廓检测# 使用 nn.Conv2d'''torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')'''conv1 = nn.Conv2d(1, 1, 3, bias=True) # 定义卷积sobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # 定义轮廓检测算子sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出conv1.weight.data = torch.from_numpy(sobel_kernel) # 给卷积的 kernel 赋值edge1 = conv1(Variable(im)) # 作用在图片上edge1 = edge1.data.squeeze().numpy() # 将输出转换为图片的格式 # 可视化边缘检测之后的结果plt.imshow(edge1, cmap='gray') &lt;matplotlib.image.AxesImage at 0x119dce9e8&gt; # 使用 F.conv2dsobel_kernek = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # 定义轮廓检测算子sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出weight = Variable(torch.from_numpy(sobel_kernel))edge2 = F.conv2d(Variable(im), weight) # 作用在图片上edge2 = edge2.data.squeeze().numpy() # 将输出转化为图片的格式plt.imshow(edge2, cmap='gray') &lt;matplotlib.image.AxesImage at 0x119c46da0&gt; 两种卷积方式的区别 nn.Conv2d() 相当于直接定义了一层卷积网络结构, 默认定义一个随机初始化的 weight, 需要修改的话，取出其中的值对其修改 torch.nn.functional.conv2d() 相当于定义了一个卷积的操作，需要再额外定义一个 weight ，weight 也必须是一个 Varibale 实际使用中，基本都使用 nn.Conv2d() 池化模块池化池化层 可以降低矩阵的大小，非常好地提升计算效率，池化层没有参数，池化的方式有很多种。 最大值池化 均值池化 在卷积网络中一般使用最大值池化。PyTorch 中两种池化方式， nn.MaxPool2d() torch.nn.functional.max_pool2d() 参数和卷积参数一致 # 使用 nn.MaxPool2dpool1 = nn.MaxPool2d(2, 2)print('before max pool, image shape: &#123;&#125; x &#123;&#125;'.format(im.shape[2], im.shape[3]))small_im1 = pool1(Variable(im))small_im1 = small_im1.data.squeeze().numpy()print('after max pool, image shape: &#123;&#125; x &#123;&#125; '.format(small_im1.shape[0], small_im1.shape[1])) before max pool, image shape: 224 x 224 after max pool, image shape: 112 x 112 # 图片大小减少了一般，看一下图像的表现plt.imshow(small_im1, cmap='gray') &lt;matplotlib.image.AxesImage at 0x119e82780&gt; # 第二种方式 F.max_pool2dprint('before max pool, image shape: &#123;&#125; x &#123;&#125;'.format(im.shape[2], im.shape[3]))small_im2 = F.max_pool2d(Variable(im), 2, 2)small_im2 = small_im2.data.squeeze().numpy()print('after max pool, image shape: &#123;&#125; x &#123;&#125; '.format(small_im1.shape[0], small_im1.shape[1]))plt.imshow(small_im2, cmap='gray') before max pool, image shape: 224 x 224 after max pool, image shape: 112 x 112 &lt;matplotlib.image.AxesImage at 0x11a074898&gt; 两种方式结果一样，一般使用 nn.MaxPool2d()","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/blog/tags/Pytorch/"}]},{"title":"多层神经网络","slug":"multilayer-neural-network","date":"2019-05-14T06:20:23.740Z","updated":"2019-05-14T06:29:26.450Z","comments":true,"path":"2019/05/14/multilayer-neural-network/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/14/multilayer-neural-network/","excerpt":"PyTorch 实现多层神经网络","text":"PyTorch 实现多层神经网络 多层神经网络概念demo 多层神经网络实现手动构建模型import torchimport numpy as npfrom torch import nnfrom torch.autograd import Variableimport torch.nn.functional as Fimport matplotlib.pyplot as plt%matplotlib inline def plot_decision_boundary(model, x, y): # Set min and max values and give it some padding x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1 y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1 h = 0.01 # Generate a grid of points with distance h between them xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # Predict the function value for the whole grid Z = model(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Plot the contour and training examples plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.ylabel('x2') plt.xlabel('x1') plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral) # 这次仍处理二分类问题，但是比之前的logistic回归更加复杂np.random.seed(1)m = 400 # 样本数量N = int(m/2) # 每一类的点的个数D = 2 # 纬度x = np.zeros((m, D))y = np.zeros((m, 1), dtype='uint8') # label 向量， 0 表示红色， 1 表示蓝色a = 4 for j in range(2): ix = range(N*j,N*(j+1)) t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius x[ix] = np.c_[r*np.sin(t), r*np.cos(t)] y[ix] = j plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral) &lt;matplotlib.collections.PathCollection at 0x120b352b0&gt; # 先尝试使用 logistic 回归解决这个问题x = torch.from_numpy(x).float()y = torch.from_numpy(y).float()w = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))optimizer = torch.optim.SGD([w, b], 1e-1)def logistic_regression(x): return torch.mm(x, w) + bcriterion = nn.BCEWithLogitsLoss() for e in range(100): out = logistic_regression(Variable(x)) loss = criterion(out, Variable(y)) optimizer.zero_grad() loss.backward() optimizer.step() if (e + 1) % 20 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 20, loss: 0.7704809308052063 epoch: 40, loss: 0.6776716113090515 epoch: 60, loss: 0.6732792854309082 epoch: 80, loss: 0.6731553673744202 epoch: 100, loss: 0.6731488108634949 def plot_logistic(x): x = Variable(torch.from_numpy(x).float()) out = torch.sigmoid(logistic_regression(x)) out = (out &gt; 0.5) * 1 return out.data.numpy() plot_decision_boundary(lambda x: plot_logistic(x), x.numpy(), y.numpy())plt.title('logistic regression') Text(0.5,1,&apos;logistic regression&apos;) 可以看到 logistic 并不能很好的区分开，因为 logistic 是线性分类器， 现在采用 神经网络模型 # 定义两层神经网络w1 = nn.Parameter(torch.randn(2, 4) * 0.01) # 隐藏神经元个数 2b1 = nn.Parameter(torch.zeros(4))w2 = nn.Parameter(torch.randn(4, 1) * 0.01)b2 = nn.Parameter(torch.zeros(1))# 定义模型def two_netword(x): x1 = torch.mm(x, w1) + b1 x1 = torch.tanh(x1) # 使用 PyTorch 自带的 tanh 激活函数 x2 = torch.mm(x1, w2) + b2 return x2optimizer = torch.optim.SGD([w1, w2, b1, b2], 1.)criterion = nn.BCEWithLogitsLoss() # 训练 10000 次for e in range(10000): out = two_netword(Variable(x)) loss = criterion(out, Variable(y)) optimizer.zero_grad() loss.backward() optimizer.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.2847880423069 epoch: 2000, loss: 0.27225854992866516 epoch: 3000, loss: 0.26519426703453064 epoch: 4000, loss: 0.2597326934337616 epoch: 5000, loss: 0.23362228274345398 epoch: 6000, loss: 0.2255781590938568 epoch: 7000, loss: 0.22193390130996704 epoch: 8000, loss: 0.21951931715011597 epoch: 9000, loss: 0.2176773101091385 epoch: 10000, loss: 0.21617043018341064 def plot_network(x): x = Variable(torch.from_numpy(x).float()) x1 = torch.mm(x, w1) + b1 x1 = torch.tanh(x1) x2 = torch.mm(x1, w2) + b2 out = torch.sigmoid(x2) out = (out &gt; 0.5) * 1 return out.data.numpy() plot_decision_boundary(lambda x: plot_network(x), x.numpy(), y.numpy())plt.title('2 layer network') Text(0.5,1,&apos;2 layer network&apos;) 可以看到， 神经网络能很好的分类这个复杂的数据， 和前面的logistic回归相比，神经网络因为有了激活函数的存在，成了一个非线性分类器，所以神经网络分类的边界更加复杂 自动构建一个完整的 PyTorch 模型手动构建模型对于比较小的模型是可行的，但是对于大模型，比如 100 层的神经网络，这个时候手动定义模型就会非常麻烦，接下来使用 PyTorch 提供的模块来构建模型 使用 Sequential 构建模型# Sequentialseq_net = nn.Sequential( nn.Linear(2, 4), # PyTorch 中的线性层， wx + b nn.Tanh(), nn.Linear(4, 1))# 序列模块可以通过索引访问每一层seq_net[0] # 第一层 Linear(in_features=2, out_features=4, bias=True) # 打印出第一层的权重w0 = seq_net[0].weightprint(w0) Parameter containing: tensor([[-0.5508, 0.2882], [ 0.0028, -0.5731], [-0.1803, 0.5680], [-0.5479, 0.0248]], requires_grad=True) # 通过 parameters 可以取得模型的参数param = seq_net.parameters()# 定义优化器optim = torch.optim.SGD(param, 1.) # 我们训练 10000 次for e in range(10000): out = seq_net(Variable(x)) loss = criterion(out, Variable(y)) optim.zero_grad() loss.backward() optim.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.28492191433906555 epoch: 2000, loss: 0.27243772149086 epoch: 3000, loss: 0.2653190791606903 epoch: 4000, loss: 0.25978708267211914 epoch: 5000, loss: 0.2339320182800293 epoch: 6000, loss: 0.22560137510299683 epoch: 7000, loss: 0.22195379436016083 epoch: 8000, loss: 0.21954353153705597 epoch: 9000, loss: 0.2177024632692337 epoch: 10000, loss: 0.2161943018436432 def plot_seq(x): out = torch.sigmoid(seq_net(Variable(torch.from_numpy(x).float()))).data.numpy() out = (out &gt; 0.5) * 1 return out plot_decision_boundary(lambda x: plot_seq(x), x.numpy(), y.numpy()) plt.title('sequential') Text(0.5,1,&apos;sequential&apos;) 模型保存的两种方式： 将模型结构和参数都保存在一起 只将参数保存下来 # 第一种保存方式，将模型和参数保存在一起torch.save(seq_net, 'save_seq_net.model') # 要保存的模型，保存路径 # 读取保存的模型seq_net1 = torch.load('save_seq_net.model')seq_net1 Sequential( (0): Linear(in_features=2, out_features=4, bias=True) (1): Tanh() (2): Linear(in_features=4, out_features=1, bias=True) ) # 第二种保存方式，只保存参数torch.save(seq_net.state_dict(), 'save_seq_net_params.pth') # 模型的读取, 需要先重新定义一次模型seq_net2 = nn.Sequential( nn.Linear(2, 4), nn.Tanh(), nn.Linear(4, 1))seq_net2.load_state_dict(torch.load('save_seq_net_params.pth'))seq_net2 Sequential( (0): Linear(in_features=2, out_features=4, bias=True) (1): Tanh() (2): Linear(in_features=4, out_features=1, bias=True) ) 第二种模型保存方式相比较第一种可移植性更强 使用 Module 构建模型# 使用 Module 的模版 ， 中文的地方是需要自己定义的，任何复杂的操作都可以在 forward 中定义class 网络名字(nn.Module): def __init__(self, 一些定义的参数): super(网络名字， self).__init__() self.layer1 = nn.Linear(num_input, num_hidden) self.layer2 = nn.Sequential(...) ... 定义需要的网络层 def forward(self, x): # 定义前向传播 x1 = self.layer(x) x2 = self.layer(x) x = x1 + x2 ... return x # 利用上述模版实现前面的神经网络class module_net(nn.Module): def __init__(self, num_input, num_hidden, num_output): super(module_net, self).__init__() self.layer1 = nn.Linear(num_input, num_hidden) self.layer2 = nn.Tanh() self.layer3 = nn.Linear(num_hidden, num_output) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x mo_net = module_net(2, 4, 1) # 访问模型中的某层可以直接通过名字# 第一层l1 = mo_net.layer1print(l1) Linear(in_features=2, out_features=4, bias=True) # 打印第一层的权重print(l1.weight) Parameter containing: tensor([[ 0.5183, 0.2359], [ 0.3867, 0.6806], [ 0.4755, -0.4971], [-0.4460, -0.6820]], requires_grad=True) # 定义优化器optim = torch.optim.SGD(mo_net.parameters(), 1.) # 训练 10000 次for e in range(10000): out = mo_net(Variable(x)) loss = criterion(out, Variable(y)) optim.zero_grad() loss.backward() optim.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.283423513174057 epoch: 2000, loss: 0.2715207040309906 epoch: 3000, loss: 0.26463547348976135 epoch: 4000, loss: 0.2599412798881531 epoch: 5000, loss: 0.25658532977104187 epoch: 6000, loss: 0.2540966272354126 epoch: 7000, loss: 0.25219157338142395 epoch: 8000, loss: 0.2506936192512512 epoch: 9000, loss: 0.2494892179965973 epoch: 10000, loss: 0.2485022097826004 可以看到得到了相同的结果，这种使用梯度下降来优化参数的方法，在深度学习中叫做反向传播算法 接下来，定义一个5层的神经网络 class module_net(nn.Module): def __init__(self, num_input, num_hidden1, num_hidden2, num_hidden3, num_hidden4, num_output): super(module_net, self).__init__() self.layer_activation = nn.Tanh() self.layer_input = nn.Linear(num_input, num_hidden1) self.layer_hidden1 = nn.Linear(num_hidden1, num_hidden2) self.layer_hidden2 = nn.Linear(num_hidden2, num_hidden3) self.layer_hidden3 = nn.Linear(num_hidden3, num_hidden4) self.layer_output = nn.Linear(num_hidden4, num_output) def forward(self, x): x = self.layer_input(x) x = self.layer_activation(x) x = self.layer_hidden1(x) x = self.layer_activation(x) x = self.layer_hidden2(x) x = self.layer_activation(x) x = self.layer_hidden3(x) x = self.layer_activation(x) x = self.layer_output(x) return xmo_net5 = module_net(2, 10, 10, 10, 10, 1)optim = torch.optim.SGD(mo_net5.parameters(), 0.1)# 训练 20000 次for e in range(20000): out = mo_net5(Variable(x)) loss = criterion(out, Variable(y)) optim.zero_grad() loss.backward() optim.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.2958005964756012 epoch: 2000, loss: 0.219310462474823 epoch: 3000, loss: 0.19828572869300842 epoch: 4000, loss: 0.18876363337039948 epoch: 5000, loss: 0.18029095232486725 epoch: 6000, loss: 0.16922634840011597 epoch: 7000, loss: 0.16025283932685852 epoch: 8000, loss: 0.1556335687637329 epoch: 9000, loss: 0.15196143090724945 epoch: 10000, loss: 0.14857107400894165 epoch: 11000, loss: 0.1452614665031433 epoch: 12000, loss: 0.13823415338993073 epoch: 13000, loss: 0.13576728105545044 epoch: 14000, loss: 0.13504637777805328 epoch: 15000, loss: 0.13344115018844604 epoch: 16000, loss: 0.13231371343135834 epoch: 17000, loss: 0.1298571079969406 epoch: 18000, loss: 0.1285153031349182 epoch: 19000, loss: 0.12435487657785416 epoch: 20000, loss: 0.1194448247551918 def plot_net5(x): out = torch.sigmoid(mo_net5(Variable(torch.from_numpy(x).float()))).data.numpy() out = (out &gt; 0.5) * 1 return out plot_decision_boundary(lambda x: plot_net5(x), x.numpy(), y.numpy()) plt.title('sequential') Text(0.5,1,&apos;sequential&apos;) 可以看出多层神经网络拟合效果会更好","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"深度学习基础知识","slug":"deeplearning-base","date":"2019-05-09T05:52:55.739Z","updated":"2019-05-13T11:00:40.922Z","comments":true,"path":"2019/05/09/deeplearning-base/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/09/deeplearning-base/","excerpt":"介绍深度学习中常用的一些概念","text":"介绍深度学习中常用的一些概念 梯度下降梯度下降 梯度下降法 梯度下降代码(PyTorch)梯度下降在线性模型中的应用import torchimport numpy as npfrom torch.autograd import Variable torch.manual_seed(2019) &lt;torch._C.Generator at 0x10c9b39f0&gt; # 读取 x 和 yx_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32) # 画出图像import matplotlib.pyplot as plt%matplotlib inlineplt.plot(x_train, y_train, 'bo') [&lt;matplotlib.lines.Line2D at 0x11ca8cfd0&gt;] # 转换成 Tensorx_train = torch.from_numpy(x_train)y_train = torch.from_numpy(y_train)# 定义参数 w 和 bw = Variable(torch.randn(1), requires_grad=True) # 随机初始化 wb = Variable(torch.zeros(1), requires_grad=True) # 使用 0 初始化 b # 构建线性回归模型x_train = Variable(x_train)y_train = Variable(y_train)def liner_model(x): return x * w + b y_ = liner_model(x_train) # 在更新参数之前，模型输出结果plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')plt.legend() &lt;matplotlib.legend.Legend at 0x11cc6beb8&gt; 计算误差函数：$$\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\hat{y}{i}-y{i}\\right)^{2}$$ # 计算误差def get_loss(y_, y): return torch.mean((y_-y) ** 2)loss = get_loss(y_, y_train) # 查看 loss 的大小loss tensor(10.2335, grad_fn=&lt;MeanBackward1&gt;) 计算 w 和 b 的梯度，采用PyTorch的自动求导，不需要手动取计算梯度。w 和 b 的梯度分别是： $$\\frac{\\partial}{\\partial w}=\\frac{2}{n} \\sum_{i=1}^{n} x_{i}\\left(w x_{i}+b-y_{i}\\right)$$ $$\\frac{\\partial}{\\partial b}=\\frac{2}{n} \\sum_{i=1}^{n}\\left(w x_{i}+b-y_{i}\\right)$$ # 自动求导loss.backward() # 查看 w 和 b 的梯度print(w.grad)print(b.grad) tensor([-41.1289]) tensor([-6.0890]) # 更新一次参数w.data = w.data - 1e-2 * w.grad.datab.data = b.data - 1e-2 * b.grad.data # 更新一次参数后，模型输出结果y_ = liner_model(x_train)plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')plt.legend() &lt;matplotlib.legend.Legend at 0x11cbd84e0&gt; # 多次更新for e in range(20): y_ = liner_model(x_train) loss = get_loss(y_, y_train) w.grad.zero_() # 梯度归零 b.grad.zero_() # 梯度归零 loss.backward() w.data = w.data - 1e-2 * w.grad.data # 更新 w b.data = b.data - 1e-2 * b.grad.data # 更新 b print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e, loss.data.item())) epoch: 0, loss: 0.23505009710788727 epoch: 1, loss: 0.23023782670497894 epoch: 2, loss: 0.2298405021429062 epoch: 3, loss: 0.22952641546726227 epoch: 4, loss: 0.22921547293663025 epoch: 5, loss: 0.22890615463256836 epoch: 6, loss: 0.22859837114810944 epoch: 7, loss: 0.22829222679138184 epoch: 8, loss: 0.227987602353096 epoch: 9, loss: 0.2276846319437027 epoch: 10, loss: 0.2273831069469452 epoch: 11, loss: 0.22708319127559662 epoch: 12, loss: 0.22678478062152863 epoch: 13, loss: 0.2264879196882248 epoch: 14, loss: 0.2261926382780075 epoch: 15, loss: 0.22589880228042603 epoch: 16, loss: 0.22560644149780273 epoch: 17, loss: 0.225315660238266 epoch: 18, loss: 0.22502633929252625 epoch: 19, loss: 0.2247384935617447 # 参数更新20次之后的结果y_ = liner_model(x_train)plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')plt.legend() &lt;matplotlib.legend.Legend at 0x11cb5fc18&gt; 经过 20 次更新， 红色的预测结果已经比较好的拟合了蓝色的真实值 梯度下降在多项式模型中的应用# 定义一个多变量函数w_target = np.array([0.5, 3, 2.4]) # 定义参数b_target = np.array([0.9]) # 定义参数f_des = 'y = &#123;:.2f&#125; + &#123;:.2f&#125; * x + &#123;:.2f&#125; * x^2 + &#123;:.2f&#125; * x^3'.format( b_target[0], w_target[0], w_target[1], w_target[2]) # 打印出函数的样式print(f_des) y = 0.90 + 0.50 * x + 3.00 * x^2 + 2.40 * x^3 # 先画出这个函数的图像x_sample = np.arange(-3, 3, 0.1)y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample **2 + w_target[2] * x_sample ** 3plt.plot(x_sample, y_sample, label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x11c8255f8&gt; # 构建训练数据 x 和 y# x 是一个如下矩阵 [x, x^2, x^3]# y 是函数的结果 [y]x_train = np.stack([x_sample **i for i in range(1, 4)], axis=1)x_train = torch.from_numpy(x_train).float() # 转换成 float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # 转换为 列向量 # 定义要优化的参数 w 和 bw = Variable(torch.randn(3,1), requires_grad=True)b = Variable(torch.zeros(1), requires_grad=True)# 将 x 和 y 转换成 Variabelx_train = Variable(x_train)y_train = Variable(y_train)# 定义多元线性模型def multi_model(x): return torch.mm(x, w) + b # torch.mm 矩阵相乘 # 画出没有参数没有更新之前的模型与真实模型之间的对比y_pred = multi_model(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), color='r', label='fitting curve')plt.plot(x_train.data.numpy()[:, 0], y_sample, color='b', label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x11cf3f390&gt; loss = get_loss(y_pred, y_train)print(loss) tensor(142.2003, grad_fn=&lt;MeanBackward1&gt;) # 自动求导loss.backward() # 查看 w 和 b 的梯度print(w.grad)print(b.grad) tensor([[ -36.4598], [ -13.2381], [-236.6262]]) tensor([-3.9543]) # 更新一次参数w.data = w.data - 0.001 * w.grad.datab.data = b.data - 0.001 * b.grad.data # 画出更新一次参数之后模型y_pred = multi_model(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), color='r', label='fitting curve')plt.plot(x_train.data.numpy()[:, 0], y_sample, color='b', label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x11ccc1b70&gt; # 更新 100 次for e in range(100): y_pred = multi_model(x_train) loss = get_loss(y_pred, y_train) w.grad.data.zero_() b.grad.data.zero_() loss.backward() # 更新参数 w.data = w.data - 0.001 * w.grad.data b.data = b.data - 0.001 * b.grad.data if (e + 1) % 20 == 0: print('epoch &#123;&#125;, Loss:&#123;:.5f&#125;'.format(e+1, loss.data.item())) epoch 20, Loss:2.84467 epoch 40, Loss:1.08338 epoch 60, Loss:0.61722 epoch 80, Loss:0.48439 epoch 100, Loss:0.43824 # 更新 100 次之后的模型结果y_pred = multi_model(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r') plt.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b') plt.legend() &lt;matplotlib.legend.Legend at 0x11cf335c0&gt; # 输出此时的参数 w 和 bprint(w.data)print(b.data) tensor([[1.1381], [3.1028], [2.3013]]) tensor([0.1931]) 可以看到模型迭代100次之后已经非常接近原始数据了 激活函数定义类比人脑神经元，解释如下激活函数在神经网络中非常重要，只有通过激活函数才会进入下一层继续传播，如果不使用激活函数，无论多少层神经网络，最后都会变成单层神经网络，所以每一层必须使用激活函数 常见的激活函数sigmoid 函数$$f(z)=\\frac{1}{1+\\exp (-z)}$$导函数:$$f^{\\prime}(z)=f(z)(1-f(z))$$ 图像： tanh 函数$$f(z)=\\tanh (z)=\\frac{\\mathrm{e}^{z}-\\mathrm{e}^{-z}}{\\mathrm{e}^{z}+\\mathrm{e}^{-z}}$$ 导函数：$$f^{\\prime}(z)=1-(f(z))^{2}$$ 图像: ReLU 函数$$f(z)=\\max (0, z)$$ 导函数:$$f^{\\prime}(z)=\\left{\\begin{array}{l}{1, z&gt;0} \\ {0, z \\leqslant 0}\\end{array}\\right.$$ 图像: 激活函数的选择 现在的神经网络中，90%都使用ReLU激活函数，该激活函数能够加快梯度下降的收敛速度，对比其他激活函数计算更加简单。 Sigmoid和Tanh会导致梯度消失，当z很大时，sigmoid函数的导函数会=趋近于0，造成梯度消失，Tanh相当于Sigmoid函数的平移，原理类似。 RuLU的优势，以及局限性： 优点：计算简单，避免梯度消失，单侧抑制提供了网络的稀疏性表达能力 局限性：在训练过程中会导致神经元死亡问题，这是由于函数负梯度在经过ReLU单元时被置为0，且之后也不被任何数据激活，即改神经元梯度永远为0，不对任何数据产生效应。在实习训练过程中，如果学习率设置较大，会导致一定比例的神经元不可逆死亡。导致参数梯度无法更新，整个训练过程失败。 优化：采用Leaky ReLU 表达式 $$f(z)=\\left{\\begin{array}{cl}{z,} &amp; {z&gt;0} \\ {a z,} &amp; {z \\leqslant 0}\\end{array}\\right.$$","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"基础概念","slug":"基础概念","permalink":"http://lucas0625.github.io/blog/tags/基础概念/"}]},{"title":"算法面试编程试题集锦","slug":"interviewCode","date":"2019-05-05T08:34:34.929Z","updated":"2019-05-27T06:11:01.915Z","comments":true,"path":"2019/05/05/interviewCode/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/05/interviewCode/","excerpt":"面试准备(编程篇)","text":"面试准备(编程篇) 位运算leetcode-78.Subsetsdesc: Given a set of distinct integers, nums, return all possible subsets (the power set)Note: The solution set must not contain duplicate subsets.返回列表中的所有子集 thought: N 个数的集合有 2*N 个子集，将枚举子集转换成枚举组合数，对应位置为1，则包含这个数。时间复杂度O(n 2^n), 空间复杂度O(1)。 solution class Solution: # 时间复杂度O(n * 2^n), 空间复杂度O(1) def subsets(self, nums: List[int]) -&gt; List[List[int]]: N = len(nums) res = [] # N 个数, 有 2**N 种组合, 枚举所有子集转化为枚举所有 组合数 for i in range(pow(2, N)): tmp = [] for j in range(N): # 检测 i 的第 j 位是否为 1 # 1 包含 0 不包含 if (i &gt;&gt; j) % 2 == 1: tmp.append(nums[j]) res.append(tmp) return res leetcode-762.Prime Number of Set Bits in Binary Representationdesc:Given two integers L and R, find the count of numbers in the range [L, R] (inclusive) having a prime number of set bits in their binary representation. (Recall that the number of set bits an integer has is the number of 1s present when written in binary. For example, 21 written in binary is 10101 which has 3 set bits. Also, 1 is not a prime.)给定的[L,R]中，转换成二进制后1的个数是质数的个数。 thought:暴力解法，先求出给定数的二进制中1的个数，在判断是否为质数 solution:class Solution &#123;public: int countPrimeSetBits(int L, int R) &#123; int ans=0; for(int n=L; n&lt;=R; ++n)&#123; if(isPrime(bits(n)))&#123; ans++; &#125; &#125; return ans; &#125;private: int bits(int n)&#123; int s=0; while(n)&#123; s += n&amp;1; n &gt;&gt;= 1; &#125; return s; &#125; bool isPrime(int n)&#123; if(n&lt;=1) return false; if(n==2) return true; for(int i=2; i&lt;=sqrt(n); ++i)&#123; if(n%i == 0) return false; &#125; return true; &#125;&#125;; 二分搜索分治模拟字符串几何链表贪心哈希表leetcode-242.Valid Anagramdesc: Given two strings s and t , write a function to determine if t is an anagram of s.给定两个字符串，判断是否是字符相同但是位置不同 thought: 暴力法可以将两个字符串排序，然后比较大小采用散列表可以先统计每个字符串的各字符的个数，然后比较。 solution # 解法一 时间复杂度Nlog(N)calss Solution: def isAnagram(self, s: str, t: str) -&gt; bool: return sorted(s) == sorted(t) # 解法二 时间复杂度O(N)class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: dic1, dic2 = &#123;&#125;, &#123;&#125; for item in s: dic1[item] = dic1.get(item, 0) + 1 for item in t: dic2[item] = dic2.get(item, 0) + 1 return dic1 == dic2 leetcode-1.Two Sumdesc: Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.找到列表中的两个数a， b 使 a+b=target， 返回 a，b 的下标 thought 采用哈希表的思想，枚举列表，判断 target-a 是否在除去此枚举元素的哈希表中 solution class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: hash_map = dict() for i, item in enumerate(nums): if target - item in hash_map: return [hash_map[target - item], i] hash_map[item] = i leetcode-15.3Sumdesc: Given an array nums of n integers, are there elements a, b, c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero.The solution set must not contain duplicate triplets.返回三数之和为0的所有子列表 thought: 一种方法是蛮力法，嵌套三层循环。第二种方法是，哈希函数的思想，嵌套两次循环，外层循环枚举 a ，内层循环判断 -b-c 是否在除枚举元素之外的散列表中。第三种方法是先排序，然后嵌套两次循环，外层循环枚举 a， 内层循环使用首尾指针进行移位判断。 solution: # 解法一 散列表 时间复杂度O(N^2), 空间复杂度O(N)class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: if len(nums) &lt; 3: return [] nums.sort() res = set() for i, v in enumerate(nums[:-2]): if i&gt;= 1 and v == nums[i-1]: continue d = &#123;&#125; for x in nums[i+1:]: if x not in d: d[-v-x] = 1 else: res.add((v, -v-x, x)) return [list(item) for item in res] # 解法二 首尾指针 时间复杂度O(N^2), 空间复杂度O(1)class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: res = [] nums.sort() for i in range(len(nums)-2): if i &gt; 0 and nums[i] == nums[i-1]: continue l, r = i+1, len(nums)-1 while l &lt; r: s = nums[i] + nums[l] + nums[r] if s &lt; 0 :l += 1 elif s &gt; 0 : r -= 1 else: res.append([nums[i], nums[l], nums[r]]) while l &lt; r and nums[l] == nums[l+1]: l += 1 while l&lt; r and nums[r] == nums[r-1]: r -= 1 l += 1 r -= 1 return res leetcode-3.Longest Substring Without Repeating Charactersdesc: Given a string, find the length of the longest substring without repeating characters.求最大无重复子串的长度 thought: 采用滑动窗口，滑动窗口从最大依次变小，利用hash表，判断窗口内的元素数量是否等于hash之后的元素数量，相等即为最大子串的长度 sulution: class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: if len(s) == 0: return 0 s_len = len(s) # 窗口最大值 max_slide = len(set(s)) # 窗口依次减小 while max_slide: if max_slide == 1: return 1 i =0 # 从字符串开始位置滑动窗口 while i + max_slide &lt;= s_len: sub_s = s[i:i+max_slide] sub_s_set = set(sub_s) if len(sub_s) == len(sub_s_set): return len(sub_s) i += 1 max_slide -= 1 图搜索树leetcode-98.Validate Binary Search Treedesc: Given a binary tree, determine if it is a valid binary search tree (BST).Assume a BST is defined as follows:The left subtree of a node contains only nodes with keys less than the node’s key.The right subtree of a node contains only nodes with keys greater than the node’s key.Both the left and right subtrees must also be binary search trees.验证给定的二叉树是否为二叉搜索树 thought:1. 将给定的二叉树中序遍历，如果遍历之后是升序的，则是二叉搜索树。时间复杂度O(n)2.递归，查找左子树最大值max， 查找右子树最小值min，每次递归都保证 max &lt; root &lt; min，则是二叉搜索树。时间复杂度O(n) solution: # 第一种方法# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isValidBST(self, root: TreeNode) -&gt; bool: inorder = self.inorder(root) return inorder == list(sorted(set(inorder))) def inorder(self, root): if root is None: return [] return self.inorder(root.left) + [root.val] + self.inorder(root.right) # 第二种方法 二叉树遍历leetcode-144. Binary Tree Preorder Traversaldesc: Given a binary tree, return the preorder traversal of its nodes’ values.二叉树前序遍历 thought: 遍历顺序 root &gt; left &gt; right solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.l = [] # 时间复杂度 O(N)， 空间复杂度O(1) def preorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用递归实现前序遍历''' if not root: return [] self.l.append(root.val) self.preorderTraversal(root.left) self.preorderTraversal(root.right) return self.l # 时间复杂度 O(N), 空间复杂度O(N) def preorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用循环实现前序遍历''' if not root: return [] mystack = [] # 存储根节点 out = [] cur = root while cur or mystack: # 从根节点开始，一直找它的左子树 while cur: mystack.append(cur) out.append(cur.val) cur = cur.left # 循环结束，表示前一个节点没有左子树了，开始查看右子树 cur = mystack.pop().right return out leetcode-94. Binary Tree Inorder Traversaldesc: Given a binary tree, return the inorder traversal of its nodes’ values.二叉树中序遍历 thought: 遍历顺序 left &gt; root &gt; right solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.l = [] # 时间复杂度 O(N)， 空间复杂度O(1) def inorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用递归实现前序遍历''' if not root: return [] self.inorderTraversal(root.left) self.l.append(root.val) self.inorderTraversal(root.right) return self.l # 时间复杂度 O(N), 空间复杂度O(N) def inorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用循环实现中序遍历''' if not root: return [] mystack = [] out = [] cur = root while cur or mystack: while cur: mystack.append(cur) cur = cur.left tmp = mystack.pop() out.append(tmp.val) cur = tmp.right return out leetcode-145. Binary Tree Postorder Traversaldesc: Given a binary tree, return the postorder traversal of its nodes’ values.二叉树后序遍历 thought: 遍历顺序 left &gt; right &gt; root, 左右根 是 根右左的反转，可以参考前序遍历的做法 solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.l = [] # 时间复杂度 O(N)， 空间复杂度O(1) def postorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用递归实现后序遍历''' if not root: return [] self.postorderTraversal(root.left) self.postorderTraversal(root.right) self.l.append(root.val) return self.l # 时间复杂度 O(N), 空间复杂度O(N) def postorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用循环实现后序遍历''' if not root: return [] cur = root mystack = [] out = [] # 参考前序遍历的做法，先遍历出右左根，最后反转结果-&gt;根左右 while cur or mystack: while cur: out.append(cur.val) mystack.append(cur) cur = cur.right cur = mystack.pop().left return out[::-1] 递归动态规划数学题其他leetcode-703.Kth Largest Element in a Streamdesc: Design a class to find the kth largest element in a stream. Note that it is the kth largest element in the sorted order, not the kth distinct element.Your KthLargest class will have a constructor which accepts an integer k and an integer array nums, which contains initial elements from the stream. For each call to the method KthLargest.add, return the element representing the kth largest element in the stream.返回数据流中第K大的元素 thought:采用优先队列，维护一个大小为K的最小堆 solution class KthLargest &#123;public: KthLargest(int k, vector&lt;int&gt;&amp; nums):k_(k) &#123; for(int num : nums) &#123; add(num); &#125; &#125; int add(int val) &#123; s_.push(val); if(s_.size() &gt; k_) &#123; s_.pop(); &#125; return s_.top(); &#125;private: const int k_; priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; s_;&#125;;/** * Your KthLargest object will be instantiated and called as such: * KthLargest* obj = new KthLargest(k, nums); * int param_1 = obj-&gt;add(val); */","categories":[{"name":"求职面试","slug":"求职面试","permalink":"http://lucas0625.github.io/blog/categories/求职面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://lucas0625.github.io/blog/tags/面试/"}]},{"title":"算法面试理论试题集锦","slug":"interviewTheory","date":"2019-05-05T08:31:43.372Z","updated":"2019-05-05T08:31:43.373Z","comments":true,"path":"2019/05/05/interviewTheory/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/05/interviewTheory/","excerpt":"面试准备(理论篇)","text":"面试准备(理论篇)","categories":[{"name":"求职面试","slug":"求职面试","permalink":"http://lucas0625.github.io/blog/categories/求职面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://lucas0625.github.io/blog/tags/面试/"}]},{"title":"ElasticSearch","slug":"ElasticSearch","date":"2019-04-29T07:18:55.059Z","updated":"2019-04-29T07:18:55.060Z","comments":true,"path":"2019/04/29/ElasticSearch/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/29/ElasticSearch/","excerpt":"ElasticSearch 原理与应用","text":"ElasticSearch 原理与应用 第一节 ElasticSearch概述1.1 ElasticSearch简介ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTfulweb接口。ElasticSearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。构建在全文检索开源软件Lucene之上的Elasticsearch，不仅能对海量规模的数据完成分布式索引与检索，还能提供数据聚合分析。据国际权威的数据库产品评测机构的统计，在2016年1月，Elasticsearch已超过Solr等，成为排名第一的搜索引擎类应用概括：基于Restful标准的高扩展高可用的实时数据分析的全文搜索工具 1.2 ElasticSearch的基本概念 Index:类似于mysql数据库中的database Type: 类似于mysql数据库中的table表，es中可以在Index中建立type（table），通过mapping进行映射。 Document: 由于es存储的数据是文档型的，一条数据对应一篇文档即相当于mysql数据库中的一行数据row，一个文档中可以有多个字段也就是mysql数据库一行可以有多列。 Field: es中一个文档中对应的多个列与mysql数据库中每一列对应 Mapping: 可以理解为mysql或者solr中对应的schema，只不过有些时候es中的mapping增加了动态识别功能，感觉很强大的样子，其实实际生产环境上不建议使用，最好还是开始制定好了对应的schema为主。 indexed: 就是名义上的建立索引。mysql中一般会对经常使用的列增加相应的索引用于提高查询速度，而在es中默认都是会加上索引的，除非你特殊制定不建立索引只是进行存储用于展示，这个需要看你具体的需求和业务进行设定了。 Query DSL: 类似于mysql的sql语句，只不过在es中是使用的json格式的查询语句，专业术语就叫：QueryDSL GET/PUT/POST/DELETE: 分别类似与mysql中的select/update/delete…… 1.3 Elasticsearch的架构 Gateway层 es用来存储索引文件的一个文件系统且它支持很多类型，例如：本地磁盘、共享存储（做snapshot的时候需要用到）、hadoop的hdfs分布式存储、亚马逊的S3。它的主要职责是用来对数据进行长持久化以及整个集群重启之后可以通过gateway重新恢复数据。 Distributed Lucene Directory Gateway上层就是一个lucene的分布式框架，lucene是做检索的，但是它是一个单机的搜索引擎，像这种es分布式搜索引擎系统，虽然底层用lucene，但是需要在每个节点上都运行lucene进行相应的索引、查询以及更新，所以需要做成一个分布式的运行框架来满足业务的需要。 四大模块组件 districted lucene directory之上就是一些es的模块 Index Module是索引模块，就是对数据建立索引也就是通常所说的建立一些倒排索引等； Search Module是搜索模块，就是对数据进行查询搜索； Mapping模块是数据映射与解析模块，就是你的数据的每个字段可以根据你建立的表结构通过mapping进行映射解析，如果你没有建立表结构，es就会根据你的数据类型推测你的数据结构之后自己生成一个mapping，然后都是根据这个mapping进行解析你的数据； River模块在es2.0之后应该是被取消了，它的意思表示是第三方插件，例如可以通过一些自定义的脚本将传统的数据库（mysql）等数据源通过格式化转换后直接同步到es集群里，这个river大部分是自己写的，写出来的东西质量参差不齐，将这些东西集成到es中会引发很多内部bug，严重影响了es的正常应用，所以在es2.0之后考虑将其去掉。 Discovery、Script es4大模块组件之上有 Discovery模块：es是一个集群包含很多节点，很多节点需要互相发现对方，然后组成一个集群包括选主的，这些es都是用的discovery模块，默认使用的是 Zen，也可是使用EC2；es查询还可以支撑多种script即脚本语言，包括mvel、js、python等等。 Transport协议层 再上一层就是es的通讯接口Transport，支持的也比较多：Thrift、Memcached以及Http，默认的是http，JMX就是java的一个远程监控管理框架，因为es是通过java实现的。 RESTful接口层 最上层就是es暴露给我们的访问接口，官方推荐的方案就是这种Restful接口，直接发送http请求，方便后续使用nginx做代理、分发包括可能后续会做权限的管理，通过http很容易做这方面的管理。如果使用java客户端它是直接调用api，在做负载均衡以及权限管理还是不太好做。 1.4 RESTfull API一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。在目前主流的三种Web服务交互方案中，REST相比于SOAP（Simple Object Access protocol，简单对象访问协议）以及XML-RPC更加简单明了 (Representational State Transfer 意思是：表述性状态传递) 它使用典型的HTTP方法，诸如GET,POST.DELETE,PUT来实现资源的获取，添加，修改，删除等操作。即通过HTTP动词来实现资源的状态扭转复制代码 GET 用来获取资源 POST 用来新建资源（也可以用于更新资源） PUT 用来更新资源 DELETE 用来删除资源 1.5 CRUL命令以命令的方式执行HTTP协议的请求GET/POST/PUT/DELETE 示例： 访问一个网页 curl www.baidu.com curl -o tt.html www.baidu.com 显示响应的头信息 curl -i www.baidu.com 显示一次HTTP请求的通信过程 curl -v www.baidu.com 执行GET/POST/PUT/DELETE操作 curl -X GET/POST/PUT/DELETE url 1.6 CentOS7下安装ElasticSearch6.2.4(1)配置JDK环境 配置环境变量 export JAVA_HOME=&quot;/opt/jdk1.8.0_144&quot;export PATH=&quot;$JAVA_HOME/bin:$PATH&quot;export CLASSPATH=&quot;.:$JAVA_HOME/lib&quot; (2)安装ElasticSearch6.2.4 下载地址：https://www.elastic.co/cn/downloads/elasticsearch 启动报错： 解决方式：bin/elasticsearch -Des.insecure.allow.root=true 或者修改bin/elasticsearch，加上ES_JAVA_OPTS属性：ES_JAVA_OPTS=”-Des.insecure.allow.root=true” 再次启动： 这是出于系统安全考虑设置的条件。由于ElasticSearch可以接收用户输入的脚本并且执行，为了系统安全考 虑，建议创建一个单独的用户用来运行ElasticSearch。 创建用户组和用户： groupadd esgroup useradd esuser -g esgroup -p espassword 更改elasticsearch文件夹及内部文件的所属用户及组： cd /opt chown -R esuser:esgroup elasticsearch-6.2.4 切换用户并运行： su esuser ./bin/elasticsearch 再次启动显示已杀死： 需要调整JVM的内存大小： vi bin/elasticsearch ES_JAVA_OPTS=”-Xms512m -Xmx512m” 再次启动：启动成功 如果显示如下类似信息： [INFO ][o.e.c.r.a.DiskThresholdMonitor] [ZAds5FP] low disk watermark [85%] exceeded on [ZAds5FPeTY-ZUKjXd7HJKA][ZAds5FP][/opt/elasticsearch-6.2.4/data/nodes/0] free: 1.2gb[14.2%], replicas will not be assigned to this node 需要清理磁盘空间。 后台运行：./bin/elasticsearch -d 测试连接：curl 127.0.0.1:9200 会看到一下JSON数据： [root@localhost ~]# curl 127.0.0.1:9200 &#123; &quot;name&quot; : &quot;rBrMTNx&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;-noR5DxFRsyvAFvAzxl07g&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.1.1&quot;, &quot;build_hash&quot; : &quot;5395e21&quot;, &quot;build_date&quot; : &quot;2016-12-06T12:36:15.409Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.3.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 实现远程访问：需要对config/elasticsearch.yml进行 配置： network.host: 192.168.25.131 再次启动报错： 处理第一个错误： vim /etc/security/limits.conf //文件最后加入 esuser soft nofile 65536 esuser hard nofile 65536 esuser soft nproc 4096 esuser hard nproc 4096 处理第二个错误： 进入limits.d目录下修改配置文件。 vim /etc/security/limits.d/20-nproc.conf修改为 esuser soft nproc 4096 处理第三个错误： vim /etc/sysctl.conf vm.max_map_count=655360 执行以下命令生效：sysctl -p 关闭防火墙：systemctl stop firewalld.service 再次启动成功！ 1.7 安装Head插件Head是elasticsearch的集群管理工具，可以用于数据的浏览和查询 (1)elasticsearch-head是一款开源软件，被托管在github上面，所以如果我们要使用它，必须先安装git，通过git获取elasticsearch-head (2)运行elasticsearch-head会用到grunt，而grunt需要npm包管理器，所以nodejs是必须要安装的 (3)elasticsearch5.0之后，elasticsearch-head不做为插件放在其plugins目录下了。使用git拷贝elasticsearch-head到本地 cd /usr/local/ git clone git://github.com/mobz/elasticsearch-head.git (4)安装elasticsearch-head依赖包 [root@localhost local]# npm install -g grunt-cli [root@localhost _site]# cd /usr/local/elasticsearch-head/ [root@localhost elasticsearch-head]# cnpm install (5)修改Gruntfile.js [root@localhost _site]# cd /usr/local/elasticsearch-head/ [root@localhost elasticsearch-head]# vi Gruntfile.js 在connect–&gt;server–&gt;options下面添加：hostname:’*’，允许所有IP可以访问 (6)修改elasticsearch-head默认连接地址[root@localhost elasticsearch-head]# cd /usr/local/elasticsearch-head/_site/ [root@localhost _site]# vi app.js 将this.base_uri = this.config.base_uri || this.prefs.get(“app-base_uri”) || “http://localhost:9200&quot;;中的localhost修改成你es的服务器地址 (7)配置elasticsearch允许跨域访问 打开elasticsearch的配置文件elasticsearch.yml，在文件末尾追加下面两行代码即可： http.cors.enabled: true http.cors.allow-origin: “*” (8)打开9100端口 [root@localhost elasticsearch-head]# firewall-cmd –zone=public –add-port=9100/tcp –permanent 重启防火墙 [root@localhost elasticsearch-head]# firewall-cmd –reload (9)启动elasticsearch (10)启动elasticsearch-head [root@localhost _site]# cd /usr/local/elasticsearch-head/ [root@localhost elasticsearch-head]# node_modules/grunt/bin/grunt server (11)访问elasticsearch-head 关闭防火墙：systemctl stop firewalld.service 浏览器输入网址：http://192.168.25.131:9100/ 1.8 安装KibanaKibana是一个针对Elasticsearch的开源分析及可视化平台，使用Kibana可以查询、查看并与存储在ES索引的数据进行交互操作，使用Kibana能执行高级的数据分析，并能以图表、表格和地图的形式查看数据 (1)下载Kibanahttps://www.elastic.co/downloads/kibana (2)把下载好的压缩包拷贝到/soft目录下 (3)解压缩，并把解压后的目录移动到/user/local/kibana (4)编辑kibana配置文件 [root@localhost /]# vi /usr/local/kibana/config/kibana.yml 将server.host,elasticsearch.url修改成所在服务器的ip地址 (5)开启5601端口 Kibana的默认端口是5601 开启防火墙:systemctl start firewalld.service 开启5601端口:firewall-cmd –permanent –zone=public –add-port=5601/tcp 重启防火墙：firewall-cmd –reload (6)启动Kibana [root@localhost /]# /usr/local/kibana/bin/kibana 浏览器访问：http://192.168.25.131:5601 1.9 安装中文分词器(1)下载中文分词器https://github.com/medcl/elasticsearch-analysis-ik 下载elasticsearch-analysis-ik-master.zip (2)解压elasticsearch-analysis-ik-master.zip unzip elasticsearch-analysis-ik-master.zip (3)进入elasticsearch-analysis-ik-master，编译源码 mvn clean install -Dmaven.test.skip=true (4)在es的plugins文件夹下创建目录ik (5)将编译后生成的elasticsearch-analysis-ik-版本.zip移动到ik下，并解压 (6)解压后的内容移动到ik目录下 第二节 ElasticSearch基本操作2.1 倒排索引Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 示例： (1)：假设文档集合包含五个文档，每个文档内容如图所示，在图中最左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。 (2):中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统将文档自动切分成单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引“单词ID”一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表 (3):索引系统还可以记录除此之外的更多信息,下图还记载了单词频率信息（TF）即这个单词在某个文档中的出现次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。 (4):倒排列表中还可以记录单词在某个文档出现的位置信息 (1,,1),(2,,1),(3,&lt;3,9&gt;,2) 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“Facebook”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。 2.1.1 倒排索引原理1.The quick brown fox jumped over the lazy dog 2.Quick brown foxes leap over lazy dogs in summer 倒排索引： Term Doc_1 Doc_2 Quick X The X brown X X dog X dogs X fox X foxes X in X jumped X lazy X X leap X over X X quick X summer X the X 搜索quick brown ： Term Doc_1 Doc_2 brown X X quick X Total 2 1 计算相关度分数时，文档1的匹配度高，分数会比文档2高 问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。 fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。 jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。 搜索含有 Quick fox的文档是搜索不到的 使用标准化规则(normalization)：建立倒排索引的时候，会对拆分出的各个单词进行相应的处理，以提升后面搜索的时候能够搜索到相关联的文档的概率 Term Doc_1 Doc_2 brown X X dog X X fox X X in X jump X X lazy X X over X X quick X X summer X the X X 2.1.2 分词器介绍及内置分词器分词器：从一串文本中切分出一个一个的词条，并对每个词条进行标准化 包括三部分： character filter：分词之前的预处理，过滤掉HTML标签，特殊符号转换等 tokenizer：分词 token filter：标准化 内置分词器： standard 分词器：(默认的)他会将词汇单元转换成小写形式，并去除停用词和标点符号，支持中文采用的方法为单字切分 simple 分词器：首先会通过非字母字符来分割文本信息，然后将词汇单元统一为小写形式。该分析器会去掉数字类型的字符。 Whitespace 分词器：仅仅是去除空格，对字符没有lowcase化,不支持中文；并且不对生成的词汇单元进行其他的标准化处理。 language 分词器：特定语言的分词器，不支持中文 2.2 使用ElasticSearch API 实现CRUD添加索引： PUT /lib/ &#123; &quot;settings&quot;:&#123; &quot;index&quot;:&#123; &quot;number_of_shards&quot;: 5, &quot;number_of_replicas&quot;: 1 &#125; &#125;&#125; PUT lib 查看索引信息: GET /lib/_settings GET _all/_settings 添加文档: PUT /lib/user/1 &#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125; POST /lib/user/ &#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 23, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ] &#125; 查看文档: GET /lib/user/1 GET /lib/user/ GET /lib/user/1?_source=age,interests 更新文档: PUT /lib/user/1 &#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 36, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125; POST /lib/user/1/_update &#123; &quot;doc&quot;:&#123; &quot;age&quot;:33 &#125;&#125; 删除一个文档: DELETE /lib/user/1 删除一个索引: DELETE /lib 2.3 批量获取文档使用es提供的Multi Get API： 使用Multi Get API可以通过索引名、类型名、文档id一次得到一个文档集合，文档可以来自同一个索引库，也可以来自不同索引库 使用curl命令： curl &apos;http://192.168.25.131:9200/_mget&apos; -d &apos;&#123;&quot;docs&quot;：[ &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 1 &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2 &#125; ]&#125;&apos; 在客户端工具中： GET /_mget &#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 1 &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2 &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 3 &#125; ]&#125; 可以指定具体的字段： GET /_mget &#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 1, &quot;_source&quot;: &quot;interests&quot; &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2, &quot;_source&quot;: [&quot;age&quot;,&quot;interests&quot;] &#125; ]&#125; 获取同索引同类型下的不同文档： GET /lib/user/_mget &#123; &quot;docs&quot;:[ &#123; &quot;_id&quot;: 1 &#125;, &#123; &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2, &#125; ]&#125; GET /lib/user/_mget &#123; &quot;ids&quot;: [&quot;1&quot;,&quot;2&quot;] &#125; 2.4 使用Bulk API 实现批量操作bulk的格式： {action:{metadata}}\\n {requstbody}\\n action:(行为) create：文档不存在时创建 update:更新文档 index:创建新文档或替换已有文档 delete:删除一个文档 metadata：_index,_type,_id create和index的区别 如果数据存在，使用create操作失败，会提示文档已经存在，使用index则可以成功执行。 示例： {“delete”:{“_index”:”lib”,”_type”:”user”,”_id”:”1”}} 批量添加: POST /lib2/books/_bulk &#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;&#123;&quot;title&quot;:&quot;Java&quot;,&quot;price&quot;:55&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;&#123;&quot;title&quot;:&quot;Html5&quot;,&quot;price&quot;:45&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;&#123;&quot;title&quot;:&quot;Php&quot;,&quot;price&quot;:35&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:4&#125;&#125;&#123;&quot;title&quot;:&quot;Python&quot;,&quot;price&quot;:50&#125; 批量获取: GET /lib2/books/_mget &#123;&quot;ids&quot;: [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;]&#125; 删除：没有请求体 POST /lib2/books/_bulk &#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;lib2&quot;,&quot;_type&quot;:&quot;books&quot;,&quot;_id&quot;:4&#125;&#125;&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;tt&quot;,&quot;_type&quot;:&quot;ttt&quot;,&quot;_id&quot;:&quot;100&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;lisi&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;tt&quot;,&quot;_type&quot;:&quot;ttt&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;zhaosi&quot;&#125;&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;lib2&quot;,&quot;_type&quot;:&quot;books&quot;,&quot;_id&quot;:&quot;4&quot;&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;price&quot;:58&#125;&#125; bulk一次最大处理多少数据量: bulk会把将要处理的数据载入内存中，所以数据量是有限制的，最佳的数据量不是一个确定的数值，它取决于你的硬件，你的文档大小以及复杂性，你的索引以及搜索的负载。 一般建议是1000-5000个文档，大小建议是5-15MB，默认不能超过100M，可以在es的配置文件（即$ES_HOME下的config下的elasticsearch.yml）中。 2.5 版本控制ElasticSearch采用了乐观锁来保证数据的一致性，也就是说，当用户对document进行操作时，并不需要对该document作加锁和解锁的操作，只需要指定要操作的版本即可。当版本号一致时，ElasticSearch会允许该操作顺利执行，而当版本号存在冲突时，ElasticSearch会提示冲突并抛出异常（VersionConflictEngineException异常）。 ElasticSearch的版本号的取值范围为1到2^63-1。 内部版本控制：使用的是_version 外部版本控制：elasticsearch在处理外部版本号时会与对内部版本号的处理有些不同。它不再是检查version是否与请求中指定的数值相同_,而是检查当前的_version是否比指定的数值小。如果请求成功，那么外部的版本号就会被存储到文档中的_version中。 为了保持_version与外部版本控制的数据一致使用version_type=external 2.6 什么是MappingPUT /myindex/article/1 &#123; &quot;post_date&quot;: &quot;2018-05-10&quot;, &quot;title&quot;: &quot;Java&quot;, &quot;content&quot;: &quot;java is the best language&quot;, &quot;author_id&quot;: 119&#125; PUT /myindex/article/2 &#123; &quot;post_date&quot;: &quot;2018-05-12&quot;, &quot;title&quot;: &quot;html&quot;, &quot;content&quot;: &quot;I like html&quot;, &quot;author_id&quot;: 120&#125; PUT /myindex/article/3 &#123; &quot;post_date&quot;: &quot;2018-05-16&quot;, &quot;title&quot;: &quot;es&quot;, &quot;content&quot;: &quot;Es is distributed document store&quot;, &quot;author_id&quot;: 110&#125; GET /myindex/article/_search?q=2018-05 GET /myindex/article/_search?q=2018-05-10 GET /myindex/article/_search?q=html GET /myindex/article/_search?q=java 2.6.1 查看es自动创建的mappingGET /myindex/article/_mapping es自动创建了index，type，以及type对应的mapping(dynamic mapping) 什么是映射：mapping定义了type中的每个字段的数据类型以及这些字段如何分词等相关属性 &#123; &quot;myindex&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 创建索引的时候,可以预先定义字段的类型以及相关属性，这样就能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理字符串值等 支持的数据类型： (1)核心数据类型（Core datatypes） 字符型：string，string类型包括 text 和 keyword text类型被用来索引长文本，在建立索引前会将这些文本进行分词，转化为词的组合，建立索引。允许es来检索这些词语。text类型不能用来排序和聚合。 Keyword类型不需要进行分词，可以被用来检索过滤、排序和聚合。keyword 类型字段只能用本身来进行检索 数字型：long, integer, short, byte, double, float 日期型：date 布尔型：boolean 二进制型：binary (2)复杂数据类型（Complex datatypes） 数组类型（Array datatype）：数组类型不需要专门指定数组元素的type，例如： 字符型数组: [ &quot;one&quot;, &quot;two&quot; ] 整型数组：[ 1, 2 ] 数组型数组：[ 1, [ 2, 3 ]] 等价于[ 1, 2, 3 ] 对象数组：[ { &quot;name&quot;: &quot;Mary&quot;, &quot;age&quot;: 12 }, { &quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 10 }] 对象类型（Object datatype）：_ object _ 用于单个JSON对象； 嵌套类型（Nested datatype）：_ nested _ 用于JSON数组； (3)地理位置类型（Geo datatypes） 地理坐标类型（Geo-point datatype）：_ geo_point _ 用于经纬度坐标； 地理形状类型（Geo-Shape datatype）：_ geo_shape _ 用于类似于多边形的复杂形状； (4)特定类型（Specialised datatypes） IPv4 类型（IPv4 datatype）：_ ip _ 用于IPv4 地址； Completion 类型（Completion datatype）：_ completion _提供自动补全建议； Token count 类型（Token count datatype）：_ token_count _ 用于统计做了标记的字段的index数目，该值会一直增加，不会因为过滤条件而减少。 mapper-murmur3 类型：通过插件，可以通过 _ murmur3 _ 来计算 index 的 hash 值； 附加类型（Attachment datatype）：采用 mapper-attachments 插件，可支持_ attachments _ 索引，例如 Microsoft Office 格式，Open Document 格式，ePub, HTML 等。 支持的属性： “store”:false//是否单独设置此字段的是否存储而从_source字段中分离，默认是false，只能搜索，不能获取值 “index”: true//分词，不分词是：false ，设置成false，字段将不会被索引 “analyzer”:”ik”//指定分词器,默认分词器为standard analyzer “boost”:1.23//字段级别的分数加权，默认值是1.0 “doc_values”:false//对not_analyzed字段，默认都是开启，分词字段不能使用，对排序和聚合能提升较大性能，节约内存 “fielddata”:{“format”:”disabled”}//针对分词字段，参与排序或聚合时能提高性能，不分词字段统一建议使用doc_value “fields”:{“raw”:{“type”:”string”,”index”:”not_analyzed”}} //可以对一个字段提供多种索引模式，同一个字段的值，一个分词，一个不分词 “ignore_above”:100 //超过100个字符的文本，将会被忽略，不被索引 “include_in_all”:ture//设置是否此字段包含在_all字段中，默认是true，除非index设置成no选项 “index_options”:”docs”//4个可选参数docs（索引文档号） ,freqs（文档号+词频），positions（文档号+词频+位置，通常用来距离查询），offsets（文档号+词频+位置+偏移量，通常被使用在高亮字段）分词字段默认是position，其他的默认是docs “norms”:{“enable”:true,”loading”:”lazy”}//分词字段默认配置，不分词字段：默认{“enable”:false}，存储长度因子和索引时boost，建议对需要参与评分字段使用 ，会额外增加内存消耗量 “null_value”:”NULL”//设置一些缺失字段的初始化值，只有string可以使用，分词字段的null值也会被分词 “position_increament_gap”:0//影响距离查询或近似查询，可以设置在多值字段的数据上火分词字段上，查询时可指定slop间隔，默认值是100 “search_analyzer”:”ik”//设置搜索时的分词器，默认跟ananlyzer是一致的，比如index时用standard+ngram，搜索时用standard用来完成自动提示功能 “similarity”:”BM25”//默认是TF/IDF算法，指定一个字段评分策略，仅仅对字符串型和分词类型有效 “term_vector”:”no”//默认不存储向量信息，支持参数yes（term存储），with_positions（term+位置）,with_offsets（term+偏移量），with_positions_offsets(term+位置+偏移量) 对快速高亮fast vector highlighter能提升性能，但开启又会加大索引体积，不适合大数据量用 映射的分类： (1)动态映射： 当ES在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动地对该字段添加映射。 可以通过dynamic设置来控制这一行为，它能够接受以下的选项： true：默认值。动态添加字段 false：忽略新字段 strict：如果碰到陌生字段，抛出异常 dynamic设置可以适用在根对象上或者object类型的任意字段上。 POST /lib2 2.6.2 给索引lib2创建映射类型&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;books&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:false&#125;, &quot;publish_date&quot;:&#123;&quot;type&quot;:&quot;date&quot;,&quot;index&quot;:false&#125;, &quot;price&quot;:&#123;&quot;type&quot;:&quot;double&quot;&#125;, &quot;number&quot;:&#123;&quot;type&quot;:&quot;integer&quot;&#125; &#125; &#125; &#125;&#125; POST /lib2 2.6.3 给索引lib2创建映射类型&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;books&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:false&#125;, &quot;publish_date&quot;:&#123;&quot;type&quot;:&quot;date&quot;,&quot;index&quot;:false&#125;, &quot;price&quot;:&#123;&quot;type&quot;:&quot;double&quot;&#125;, &quot;number&quot;:&#123; &quot;type&quot;:&quot;object&quot;, &quot;dynamic&quot;:true &#125; &#125; &#125; &#125;&#125; 2.7基本查询(Query查询)2.7.1数据准备PUT /lib3 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;age&quot;: &#123;&quot;type&quot;:&quot;integer&quot;&#125;, &quot;interests&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;birthday&quot;: &#123;&quot;type&quot;:&quot;date&quot;&#125; &#125; &#125; &#125;&#125; GET /lib3/user/_search?q=name:lisi GET /lib3/user/_search?q=name:zhaoliu&amp;sort=age:desc 2.7.2 term查询和terms查询term query会去倒排索引中寻找确切的term，它并不知道分词器的存在。这种查询适合keyword 、numeric、date。 term:查询某个字段里含有某个关键词的文档 GET /lib3/user/_search/ &#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123;&quot;interests&quot;: &quot;changge&quot;&#125; &#125;&#125; terms:查询某个字段里含有多个关键词的文档 GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; 2.7.3 控制查询返回的数量from：从哪一个文档开始size：需要的个数 GET /lib3/user/_search &#123; &quot;from&quot;:0, &quot;size&quot;:2, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; 2.7.4 返回版本号GET /lib3/user/_search &#123; &quot;version&quot;:true, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; 2.7.5 match查询match query知道分词器的存在，会对filed进行分词操作，然后再查询 GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;name&quot;: &quot;zhaoliu&quot; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;age&quot;: 20 &#125; &#125;&#125; match_all:查询所有文档 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; multi_match:可以指定多个字段 GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;lvyou&quot;, &quot;fields&quot;: [&quot;interests&quot;,&quot;name&quot;] &#125; &#125;&#125; match_phrase:短语匹配查询 ElasticSearch引擎首先分析（analyze）查询字符串，从分析后的文本中构建短语查询，这意味着必须匹配短语中的所有分词，并且保证各个分词的相对位置不变： GET lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;interests&quot;: &quot;duanlian，shuoxiangsheng&quot; &#125; &#125;&#125; 2.7.6 指定返回的字段GET /lib3/user/_search &#123; &quot;_source&quot;: [&quot;address&quot;,&quot;name&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;&#125; 2.7.7控制加载的字段GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot;: &#123; &quot;includes&quot;: [&quot;name&quot;,&quot;address&quot;], &quot;excludes&quot;: [&quot;age&quot;,&quot;birthday&quot;] &#125;&#125; 使用通配符* GET /lib3/user/_search &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: &quot;addr*&quot;, &quot;excludes&quot;: [&quot;name&quot;,&quot;bir*&quot;] &#125;, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 2.7.8 排序使用sort实现排序：desc:降序，asc:升序 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;:&quot;asc&quot; &#125; &#125; ] &#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125; ] &#125; 2.7.9 前缀匹配查询GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;zhao&quot; &#125; &#125; &#125;&#125; 2.7.10 范围查询range:实现范围查询 参数：from,to,include_lower,include_upper,boost include_lower:是否包含范围的左边界，默认是true include_upper:是否包含范围的右边界，默认是true GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;birthday&quot;: &#123; &quot;from&quot;: &quot;1990-10-10&quot;, &quot;to&quot;: &quot;2018-05-01&quot; &#125; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;from&quot;: 20, &quot;to&quot;: 25, &quot;include_lower&quot;: true, &quot;include_upper&quot;: false &#125; &#125; &#125;&#125; 2.7.11 wildcard查询允许使用通配符* 和 ?来进行查询 *代表0个或多个字符 ？代表任意一个字符 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;name&quot;: &quot;zhao*&quot; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;name&quot;: &quot;li?i&quot; &#125; &#125;&#125; 2.7.12 fuzzy实现模糊查询value：查询的关键字 boost：查询的权值，默认值是1.0 min_similarity:设置匹配的最小相似度，默认值为0.5，对于字符串，取值为0-1(包括0和1);对于数值，取值可能大于1;对于日期型取值为1d,1m等，1d就代表1天 prefix_length:指明区分词项的共同前缀长度，默认是0 max_expansions:查询中的词项可以扩展的数目，默认可以无限大 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;interests&quot;: &quot;chagge&quot; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;interests&quot;: &#123; &quot;value&quot;: &quot;chagge&quot; &#125; &#125; &#125;&#125; 2.7.13 高亮搜索结果GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;interests&quot;: &#123;&#125; &#125; &#125;&#125; 2.8 Filter查询filter是不计算相关性的，同时可以cache。因此，filter速度要快于query。 POST /lib4/items/_bulk &#123;&quot;index&quot;: &#123;&quot;_id&quot;: 1&#125;&#125;&#123;&quot;price&quot;: 40,&quot;itemID&quot;: &quot;ID100123&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 2&#125;&#125;&#123;&quot;price&quot;: 50,&quot;itemID&quot;: &quot;ID100124&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 3&#125;&#125;&#123;&quot;price&quot;: 25,&quot;itemID&quot;: &quot;ID100124&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 4&#125;&#125;&#123;&quot;price&quot;: 30,&quot;itemID&quot;: &quot;ID100125&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 5&#125;&#125;&#123;&quot;price&quot;: null,&quot;itemID&quot;: &quot;ID100127&quot;&#125; ###2.8.1 简单的过滤查询 GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;term&quot;: &#123; &quot;price&quot;: 40 &#125; &#125;&#125; GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;price&quot;: [25,40] &#125; &#125;&#125; GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;term&quot;: &#123; &quot;itemID&quot;: &quot;ID100123&quot; &#125; &#125;&#125; 查看分词器分析的结果： GET /lib4/_mapping 不希望商品id字段被分词，则重新创建映射 DELETE lib4 PUT /lib4 &#123; &quot;mappings&quot;: &#123; &quot;items&quot;: &#123; &quot;properties&quot;: &#123; &quot;itemID&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; 2.8.2 bool过滤查询可以实现组合过滤查询 格式： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [], &quot;should&quot;: [], &quot;must_not&quot;: [] &#125;&#125; must:必须满足的条件—and should：可以满足也可以不满足的条件–or must_not:不需要满足的条件–not GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;: &#123;&quot;price&quot;:25&#125;&#125;, &#123;&quot;term&quot;: &#123;&quot;itemID&quot;: &quot;id100123&quot;&#125;&#125; ], &quot;must_not&quot;: &#123; &quot;term&quot;:&#123;&quot;price&quot;: 30&#125; &#125; &#125; &#125;&#125; 嵌套使用bool： GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;: &#123;&quot;itemID&quot;: &quot;id100123&quot;&#125;&#125;, &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123;&quot;itemID&quot;: &quot;id100124&quot;&#125;&#125;, &#123;&quot;term&quot;: &#123;&quot;price&quot;: 40&#125;&#125; ] &#125; &#125; ] &#125; &#125;&#125; 2.8.3 范围过滤gt: &gt; lt: &lt; gte: &gt;= lte: &lt;= GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gt&quot;: 25, &quot;lt&quot;: 50 &#125; &#125; &#125;&#125; 2.8.5 过滤非空GET /lib4/items/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;exists&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125; &#125;&#125; GET /lib4/items/_search &#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot;: &#123; &quot;exists&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125; &#125;&#125; 2.8.6 过滤器缓存ElasticSearch提供了一种特殊的缓存，即过滤器缓存（filter cache），用来存储过滤器的结果，被缓存的过滤器并不需要消耗过多的内存（因为它们只存储了哪些文档能与过滤器相匹配的相关信息），而且可供后续所有与之相关的查询重复使用，从而极大地提高了查询性能。 注意：ElasticSearch并不是默认缓存所有过滤器，以下过滤器默认不缓存： numeric_range script geo_bbox geo_distance geo_distance_range geo_polygon geo_shape and or not exists,missing,range,term,terms默认是开启缓存的 开启方式：在filter查询语句后边加上“_catch”:true 2.9 聚合查询(1)sum GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_of_sum&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (2)min GET /lib4/items/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;price_of_min&quot;: &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (3)max GET /lib4/items/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;price_of_max&quot;: &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (4)avg GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_of_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (5)cardinality:求基数 GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_of_cardi&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (6)terms:分组 GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_group_by&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; 对那些有唱歌兴趣的用户按年龄分组GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;, &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;age_group_by&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;order&quot;: &#123; &quot;avg_of_age&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_of_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 2.10 复合查询将多个基本查询组合成单一查询的查询 2.10.1 使用bool查询接收以下参数： must： 文档 必须匹配这些条件才能被包含进来。must_not： 文档 必须不匹配这些条件才能被包含进来。should： 如果满足这些语句中的任意语句，将增加 _score，否则，无任何影响。它们主要用于修正每个文档的相关性得分。filter： 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。 相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 _两者_ 都满足，那么它排名将更高： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125; ] &#125;&#125; 如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。但，如果存在至少一条 must 语句，则对 should 语句的匹配没有要求。如果我们不想因为文档的时间而影响得分，可以用 filter 语句来重写前面的例子： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125; &#125; &#125;&#125; 通过将 range 查询移到 filter 语句中，我们将它转成不评分的查询，将不再影响文档的相关性排名。由于它现在是一个不评分的查询，可以使用各种对 filter 查询有效的优化手段来提升性能。 bool 查询本身也可以被用做不评分的查询。简单地将它放置到 filter 语句中并在内部构建布尔逻辑： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;lte&quot;: 29.99 &#125;&#125;&#125; ], &quot;must_not&quot;: [ &#123; &quot;term&quot;: &#123; &quot;category&quot;: &quot;ebooks&quot; &#125;&#125; ] &#125; &#125; &#125;&#125; 2.10.2 constant_score查询它将一个不变的常量评分应用于所有匹配的文档。它被经常用于你只需要执行一个 filter 而没有其它查询（例如，评分查询）的情况下。 &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;category&quot;: &quot;ebooks&quot; &#125; &#125; &#125;&#125; term 查询被放置在 constant_score 中，转成不评分的filter。这种方式可以用来取代只有 filter 语句的 bool 查询。 第三节 ElasticSearch原理3.1 解析es的分布式架构3.1.1 分布式架构的透明隐藏特性 ElasticSearch是一个分布式系统，隐藏了复杂的处理机制 分片机制：我们不用关心数据是按照什么机制分片的、最后放入到哪个分片中 分片的副本：集群发现机制(cluster discovery)：比如当前我们启动了一个es进程，当启动了第二个es进程时，这个进程作为一个node自动就发现了集群，并且加入了进去shard负载均衡：比如现在有10shard，集群中有3个节点，es会进行均衡的进行分配，以保持每个节点均衡的负载请求 请求路由 3.1.2 扩容机制垂直扩容：购置新的机器，替换已有的机器 水平扩容：直接增加机器 3.1.3 rebalance增加或减少节点时会自动均衡 3.1.4 master节点主节点的主要职责是和集群操作相关的内容，如创建或删除索引，跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点。稳定的主节点对集群的健康是非常重要的。 3.1.5 节点对等每个节点都能接收请求每个节点接收到请求后都能把该请求路由到有相关数据的其它节点上接收原始请求的节点负责采集数据并返回给客户端 3.2 分片和副本机制 index包含多个shard 每个shard都是一个最小工作单元，承载部分数据；每个shard都是一个lucene实例，有完整的建立索引和处理请求的能力 增减节点时，shard会自动在nodes中负载均衡 primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard replica shard是primary shard的副本，负责容错，以及承担读请求负载 primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改 primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上 3.3 单节点环境下创建索引分析PUT /myindex &#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的（一个shard的副本replica，他们两个是不能在同一个节点的）。集群可以正常工作，但是一旦出现节点宕机，数据全部丢失，而且集群不可用，无法接收任何请求。 3.4 两个节点环境下创建索引分析将3个primary shard分配到一个node上去，另外3个replica shard分配到另一个节点上 primary shard 和replica shard 保持同步 primary shard 和replica shard 都可以处理客户端的读请求 3.5 水平扩容的过程 扩容后primary shard和replica shard会自动的负载均衡 扩容后每个节点上的shard会减少，那么分配给每个shard的CPU，内存，IO资源会更多，性能提高 扩容的极限，如果有6个shard，扩容的极限就是6个节点，每个节点上一个shard，如果想超出扩容的极限，比如说扩容到9个节点，那么可以增加replica shard的个数 6个shard，3个节点，最多能承受几个节点所在的服务器宕机？(容错性)任何一台服务器宕机都会丢失部分数据。为了提高容错性，增加shard的个数：9个shard，(3个primary shard，6个replicashard)，这样就能容忍最多两台服务器宕机了 总结：扩容是为了提高系统的吞吐量，同时也要考虑容错性，也就是让尽可能多的服务器宕机还能保证数据不丢失 3.6ElasticSearch的容错机制以9个shard，3个节点为例： 1.如果master node 宕机，此时不是所有的primary shard都是Active status，所以此时的集群状态是red。 容错处理的第一步:是选举一台服务器作为master容错处理的第二步:新选举出的master会把挂掉的primary shard的某个replica shard 提升为primary shard,此时集群的状态为yellow，因为少了一个replica shard，并不是所有的replica shard都是active status 容错处理的第三步：重启故障机，新master会把所有的副本都复制一份到该节点上，（同步一下宕机后发生的修改），此时集群的状态为green，因为所有的primary shard和replica shard都是Active status 3.7文档的核心元数据 _index: 说明了一个文档存储在哪个索引中 同一个索引下存放的是相似的文档(文档的field多数是相同的) 索引名必须是小写的，不能以下划线开头，不能包括逗号 _type: 表示文档属于索引中的哪个类型 一个索引下只能有一个type 类型名可以是大写也可以是小写的，不能以下划线开头，不能包括逗号 _id: 文档的唯一标识，和索引，类型组合在一起唯一标识了一个文档 可以手动指定值，也可以由es来生成这个值 3.8 文档id生成方式 手动指定 put /index/type/66 通常是把其它系统的已有数据导入到es时 由es生成id值 post /index/type es生成的id长度为20个字符，使用的是base64编码，URL安全，使用的是GUID算法，分布式下并发生成id值时不会冲突 3.9 _source元数据分析其实就是我们在添加文档时request body中的内容 指定返回的结果中含有哪些字段： get /index/type/1?_source=name 3.10 改变文档内容原理解析替换方式： PUT /lib/user/4 &#123; &quot;first_name&quot; : &quot;Jane&quot;,&quot;last_name&quot; : &quot;Lucy&quot;,&quot;age&quot; : 24,&quot;about&quot; : &quot;I like to collect rock albums&quot;,&quot;interests&quot;: [ &quot;music&quot; ]&#125; 修改方式(partial update)： POST /lib/user/2/_update &#123; &quot;doc&quot;:&#123; &quot;age&quot;:26 &#125;&#125; 删除文档：标记为deleted，随着数据量的增加，es会选择合适的时间删除掉 3.11 基于groovy脚本执行partial updatees有内置的脚本支持，可以基于groovy脚本实现复杂的操作 修改年龄 POST /lib/user/4/_update &#123; &quot;script&quot;: &quot;ctx._source.age+=1&quot;&#125; 修改名字 POST /lib/user/4/_update &#123; &quot;script&quot;: &quot;ctx._source.last_name+=&apos;hehe&apos;&quot;&#125; 添加爱好 POST /lib/user/4/_update &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.interests.add(params.tag)&quot;, &quot;params&quot;: &#123; &quot;tag&quot;:&quot;picture&quot; &#125; &#125;&#125; 删除爱好 POST /lib/user/4/_update &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.interests.remove(ctx._source.interests.indexOf(params.tag))&quot;, &quot;params&quot;: &#123; &quot;tag&quot;:&quot;picture&quot; &#125; &#125;&#125; 删除文档 POST /lib/user/4/_update &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx.op=ctx._source.age==params.count?&apos;delete&apos;:&apos;none&apos;&quot;, &quot;params&quot;: &#123; &quot;count&quot;:29 &#125; &#125;&#125; upsert POST /lib/user/4/_update &#123; &quot;script&quot;: &quot;ctx._source.age += 1&quot;, &quot;upsert&quot;: &#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Lucy&quot;, &quot;age&quot; : 20, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125;&#125; 3.12 partial update 处理并发冲突使用的是乐观锁:_version retry_on_conflict: POST /lib/user/4/_update?retry_on_conflict=3 重新获取文档数据和版本信息进行更新，不断的操作，最多操作的次数就是retry_on_conflict的值 3.13 文档数据路由原理解析 文档路由到分片上： 一个索引由多个分片构成，当添加(删除，修改)一个文档时，es就需要决定这个文档存储在哪个分片上，这个过程就称为数据路由(routing) 路由算法： shard=hash(routing) % number_of_pirmary_shards 示例：一个索引，3个primary shard (1)每次增删改查时，都有一个routing值，默认是文档的_id的值 (2)对这个routing值使用哈希函数进行计算 (3)计算出的值再和主分片个数取余数 余数肯定在0—（number_of_pirmary_shards-1）之间，文档就在对应的shard上 routing值默认是文档的_id的值，也可以手动指定一个值，手动指定对于负载均衡以及提高批量读取的性能都有帮助 primary shard个数一旦确定就不能修改了 3.14 文档增删改内部原理1:发送增删改请求时，可以选择任意一个节点，该节点就成了协调节点(coordinating node) 2.协调节点使用路由算法进行路由，然后将请求转到primary shard所在节点，该节点处理请求，并把数据同步到它的replica shard 3.协调节点对客户端做出响应 3.15 写一致性原理和quorum机制 任何一个增删改操作都可以跟上一个参数consistency 可以给该参数指定的值： one: (primary shard)只要有一个primary shard是活跃的就可以执行 all: (all shard)所有的primary shard和replica shard都是活跃的才能执行 quorum: (default) 默认值，大部分shard是活跃的才能执行 （例如共有6个shard，至少有3个shard是活跃的才能执行写操作） quorum机制：多数shard都是可用的， int((primary+number_of_replica)/2)+1 例如：3个primary shard，1个replica int((3+1)/2)+1=3 至少3个shard是活跃的 注意：可能出现shard不能分配齐全的情况 比如：1个primary shard,1个replicaint((1+1)/2)+1=2但是如果只有一个节点，因为primary shard和replica shard不能在同一个节点上，所以仍然不能执行写操作 再举例：1个primary shard,3个replica,2个节点 int((1+3)/2)+1=3 最后:当活跃的shard的个数没有达到要求时，es默认会等待一分钟，如果在等待的期间活跃的shard的个数没有增加，则显示timeout put /index/type/id?timeout=60s 3.16 文档查询内部原理第一步：查询请求发给任意一个节点，该节点就成了coordinating node，该节点使用路由算法算出文档所在的primary shard 第二步：协调节点把请求转发给primary shard也可以转发给replica shard(使用轮询调度算法(Round-Robin Scheduling，把请求平均分配至primary shard 和replica shard) 第三步：处理请求的节点把结果返回给协调节点，协调节点再返回给应用程序 特殊情况：请求的文档还在建立索引的过程中，primary shard上存在，但replica shar上不存在，但是请求被转发到了replica shard上，这时就会提示找不到文档 3.17 bulk批量操作的json格式解析bulk的格式： {action:{metadata}}\\n {requstbody}\\n 为什么不使用如下格式： [&#123;&quot;action&quot;: &#123;&#125;,&quot;data&quot;: &#123;&#125;&#125;] 这种方式可读性好，但是内部处理就麻烦了： 将json数组解析为JSONArray对象，在内存中就需要有一份json文本的拷贝，另外还有一个JSONArray对象。 解析json数组里的每个json，对每个请求中的document进行路由 为路由到同一个shard上的多个请求，创建一个请求数组 将这个请求数组序列化 将序列化后的请求数组发送到对应的节点上去 耗费更多内存，增加java虚拟机开销 不用将其转换为json对象，直接按照换行符切割json，内存中不需要json文本的拷贝 对每两个一组的json，读取meta，进行document路由 直接将对应的json发送到node上去 3.18 查询结果分析&#123; &quot;took&quot;: 419, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.6931472, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lib3&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.6931472, &quot;_source&quot;: &#123; &quot;address&quot;: &quot;bei jing hai dian qu qing he zhen&quot;, &quot;name&quot;: &quot;lisi&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lib3&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.47000363, &quot;_source&quot;: &#123; &quot;address&quot;: &quot;bei jing hai dian qu qing he zhen&quot;, &quot;name&quot;: &quot;zhaoming&quot; &#125; &#125; took：查询耗费的时间，单位是毫秒 _shards：共请求了多少个shard total：查询出的文档总个数 max_score：本次查询中，相关度分数的最大值，文档和此次查询的匹配度越高，_score的值越大，排位越靠前 hits：默认查询前10个文档 timed_out： GET /lib3/user/_search?timeout=10ms &#123; &quot;_source&quot;: [&quot;address&quot;,&quot;name&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;&#125; 3.19 多index，多type查询模式GET _search GET /lib/_search GET /lib,lib3/_search GET /3,4/_search GET /lib/user/_search GET /lib,lib4/user,items/_search GET /_all/_search GET /_all/user,items/_search 3.20 分页查询中的deep paging问题GET /lib3/user/_search &#123; &quot;from&quot;:0, &quot;size&quot;:2, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; GET /_search?from=0&amp;size=3 deep paging:查询的很深，比如一个索引有三个primary shard，分别存储了6000条数据，我们要得到第100页的数据(每页10条)，类似这种情况就叫deep paging 如何得到第100页的10条数据？ 在每个shard中搜索990到999这10条数据，然后用这30条数据排序，排序之后取10条数据就是要搜索的数据，这种做法是错的，因为3个shard中的数据的_score分数不一样，可能这某一个shard中第一条数据的_score分数比另一个shard中第1000条都要高，所以在每个shard中搜索990到999这10条数据然后排序的做法是不正确的。 正确的做法是每个shard把0到999条数据全部搜索出来（按排序顺序），然后全部返回给coordinate node，由coordinate node按_score分数排序后，取出第100页的10条数据，然后返回给客户端。 deep paging性能问题 耗费网络带宽，因为搜索过深的话，各shard要把数据传送给coordinate node，这个过程是有大量数据传递的，消耗网络， 消耗内存，各shard要把数据传送给coordinate node，这个传递回来的数据，是被coordinate node保存在内存中的，这样会大量消耗内存。 消耗cpu coordinate node要把传回来的数据进行排序，这个排序过程很消耗cpu. 鉴于deep paging的性能问题，所以应尽量减少使用。 3.21 query string查询及copy_to解析GET /lib3/user/_search?q=interests:changge GET /lib3/user/_search?q=+interests:changge GET /lib3/user/_search?q=-interests:changge copy_to字段是把其它字段中的值，以空格为分隔符组成一个大字符串，然后被分析和索引，但是不存储，也就是说它能被查询，但不能被取回显示。 注意:copy_to指向的字段字段类型要为：text 当没有指定field时，就会从copy_to字段中查询GET /lib3/user/_search?q=changge 3.22字符串排序问题对一个字符串类型的字段进行排序通常不准确，因为已经被分词成多个词条了 解决方式：对字段索引两次，一次索引分词（用于搜索），一次索引不分词(用于排序) GET /lib3/_search GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;interests&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;interests.raw&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125; DELETE lib3 PUT /lib3 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;age&quot;: &#123;&quot;type&quot;:&quot;integer&quot;&#125;, &quot;birthday&quot;: &#123;&quot;type&quot;:&quot;date&quot;&#125;, &quot;interests&quot;: &#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125;, &quot;fielddata&quot;: true &#125; &#125; &#125; &#125;&#125; 3.23 如何计算相关度分数使用的是TF/IDF算法(Term Frequency&amp;Inverse Document Frequency) Term Frequency:我们查询的文本中的词条在document本中出现了多少次，出现次数越多，相关度越高 搜索内容： hello world Hello，I love china. Hello world,how are you! Inverse Document Frequency：我们查询的文本中的词条在索引的所有文档中出现了多少次，出现的次数越多，相关度越低 搜索内容：hello world hello，what are you doing? I like the world. hello 在索引的所有文档中出现了500次，world出现了100次 Field-length(字段长度归约) norm:field越长，相关度越低 搜索内容：hello world {“title”:”hello,what’s your name?”,”content”:{“owieurowieuolsdjflk”}} {“title”:”hi,good morning”,”content”:{“lkjkljkj…….world”}} 查看分数是如何计算的： GET /lib3/user/_search?explain=true &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;interests&quot;: &quot;duanlian,changge&quot; &#125; &#125;&#125; 查看一个文档能否匹配上某个查询： GET /lib3/user/2/_explain &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;interests&quot;: &quot;duanlian,changge&quot; &#125; &#125;&#125; 3.24 Doc Values 解析DocValues其实是Lucene在构建倒排索引时，会额外建立一个有序的正排索引(基于document =&gt; field value的映射列表) {“birthday”:”1985-11-11”,age:23} {“birthday”:”1989-11-11”,age:29} document age birthday doc1 23 1985-11-11 doc2 29 1989-11-11 存储在磁盘上，节省内存 对排序，分组和一些聚合操作能够大大提升性能 注意：默认对不分词的字段是开启的，对分词字段无效（需要把fielddata设置为true） PUT /lib3 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;age&quot;: &#123; &quot;type&quot;:&quot;integer&quot;, &quot;doc_values&quot;:false &#125;, &quot;interests&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;birthday&quot;: &#123;&quot;type&quot;:&quot;date&quot;&#125; &#125; &#125; &#125;&#125; 3.25 基于scroll技术滚动搜索大量数据如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用scoll滚动查询，一批一批的查，直到所有数据都查询完为止。 scoll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的 采用基于_doc(不使用_score)进行排序的方式，性能较高 每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了 GET /lib3/user/_search?scroll=1m &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;:[&quot;_doc&quot;], &quot;size&quot;:3&#125; GET /_search/scroll &#123; &quot;scroll&quot;: &quot;1m&quot;, &quot;scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoAwAAAAAAAAAdFkEwRENOVTdnUUJPWVZUd1p2WE5hV2cAAAAAAAAAHhZBMERDTlU3Z1FCT1lWVHdadlhOYVdnAAAAAAAAAB8WQTBEQ05VN2dRQk9ZVlR3WnZYTmFXZw==&quot;&#125; 3.26 dynamic mapping策略dynamic: true:遇到陌生字段就 dynamic mapping false:遇到陌生字段就忽略 strict:约到陌生字段就报错 PUT /lib8 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;dynamic&quot;:strict, &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;:&#123; &quot;type&quot;:&quot;object&quot;, &quot;dynamic&quot;:true &#125;, &#125; &#125; &#125;&#125; 会报错 PUT /lib8/user/1 &#123; &quot;name&quot;:&quot;lisi&quot;, &quot;age&quot;:20, &quot;address&quot;:&#123; &quot;province&quot;:&quot;beijing&quot;, &quot;city&quot;:&quot;beijing&quot; &#125;&#125; date_detection:默认会按照一定格式识别date，比如yyyy-MM-dd 可以手动关闭某个type的date_detection PUT /lib8 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;date_detection&quot;: false, &#125; &#125;&#125; 定制 dynamic mapping template(type) PUT /my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;en&quot;: &#123; &quot;match&quot;: &quot;*_en&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; ] &#125; &#125; &#125; 使用了模板 PUT /my_index/my_type/3 &#123; &quot;title_en&quot;: &quot;this is my dog&quot; &#125; 没有使用模板 PUT /my_index/my_type/5 &#123; &quot;title&quot;: &quot;this is my cat&quot;&#125; GET my_index/my_type/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;is&quot; &#125; &#125;&#125; 3.27重建索引一个field的设置是不能修改的，如果要修改一个field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入到index中。 批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scroll就查询指定日期的一段数据，交给一个线程即可。 PUT /index1/type1/4 &#123; &quot;content&quot;:&quot;1990-12-12&quot;&#125; GET /index1/type1/_search GET /index1/type1/_mapping 报错PUT /index1/type1/4 &#123; &quot;content&quot;:&quot;I am very happy.&quot;&#125; 修改content的类型为string类型,报错，不允许修改 PUT /index1/_mapping/type1 &#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;&#125; 创建一个新的索引，把index1索引中的数据查询出来导入到新的索引中但是应用程序使用的是之前的索引，为了不用重启应用程序，给index1这个索引起个#别名 PUT /index1/_alias/index2 创建新的索引，把content的类型改为字符串 PUT /newindex &#123; &quot;mappings&quot;: &#123; &quot;type1&quot;:&#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 使用scroll批量查询 GET /index1/type1/_search?scroll=1m &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [&quot;_doc&quot;], &quot;size&quot;: 2&#125; 使用bulk批量写入新的索引POST /_bulk{“index”:{“_index”:”newindex”,”_type”:”type1”,”_id”:1}}{“content”:”1982-12-12”} 将别名index2和新的索引关联，应用程序不用重启 POST /_aliases &#123; &quot;actions&quot;: [ &#123;&quot;remove&quot;: &#123;&quot;index&quot;:&quot;index1&quot;,&quot;alias&quot;:&quot;index2&quot;&#125;&#125;, &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;newindex&quot;,&quot;alias&quot;: &quot;index2&quot;&#125;&#125;]&#125; GET index2/type1/_search 3.28 索引不可变的原因倒排索引包括： 文档的列表，文档的数量，词条在每个文档中出现的次数，出现的位置，每个文档的长度，所有文档的平均长度 索引不变的原因： 不需要锁，提升了并发性能 可以一直保存在缓存中（filter） 节省cpu和io开销 第四节 在Java应用中访问ElasticSearch4.1在Java应用中实现查询文档pom中加入ElasticSearch6.2.4的依赖： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.2.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- java编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 4.2 在Java应用中实现添加文档 \"&#123;\" + \"\\\"id\\\":\\\"1\\\",\" + \"\\\"title\\\":\\\"Java设计模式之装饰模式\\\",\" + \"\\\"content\\\":\\\"在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能。\\\",\" + \"\\\"postdate\\\":\\\"2018-05-20 14:38:00\\\",\" + \"\\\"url\\\":\\\"csdn.net/79239072\\\"\" + \"&#125;\"XContentBuilder doc1 = XContentFactory.jsonBuilder() .startObject() .field(\"id\",\"3\") .field(\"title\",\"Java设计模式之单例模式\") .field(\"content\",\"枚举单例模式可以防反射攻击。\") .field(\"postdate\",\"2018-02-03\") .field(\"url\",\"csdn.net/79247746\") .endObject(); IndexResponse response = client.prepareIndex(\"index1\", \"blog\", null) .setSource(doc1) .get(); System.out.println(response.status()); 4.3在Java应用中实现删除文档DeleteResponse response=client.prepareDelete(\"index1\",\"blog\",\"SzYJjWMBjSAutsuLRP_P\").get();//删除成功返回OK，否则返回NOT_FOUNDSystem.out.println(response.status()); 4.4在Java应用中实现更新文档 UpdateRequest request=new UpdateRequest(); request.index(\"index1\") .type(\"blog\") .id(\"2\") .doc( XContentFactory.jsonBuilder().startObject() .field(\"title\",\"单例模式解读\") .endObject() );UpdateResponse response=client.update(request).get();//更新成功返回OK，否则返回NOT_FOUNDSystem.out.println(response.status());upsert方式：IndexRequest request1 =new IndexRequest(\"index1\",\"blog\",\"3\") .source( XContentFactory.jsonBuilder().startObject() .field(\"id\",\"3\") .field(\"title\",\"装饰模式\") .field(\"content\",\"动态地扩展一个对象的功能\") .field(\"postdate\",\"2018-05-23\") .field(\"url\",\"csdn.net/79239072\") .endObject() ); UpdateRequest request2=new UpdateRequest(\"index1\",\"blog\",\"3\") .doc( XContentFactory.jsonBuilder().startObject() .field(\"title\",\"装饰模式解读\") .endObject() ).upsert(request1); UpdateResponse response=client.update(request2).get(); //upsert操作成功返回OK，否则返回NOT_FOUNDSystem.out.println(response.status()); 4.5在Java应用中实现批量操作 MultiGetResponse mgResponse = client.prepareMultiGet() .add(\"index1\",\"blog\",\"3\",\"2\") .add(\"lib3\",\"user\",\"1\",\"2\",\"3\") .get(); for(MultiGetItemResponse response:mgResponse)&#123; GetResponse rp=response.getResponse(); if(rp!=null &amp;&amp; rp.isExists())&#123; System.out.println(rp.getSourceAsString()); &#125; &#125; bulk： BulkRequestBuilder bulkRequest = client.prepareBulk();bulkRequest.add(client.prepareIndex(\"lib2\", \"books\", \"4\") .setSource(XContentFactory.jsonBuilder() .startObject() .field(\"title\", \"python\") .field(\"price\", 68) .endObject() ) );bulkRequest.add(client.prepareIndex(\"lib2\", \"books\", \"5\") .setSource(XContentFactory.jsonBuilder() .startObject() .field(\"title\", \"VR\") .field(\"price\", 38) .endObject() ) ); //批量执行BulkResponse bulkResponse = bulkRequest.get(); System.out.println(bulkResponse.status());if (bulkResponse.hasFailures()) &#123; System.out.println(\"存在失败操作\"); &#125; 第五节 在Python应用中访问ElasticSearch5.1 创建 Indexfrom elasticsearch import Elasticsearches = Elasticsearch()result = es.indices.create(index='news', ignore=400)print(result) 5.2 删除 Indexfrom elasticsearch import Elasticsearches = Elasticsearch()result = es.indices.delete(index='news', ignore=[400, 404])print(result) 5.3 插入数据from elasticsearch import Elasticsearches = Elasticsearch()es.indices.create(index='news', ignore=400)data = &#123;'title': '美国留给伊拉克的是个烂摊子吗', 'url': 'http://view.news.qq.com/zt2011/usa_iraq/index.htm'&#125;result = es.create(index='news', doc_type='politics', id=1, body=data)print(result) 5.4 更新数据from elasticsearch import Elasticsearches = Elasticsearch()data = &#123; 'title': '美国留给伊拉克的是个烂摊子吗', 'url': 'http://view.news.qq.com/zt2011/usa_iraq/index.htm', 'date': '2011-12-16'&#125;result = es.update(index='news', doc_type='politics', body=data, id=1)print(result) 5.5 删除数据from elasticsearch import Elasticsearches = Elasticsearch()result = es.delete(index='news', doc_type='politics', id=1)print(result) 5.6 查询数据from elasticsearch import Elasticsearches = Elasticsearch()mapping = &#123; 'properties': &#123; 'title': &#123; 'type': 'text', 'analyzer': 'ik_max_word', 'search_analyzer': 'ik_max_word' &#125; &#125;&#125;es.indices.delete(index='news', ignore=[400, 404])es.indices.create(index='news', ignore=400)result = es.indices.put_mapping(index='news', doc_type='politics', body=mapping)print(result)","categories":[{"name":"数据库","slug":"数据库","permalink":"http://lucas0625.github.io/blog/categories/数据库/"}],"tags":[{"name":"es","slug":"es","permalink":"http://lucas0625.github.io/blog/tags/es/"}]},{"title":"正则表达式 简要手册","slug":"regex","date":"2019-04-29T03:32:03.363Z","updated":"2019-05-09T09:52:51.447Z","comments":true,"path":"2019/04/29/regex/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/29/regex/","excerpt":"正则表达式的简单梳理","text":"正则表达式的简单梳理 正则表达式基本语法 python中使用正则表达式import re pattern = re.compile(expression) 编译匹配模式 re.match(pattern, str) 从字符串开头开始匹配，返回match对象，使用group(0)取出匹配到的字符 re.search(pattern, str) 搜索字符串，返回第一个匹配到的match对象，使用group(0)取出匹配到的字符 re.findall(pattern, str) 以列表的形式返回所有匹配到的字符 re.finditer(pattern, str) 以迭代器的形式返回所有匹配到的字符 re.sub(pattern, repl_str, str) 将str中匹配到的字符替换成repl_str 正则表达式练习题正则表达式在线练习网址: HackerRank Regex Golf s1 = “get-element-by-id”; 给定这样一个连字符串，写一个function转换为驼峰命名法形式的字符串 getElementById import rea = 'get-element-by-id'pattern = re.compile('-[a-z]')for item in re.findall(pattern, a): a = a.replace(item, item[-1].upper())print(a) getElementById 判断字符串是否包含数字 import repattern = re.compile('\\d')for i in ['asdfdsf', 'asdfdsfs43', '343']: print(True if re.search(pattern, i) else False) False True True 判断电话号码 import repattern = re.compile('^1[34578]\\d&#123;9&#125;$')for i in ['1233456', '13366789980', '18678900875']: print(True if re.search(pattern, i) else False) False True True 判断是否符合指定格式，给定字符串str，检查其是否符合如下格式 XXX-XXX-XXXX 其中X为Number类型 import repattern = re.compile('^(\\d&#123;3&#125;-)&#123;2&#125;\\d&#123;4&#125;$')for i in ['XXX-XXX-XXXX', '123-456-6889', '1232-434-1314']: print(True if re.search(pattern, i) else False) False True False 判断是否符合USD格式，给定字符串 str，检查其是否符合美元书写格式 以 $ 开始 整数部分，从个位起，满 3 个数字用 , 分隔 如果为小数，则小数部分长度为 2 正确的格式如：$1,023,032.03 或者 $2.03，错误的格式如：$3,432,12.12 或者 $34,344.3 import repattern = re.compile('^\\$\\d&#123;1,3&#125;(,\\d&#123;3&#125;)*(\\.\\d&#123;2&#125;)?$')for i in ['$1,023,032.03', '$2.03', '$3,432,12.12', '$34,344.3']: print(True if re.search(pattern, i) else False) True True False False JS实现千位分隔符 import redef formatNum(num): if isinstance(num, int): num = str(num) pattern = re.compile(r'(\\d+)(\\d&#123;3&#125;)((,\\d&#123;3&#125;)*)') while True: num, count = re.subn(pattern, r'\\1,\\2\\3', num) if count == 0: break return numprint(formatNum(43532412245353)) 43,532,412,245,353 验证邮箱 import repattern = re.compile('^[a-zA-Z0-9_\\-]+@[a-zA-Z0-9_\\-]+\\.[a-zA-Z0-9_\\-]+$')for i in ['12@163.com', '244@qq.com', 'dsf@dsf']: print(True if re.search(pattern, i) else False) True True False 验证身份证号码, 身份证号码可能为15位或18位，15位为全数字，18位中前17位为数字，最后一位为数字或者X import repattern = re.compile('^\\d&#123;15&#125;$|^\\d&#123;17&#125;[0-9Xx]$')for i in ['34345345', '123456789012345', '12345678901234567X']: print(True if re.search(pattern, i) else False) False True True 匹配汉字 import repattern = re.compile('^[\\u4E00-\\u9FA5]*$')for i in ['34sd', '', '我们']: print(True if re.search(pattern, i) else False) False True True 去除首尾的’/‘ a = '/sdf/'a.strip('/')print(a) sdf 判断日期格式是否符合 ‘2017-05-11’的形式，简单判断，只判断格式 import repattern = re.compile('^\\d&#123;4&#125;\\-\\d&#123;1,2&#125;\\-\\d&#123;1,2&#125;$') 十六进制颜色正则 import repattern = re.compile('^#?([a-fA-F0-9]&#123;6&#125;|[a-fA-F0-9]&#123;3&#125;)$') 车牌号正则 import repattern = re.compile('^[京津沪渝冀豫云辽黑湘皖鲁新苏浙赣鄂桂甘晋蒙陕吉闽贵粤青藏川宁琼使领A-Z]&#123;1&#125;[A-Z]&#123;1&#125;[A-Z0-9]&#123;4&#125;[A-Z0-9挂学警港澳]&#123;1&#125;$')re.search(pattern , str) 密码强度正则，最少6位，包括至少1个大写字母，1个小写字母，1个数字，1个特殊字符 import repattern = re.compile('^.*(?=.&#123;6,&#125;)(?=.*\\d)(?=.*[A-Z])(?=.*[a-z])(?=.*[!@#$%^&amp;*? ]).*$') 匹配浮点数 import repattern = re.compile('^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$')","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"re","slug":"re","permalink":"http://lucas0625.github.io/blog/tags/re/"}]},{"title":"Git 常用操作指南","slug":"Git常用操作指南","date":"2019-04-28T08:11:53.115Z","updated":"2019-04-28T08:11:53.115Z","comments":true,"path":"2019/04/28/Git常用操作指南/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/28/Git常用操作指南/","excerpt":"本文介绍自己平时工作中常用的一些Git操作","text":"本文介绍自己平时工作中常用的一些Git操作 Git 常用操作指南.gitignore规范 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。[注]:所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（*）匹配零个或多个任意字符 [abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c） 问号（?）只匹配一个任意字符 如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 example: # 此为注释 – 将被 Git 忽略 # 忽略所有 .a 结尾的文件 *.a # 但 lib.a 除外 !lib.a # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO /TODO # 忽略 build/ 目录下的所有文件 build/ # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt doc/*.txt 查看文件之间的差别 git diff file 查看工作目录中当前文件和暂存区域快照之间的差异 git diff –cached / –staged 查看已经暂存起来的文件和上次提交时的快照之间的差异 提交文件 git commit -m ‘up’ 将暂存区的文件提交 git commit -a -m ‘up’ 跳过add步骤，将工作区的文件直接提交 删除文件 git rm file 从暂存区删除，并连带从工作目录中删除指定的文件 git rm -f file 如何删除之前修改过，并且已加入暂存区，则必须强制删除 git rm –cached file 将文件从git仓库中删除，但是仍保留在工作目录中 修改文件名 git mv file_form file_to 查看提交历史 git log 按照提交历史列出所有更新，最近的更新排在最上面 git log -p -n 显示每次提交的内容差异，显示最近的n次提交 git log –stat 显示每次提交的简要增改行统计 git log –pretty=oneline 将每个提交放在一行显示 git log –pretty=format:”%h - %an, %ar : %s” 定制要显示的记录格式 选项 说明 %H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字. 实际作出修改的人 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 -date= 选项定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字. 最后将此工作成果提交到仓库的人 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期，按多久以前的方式显示 %s 提交说明 git log –pretty=format:”%h %s” –graph 增加ASCII字符串表示的简单图形，展示每个提交所在的分支及其分化衍合情况 其他选项 选项 说明 -p 按补丁格式显示每个更新之间的差异。 --stat 显示每次更新的文件修改统计信息。 --shortstat 只显示 --stat 中最后的行数修改添加移除统计。 --name-only 仅在提交信息后显示已修改的文件清单。 --name-status 显示新增、修改、删除的文件清单。 --abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 --relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 --graph 显示 ASCII 图形表示的分支合并历史。 --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 git log –since=2.weeks 显示最近两周的提交 选项 说明 -(n) 仅显示最近的 n 条提交 --since, --after 仅显示指定时间之后的提交。 --until, --before 仅显示指定时间之前的提交。 --author 仅显示指定作者相关的提交。 --committer 仅显示指定提交者相关的提交。 撤销操作 git commit –amend 撤销刚才的提交操作，使用当前的暂存区域快照提交，如果刚才提交完没有作任何改动，相当于有机会重新编辑提交说明 git reset HEAD file 取消已暂存的文件 git checkout – file 取消工作区对文件的修改!!!!!!!!!!(此过程不可逆) 管理远程仓库 git remote 列出远程仓库的名字 git remote -v 显示对应的克隆地址 git pull 拉取远程仓库的更新 git push 推送数据到远程仓库 分支操作 git branch testing 新建testing分支 git branch 显示当前分支 git branch -a 显示所有分支 git checkout testing 转换到testing分支 git checkout -b testing 新建testing分支，并转向testing分支 git branch -d testing 删除testing分支 git merge testing 合并testing分支到当前分支 合并方式包括简单的指针前进操作 fast forward 或者多方合并 recursive， 当合并有冲突时，任何包含未解决冲突的文件都会以unmerged状态存在。&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 当前分支============&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;要合并的分支解决完冲突之后 执行add, commit 提交到仓库 git branch -v 查看各个分支最后一个提交的对象的信息 git branch –merged 筛选出与当前分支合并的分支，一般来说列表中没有*的分支通常都可以删除 git branch –no-merged 筛选出尚未与当前分支合并的分支 git rebase master 把当前分支里提交的改变移动到master分支里 git rebase –onto master server client 取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它在 master 上重演一遍 git rebase master server 取出特性分支 server，然后在主分支 master 上重演 储藏(stashing)操作“‘储藏”“可以获取你工作目录的中间状态——也就是你修改过的被追踪的文件和暂存的变更——并将它保存到一个未完结变更的堆栈中，随时可以重新应用。 git stash 往堆栈推送一个新的储藏， git stash list 查看现有的储藏 git stash apply 恢复最近一次储藏，但是不从堆栈中删除该储藏 git stash apply stash@{2} 恢复指定储藏，但是不从堆栈中删除该储藏 git stash pop 恢复最新一次储藏，并从堆栈中删除它 git stash drop stash@{2} 删除指定储藏","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://lucas0625.github.io/blog/tags/Git/"}]},{"title":"Python 虚拟环境的使用","slug":"Python 虚拟环境的使用","date":"2019-04-28T07:14:06.892Z","updated":"2019-04-28T07:14:06.892Z","comments":true,"path":"2019/04/28/Python 虚拟环境的使用/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/28/Python 虚拟环境的使用/","excerpt":"简要介绍python中虚拟环境的使用","text":"简要介绍python中虚拟环境的使用 Python 虚拟环境的使用安装virtualenvpip install virtualenv 创建虚拟环境在当前目录中创建一个文件夹，实际上就是将python环境克隆了一份，包括python解释器，setuptools， pip， wheel， 以及python标准库virtualenv venv 使用虚拟环境激活后，系统提示符左侧会显示虚拟环境的名字，(venv)source venv/bin/activate 退出虚拟环境deactivate 在虚拟环境中安装包pip install -r requirements.txt 将虚拟环境中的依赖库打包pip freeze &gt; requirements.txt 删除虚拟环境rm -rf venv","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/blog/categories/编程/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"Glove 原理与实现","slug":"Glove","date":"2019-04-25T12:15:37.175Z","updated":"2019-04-25T12:15:37.175Z","comments":true,"path":"2019/04/25/Glove/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/Glove/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"TextRNN 原理与实现","slug":"TextRNN","date":"2019-04-25T06:22:25.197Z","updated":"2019-04-25T06:22:25.197Z","comments":true,"path":"2019/04/25/TextRNN/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/TextRNN/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"FastText 原理与实现","slug":"FastText","date":"2019-04-25T02:30:15.713Z","updated":"2019-04-25T02:30:15.713Z","comments":true,"path":"2019/04/25/FastText/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/FastText/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Word2Vec 原理与实现","slug":"Word2Vec","date":"2019-04-25T02:30:06.908Z","updated":"2019-04-25T02:30:06.909Z","comments":true,"path":"2019/04/25/Word2Vec/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/Word2Vec/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"BERT 原理与实现","slug":"BERT","date":"2019-04-25T02:29:55.578Z","updated":"2019-04-25T02:29:55.578Z","comments":true,"path":"2019/04/25/BERT/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/BERT/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Transformer 原理与实现","slug":"Transformer","date":"2019-04-25T02:29:27.661Z","updated":"2019-04-25T02:29:27.661Z","comments":true,"path":"2019/04/25/Transformer/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/Transformer/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Bi-LSTM 原理与实现","slug":"Bi-LSTM","date":"2019-04-25T02:28:40.370Z","updated":"2019-04-25T02:28:40.370Z","comments":true,"path":"2019/04/25/Bi-LSTM/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/Bi-LSTM/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"TextLSTM 原理与实现","slug":"TextLSTM","date":"2019-04-25T02:27:57.789Z","updated":"2019-04-25T02:27:57.790Z","comments":true,"path":"2019/04/25/TextLSTM/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/TextLSTM/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"TextCNN 原理与实现","slug":"TextCNN","date":"2019-04-25T02:27:21.661Z","updated":"2019-04-25T02:27:21.662Z","comments":true,"path":"2019/04/25/TextCNN/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/TextCNN/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"NNLM 原理与实现","slug":"NNLM","date":"2019-04-25T02:25:41.973Z","updated":"2019-04-25T02:25:41.973Z","comments":true,"path":"2019/04/25/NNLM/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/NNLM/","excerpt":"","text":"","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://lucas0625.github.io/blog/categories/自然语言处理/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Pytorch 基本概念","slug":"基本概念","date":"2019-02-04T16:00:00.000Z","updated":"2019-05-13T06:17:35.645Z","comments":true,"path":"2019/02/05/基本概念/","link":"","permalink":"http://lucas0625.github.io/blog/2019/02/05/基本概念/","excerpt":"本文介绍Pytorch的基本概念。","text":"本文介绍Pytorch的基本概念。 Pytorch 基本概念import torchimport numpy as npimport pandas as pdfrom torch import nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoader torch.cuda.is_available() False Tensor 张量表示的是一个多维矩阵，零维就是一个点，一维就是向量，二维就是一般的矩阵，多维就相当于一个多维矩阵，Tensor可以和numpy的ndarray相互对应，Tensor可以和ndarray相互转换 a = torch.Tensor([[2, 3], [4, 8], [7, 9]])print('a is : &#123;&#125;'.format(a))print('a size is &#123;&#125;'.format(a.size())) a is : tensor([[2., 3.], [4., 8.], [7., 9.]]) a size is torch.Size([3, 2]) b = torch.LongTensor([[2, 3], [4, 8],[7, 9]])print('b is : &#123;&#125;'.format(b)) b is : tensor([[2, 3], [4, 8], [7, 9]]) c = torch.zeros((3, 2))print('zero tensor : &#123;&#125;'.format(c))d = torch.randn((3, 2))print('randn tensor is : &#123;&#125;'.format(d)) zero tensor : tensor([[0., 0.], [0., 0.], [0., 0.]]) randn tensor is : tensor([[-0.7097, -0.2020], [-0.1451, -0.3853], [-0.9366, 1.1942]]) a[0, 1] = 100print('changed a is : &#123;&#125;'.format(a)) changed a is : tensor([[ 2., 100.], [ 4., 8.], [ 7., 9.]]) numpy_b = b.numpy()print('conver to numpy is \\n &#123;&#125;'.format(numpy_b)) conver to numpy is [[2 3] [4 8] [7 9]] e = np.array([[2, 3], [4, 5]])torch_e = torch.from_numpy(e)print('from numpy to touch.Tensor is &#123;&#125;'.format(torch_e))f_torch_e = torch_e.float()print('change data type to float tensor : &#123;&#125;'.format(f_torch_e)) from numpy to touch.Tensor is tensor([[2, 3], [4, 5]]) change data type to float tensor : tensor([[2., 3.], [4., 5.]]) Variable 变量variable 提供了自动求导功能，Variable和Tensor没有本质区别，不过Variable会被放入一个计算图中，然后进行前向传播，反向传播，自动求导 Variable有三个比较重要的组成属性：data， grad， grad_fn data： 可以取出Variable中的Tensor数值 grad_fn： 表示的是得到这个Variable的操作，比如加减或者乘除 grad： 是这个Variable的反向传播梯度 # creat Variablex = Variable(torch.Tensor([1]), requires_grad=True)w = Variable(torch.Tensor([2]), requires_grad=True)b = Variable(torch.Tensor([3]), requires_grad=True)# build a computational graphy = w * x + b# compute gradientsy.backward()print(x.grad)print(w.grad)print(b.grad) tensor([2.]) tensor([1.]) tensor([1.]) # 矩阵求导x = torch.randn(3)x = Variable(x, requires_grad=True)y = x * 2print(y)y.backward(torch.FloatTensor([1, 0.1, 0.01]))print(x.grad) tensor([-0.6298, 1.7484, -1.1588], grad_fn=&lt;MulBackward0&gt;) tensor([2.0000, 0.2000, 0.0200]) Dataset 数据集torch.utils.data.Dataset 代表这一数据的抽象类吗可以自己定义数据类的继承和重写，只需要定义__len__和 __getitem__ 两个函数 class myDataset(Dataset): def __init__(self, csv_file, txt_file, root_dir, other_file): self.csv_data = pd.read_csv(csv_file) with open(txt_file, 'r') as f: data_list = f.readlines() self.txt_data = data_list self.root_dir = root_dir def __len__(self): return len(self.csv_data) def __getitem__(self, idx): data = (self.csv_file[idx], self.txt_file[idx]) return data # DataLoader 来定义一个新的迭代器# dataiter = DataLoader(myDataset, batch_size=32, shuffle=True, collate_fn=default_collate) nn.Module 模组PyTorch 编写的神经网络，所有层结构和损失函数都来自于torch.nn，所有的模型构建都从这个基类nn.Module继承 # 构建计算图 也就是模型class net_name(nn.Module): def __init__(self, other_arguments): super(net_name, self).__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size) # other network layer def forward(self, x): x = self.conv1(x) return x # 定义损失函数criterion = nn.CrossEntropyLoss()# loss = criterion(output, target) torch.optim 优化在机器学习或者深度学习中，我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略 优化算法分类： 一阶优化算法 梯度下降 二阶优化算法 使用二阶倒数，也叫hessian方法 # lr 学习率 momentum 动量# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 模型的保存和加载两种保存方式： 保存整个模型的结构信息和参数信息，保存的对象是模型model 保存模型的参数，保存的对象是模型的状态model.state_dict() 两种加载方式： 加载完整的模型结构和参数信息，网络较大的时候记载时间比较长，同时存储空间也比较大 加载模型参数信息，需要先导入模型的结构，然后再导入模型 # 保存# torch.save(model, './model.path')# torch.save(model.state_dict(), './model_state.path')# 加载# load_model = torch.load('model.path')# 先加载模型，再加载参数# model.load_state_dict(torch.load('model_state.path'))","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/blog/tags/Pytorch/"}]},{"title":"Logistic 回归","slug":"Logistic回归","date":"2019-02-04T16:00:00.000Z","updated":"2019-05-13T05:52:30.635Z","comments":true,"path":"2019/02/05/Logistic回归/","link":"","permalink":"http://lucas0625.github.io/blog/2019/02/05/Logistic回归/","excerpt":"Logistic 回归理论与实战","text":"Logistic 回归理论与实战 Logistic 回归模型Sigmoid 函数Sigmoid 函数，公示如下:$$f(x)=\\frac{1}{1+e^{-x}}$$ 图像如下: 损失函数 $$\\operatorname{loss} =-(y \\log (\\hat{y})+(1-y) \\log (1-\\hat{y}))$$ $$\\operatorname{loss}=-(\\log (1-\\hat{y}))$$ $$\\operatorname{loss}=-(\\log (\\hat{y}))$$ 模型 代码import torchfrom torch.autograd import Variableimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline # 设定随机种子torch.manual_seed(2017) &lt;torch._C.Generator at 0x10d916550&gt; # 从 data.txt 中读入点with open('./Logistic_data.txt', 'r') as f: data_list = [i.split('\\n')[0].split(',') for i in f.readlines()] data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]# 标准化x0_max = max([i[0] for i in data])x1_max = max([i[1] for i in data])data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点plot_x0 = [i[0] for i in x0]plot_y0 = [i[1] for i in x0]plot_x1 = [i[0] for i in x1]plot_y1 = [i[1] for i in x1]plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x112a31f98&gt; np_data = np.array(data, dtype='float32') # 转换成 numpy arrayx_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1] 实现Sigmoid 的函数，Sigmoid 函数的公式为$$f(x) = \\frac{1}{1 + e^{-x}}$$ # 定义 sigmoid 函数def sigmoid(x): return 1 / (1 + np.exp(-x)) # 画出 sigmoid 的图像plot_x = np.arange(-10, 10.01, 0.01)plot_y = sigmoid(plot_x)plt.plot(plot_x, plot_y, 'r') [&lt;matplotlib.lines.Line2D at 0x115df7fd0&gt;] x_data = Variable(x_data)y_data = Variable(y_data) # 定义 logistic 回归模型w = Variable(torch.randn(2, 1), requires_grad=True) b = Variable(torch.zeros(1), requires_grad=True)def logistic_regression(x): return torch.sigmoid(torch.mm(x, w) + b) # 画出参数更新之前的结果w0 = w[0].data[0]w1 = w[1].data[0]b0 = b.data[0]plot_x = np.arange(0.2, 1, 0.01)plot_x = torch.from_numpy(plot_x) # 转换成Tensorplot_y = (-w0 * plot_x - b0) / w1plot_x = plot_x.numpy() # 转换成numpyplot_y = plot_y.numpy() # 转换成numpyplt.plot(plot_x, plot_y, 'g', label='cutting line')plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x11686cc18&gt; 可以看到分类效果基本是混乱的，我们来计算一下 loss，公式如下 $$loss = -(y log(\\hat{y}) + (1 - y) log(1 - \\hat{y}))$$ # 计算lossdef binary_loss(y_pred, y): logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean() return -logits y_pred = logistic_regression(x_data)loss = binary_loss(y_pred, y_data)print(loss) tensor(0.7911, grad_fn=&lt;NegBackward&gt;) # 自动求导并更新参数loss.backward()w.data = w.data - 0.1 * w.grad.datab.data = b.data - 0.1 * b.grad.data# 算出一次更新之后的lossy_pred = logistic_regression(x_data)loss = binary_loss(y_pred, y_data)print(loss) tensor(0.7801, grad_fn=&lt;NegBackward&gt;) # 使用 torch.optim 更新参数from torch import nnw = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))optimizer = torch.optim.SGD([w, b], lr=1.) # 进行 1000 次更新import timestart = time.time()for e in range(1000): # 前向传播 y_pred = logistic_regression(x_data) loss = binary_loss(y_pred, y_data) # 计算 loss # 反向传播 optimizer.zero_grad() # 使用优化器将梯度归 0 loss.backward() optimizer.step() # 使用优化器来更新参数 # 计算正确率 mask = y_pred.ge(0.5).float() acc = (mask == y_data).sum().data.item() / y_data.shape[0] if (e + 1) % 200 == 0: print('epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;'.format(e+1, loss.data.item(), acc))during = time.time() - startprint()print('During Time: &#123;:.3f&#125; s'.format(during)) epoch: 200, Loss: 0.32431, Acc: 0.91000 epoch: 400, Loss: 0.29052, Acc: 0.91000 epoch: 600, Loss: 0.27069, Acc: 0.91000 epoch: 800, Loss: 0.25759, Acc: 0.90000 epoch: 1000, Loss: 0.24827, Acc: 0.89000 During Time: 0.304 s # 画出更新之后的结果w0 = w[0].data[0]w1 = w[1].data[0]b0 = b.data[0]plot_x = np.arange(0.2, 1, 0.01)plot_x = torch.from_numpy(plot_x) # 转换成Tensorplot_y = (-w0 * plot_x - b0) / w1plot_x = plot_x.numpy() # 转换成numpyplot_y = plot_y.numpy() # 转换成numpyplt.plot(plot_x, plot_y, 'g', label='cutting line')plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x116526ef0&gt; 前面我们使用了自己写的 loss，其实 PyTorch 已经为我们写好了一些常见的 loss，比如线性回归里面的 loss 是 nn.MSE()，而 Logistic 回归的二分类 loss 在 PyTorch 中是 nn.BCEWithLogitsLoss()，关于更多的 loss，可以查看文档 PyTorch 为我们实现的 loss 函数有两个好处，第一是方便我们使用，不需要重复造轮子，第二就是其实现是在底层 C++ 语言上的，所以速度上和稳定性上都要比我们自己实现的要好 另外，PyTorch 出于稳定性考虑，将模型的 Sigmoid 操作和最后的 loss 都合在了 nn.BCEWithLogitsLoss()，所以我们使用 PyTorch 自带的 loss 就不需要再加上 Sigmoid 操作了 # 使用自带的losscriterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性w = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))def logistic_reg(x): return torch.mm(x, w) + boptimizer = torch.optim.SGD([w, b], 1.) y_pred = logistic_reg(x_data)loss = criterion(y_pred, y_data)print(loss.data) tensor(0.6650) # 同样进行 1000 次更新start = time.time()for e in range(1000): # 前向传播 y_pred = logistic_reg(x_data) loss = criterion(y_pred, y_data) # 反向传播 optimizer.zero_grad() loss.backward() optimizer.step() # 计算正确率 mask = y_pred.ge(0.5).float() acc = (mask == y_data).sum().data.item() / y_data.shape[0] if (e + 1) % 200 == 0: print('epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;'.format(e+1, loss.data.item(), acc))during = time.time() - startprint()print('During Time: &#123;:.3f&#125; s'.format(during)) epoch: 200, Loss: 0.39010, Acc: 0.87000 epoch: 400, Loss: 0.32184, Acc: 0.87000 epoch: 600, Loss: 0.28917, Acc: 0.87000 epoch: 800, Loss: 0.26983, Acc: 0.87000 epoch: 1000, Loss: 0.25700, Acc: 0.88000 During Time: 0.202 s 可以看到，使用了 PyTorch 自带的 loss 之后，速度有了一定的上升，虽然看上去速度的提升并不多，但是这只是一个小网络，对于大网络，使用自带的 loss 不管对于稳定性还是速度而言，都有质的飞跃，同时也避免了重复造轮子的困扰","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://lucas0625.github.io/blog/categories/深度学习/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://lucas0625.github.io/blog/tags/Pytorch/"}]},{"title":"爬虫基础","slug":"爬虫基础","date":"2019-02-03T16:00:00.000Z","updated":"2019-02-04T09:17:45.054Z","comments":true,"path":"2019/02/04/爬虫基础/","link":"","permalink":"http://lucas0625.github.io/blog/2019/02/04/爬虫基础/","excerpt":"本文介绍爬虫的一些基础知识。","text":"本文介绍爬虫的一些基础知识。 爬虫基本原理讲解什么是爬虫自动获取网络资源机器人。请求网站并提取数据的自动化程序。获取的是html代码，所需要的数据保存在html代码中，接下来就是从html中提取出想要的信息，然后将信息存储在数据库。 爬虫的基本流程 发起请求通过HTTP库像目标站点发起请求，即发送一个Request，请求包含额外的headers等信息，等待服务器响应。 获取响应内容如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。 解析内容得到的内容可能是HTML，可以用正则表达式，网页解析库进行解析，可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。 保存数据保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。 Request中包含的内容 请求方式主要有GET，POST两种类型，另外还有HEAD，PUT，DELETE，OPTIONS等。GET请求： 请求的参数都包含在请求网址里面，即包含在Request url中；可以直接输入url然后回车直接访问。POST请求：请求的参数包含在Form Data中，不包含在请求网址中；必须构建表单，点击表单提交，才可以构造一个POST请求。 请求URLURL全称是统一资源定位符，如一个网页文档，一张图片，一个视频等都可以用URL唯一来确定。 请求头包含请求时的头部信息（配置信息），如User-Agent，Host，Cookies等信息。 请求体请求时额外携带的数据，如表单提交时的表单数据。GET请求时一般不包含，POST请求时加入Form Data信息 Response中包含的内容 响应状态Status Code，有多种响应状态，如200代表成功，301跳转，404找不到页面，502服务器错误。 响应头如内容类型，内容长度，服务器信息，设置Cookie等等。 响应体最主要的部分，包含了请求资源的内容，如网页HTML，图片，二进制数据等。 爬虫可以抓取什么样的数据 网页文本如HTML文档，Json格式文本等。 图片获取到的是二进制文件，保存为图片格式。 视频同为二进制文件，保存为视频格式即可。4.其他只要是能请求到的，都能获取。 怎么进行解析 直接处理返回最简单的字符串，可以对字符串直接进行处理。这种处理的网站比较简单。 Json解析XHR标签，解析json 正则表达式 Beautifulsoup解析 PyQuery解析 XPath解析 为什么我抓到的数据和浏览器看到的不一样抓到的只是网页源代码，浏览器看得到代码是通过JS渲染过的 解决JavaScript渲染问题 分析Ajax请求返回的结果是Json格式字符串 使用Selenium/Webdriver驱动一个浏览器 Splash 模拟JavaScript PyV8，Ghost.py进行模拟加载 怎样保存数据 文本存文本，Json，Xml等。 关系型数据库如MySQL，Oracle，SQL Server，等具有结构化表结构形式存储。 非关系型数据库如MongoDB，Redis等key-value形式存储。 环境配置所需环境python3.6, anacondaMongoDB 非关系型数据库。key,value型数据库 环境安装首先安装homebrew,然后利用homebrew安装mongodbbrew install mongodb Redis 非关系型数据库。key,value型数据库，分布式爬虫需要用到。 brew install redis MySQL 关系型数据库，体积小，使用方便，做数据存储时简便易用brew install mysql Python多版本共存 windows:环境变量：where python 可以查看各版本Pythonmac:echo $PATH 输出环境变量 爬虫常用库安装 请求库 urllib urllib.request re requests selenium 驱动浏览器，用来做自动化测试 chromedriver 驱动chrome浏览器 phantomjs 无界面浏览器 解析库 lxml 提供了xpath解析方式 beautifulsoup4 (bs4)网页解析库，依赖于lxml pyquery 网页解析库，相对于bs4更加方便 存储库 pymysql 用Python操作MySQL数据库 pymongo 用Python操作MongoDB数据库 redis 用Python操作Redis数据库 flask web库，做web代理的时候会用到 django web服务器框架，提供了完整的后台管理。做分布式爬虫管理的时候会用到 jupyter 运行在网页上的记事本Urllibk库基本使用Urllib库组成 urllib.request 请求模块,用来请求url urllib.error 异常处理模块，捕捉请求时出现的错误 urllib.parse url解析模块 urllib.robotparse robot.txt解析模块 urlopenurllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com')print(response.read().decode('utf-8')) import urllib.parseimport urllib.requestdata = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')response = urllib.request.urlopen('http://httpbin.org/post', data=data)print(response.read()) b&apos;{\\n &quot;args&quot;: {}, \\n &quot;data&quot;: &quot;&quot;, \\n &quot;files&quot;: {}, \\n &quot;form&quot;: {\\n &quot;word&quot;: &quot;hello&quot;\\n }, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;1&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Content-Length&quot;: &quot;10&quot;, \\n &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;89667a57-c909-475a-9870-f01181e8c85d&quot;\\n }, \\n &quot;json&quot;: null, \\n &quot;origin&quot;: &quot;219.238.82.169&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/post&quot;\\n}\\n&apos; import urllib.requestresponse = urllib.request.urlopen('http://httpbin.org/get', timeout=1)print(response.read()) b&apos;{\\n &quot;args&quot;: {}, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;0&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;40948f0e-e4b2-4b5f-9d84-aeb77595ca52&quot;\\n }, \\n &quot;origin&quot;: &quot;219.238.82.169&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\\n}\\n&apos; import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)except urllib.error.URLError as e: if isinstance(e.reason, socket.timeout): print('TIME OUT') TIME OUT 响应响应类型import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(type(response)) &lt;class &apos;http.client.HTTPResponse&apos;&gt; 状态码、响应头import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(response.status)print(response.getheaders())print(response.getheader('Server')) 200 [(&apos;Server&apos;, &apos;nginx&apos;), (&apos;Content-Type&apos;, &apos;text/html; charset=utf-8&apos;), (&apos;X-Frame-Options&apos;, &apos;SAMEORIGIN&apos;), (&apos;X-Clacks-Overhead&apos;, &apos;GNU Terry Pratchett&apos;), (&apos;Content-Length&apos;, &apos;47436&apos;), (&apos;Accept-Ranges&apos;, &apos;bytes&apos;), (&apos;Date&apos;, &apos;Wed, 22 Mar 2017 15:40:16 GMT&apos;), (&apos;Via&apos;, &apos;1.1 varnish&apos;), (&apos;Age&apos;, &apos;3417&apos;), (&apos;Connection&apos;, &apos;close&apos;), (&apos;X-Served-By&apos;, &apos;cache-itm7426-ITM&apos;), (&apos;X-Cache&apos;, &apos;HIT&apos;), (&apos;X-Cache-Hits&apos;, &apos;16&apos;), (&apos;X-Timer&apos;, &apos;S1490197216.605863,VS0,VE0&apos;), (&apos;Vary&apos;, &apos;Cookie&apos;), (&apos;Public-Key-Pins&apos;, &apos;max-age=600; includeSubDomains; pin-sha256=&quot;WoiWRyIOVNa9ihaBciRSC7XHjliYS9VwUGOIud4PB18=&quot;; pin-sha256=&quot;5C8kvU039KouVrl52D0eZSGf4Onjo4Khs8tmyTlV3nU=&quot;; pin-sha256=&quot;5C8kvU039KouVrl52D0eZSGf4Onjo4Khs8tmyTlV3nU=&quot;; pin-sha256=&quot;lCppFqbkrlJ3EcVFAkeip0+44VaoJUymbnOaEUk7tEU=&quot;; pin-sha256=&quot;TUDnr0MEoJ3of7+YliBMBVFB4/gJsv5zO7IxD9+YoWI=&quot;; pin-sha256=&quot;x4QzPSC810K5/cMjb05Qm4k3Bw5zBn4lTdO/nEW/Td4=&quot;;&apos;), (&apos;Strict-Transport-Security&apos;, &apos;max-age=63072000; includeSubDomains&apos;)] nginx import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(response.read().decode('utf-8')) Requestimport urllib.requestrequest = urllib.request.Request('https://python.org')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) from urllib import request, parseurl = 'http://httpbin.org/post'headers = &#123; 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) { &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: { &quot;name&quot;: &quot;Germey&quot; }, &quot;headers&quot;: { &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connect-Time&quot;: &quot;1&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Total-Route-Time&quot;: &quot;0&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;, &quot;Via&quot;: &quot;1.1 vegur&quot;, &quot;X-Request-Id&quot;: &quot;f96e736e-0b8a-4ab4-9dcc-a970fcd2fbbf&quot; }, &quot;json&quot;: null, &quot;origin&quot;: &quot;219.238.82.169&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot; } from urllib import request, parseurl = 'http://httpbin.org/post'dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, method='POST')req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')response = request.urlopen(req)print(response.read().decode('utf-8')) { &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: { &quot;name&quot;: &quot;Germey&quot; }, &quot;headers&quot;: { &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connect-Time&quot;: &quot;0&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Total-Route-Time&quot;: &quot;0&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;, &quot;Via&quot;: &quot;1.1 vegur&quot;, &quot;X-Request-Id&quot;: &quot;a624bcaa-3581-4b93-84b0-037940338e71&quot; }, &quot;json&quot;: null, &quot;origin&quot;: &quot;219.238.82.169&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot; } Handler代理import urllib.requestproxy_handler = urllib.request.ProxyHandler(&#123; 'http': 'http://127.0.0.1:9743', 'https': 'https://127.0.0.1:9743'&#125;)opener = urllib.request.build_opener(proxy_handler)response = opener.open('http://httpbin.org/get')print(response.read()) b&apos;{\\n &quot;args&quot;: {}, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;2&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;b0e2272d-1663-4192-ac45-eb958279afd8&quot;\\n }, \\n &quot;origin&quot;: &quot;110.10.176.224&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\\n}\\n&apos; Cookieimport http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+\"=\"+item.value) BAIDUID=E77BF84491E332F6F8F1D451AD0063D3:FG=1 BIDUPSID=E77BF84491E332F6F8F1D451AD0063D3 H_PS_PSSID=1466_21127_22075 PSTM=1490198051 BDSVRTM=0 BD_HOME=0 import http.cookiejar, urllib.requestfilename = \"cookie.txt\"cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) import http.cookiejar, urllib.requestfilename = 'cookie.txt'cookie = http.cookiejar.LWPCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) import http.cookiejar, urllib.requestcookie = http.cookiejar.LWPCookieJar()cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 异常处理from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.URLError as e: print(e.reason) Not Found from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.HTTPError as e: print(e.reason, e.code, e.headers, sep='\\n')except error.URLError as e: print(e.reason)else: print('Request Successfully') Not Found 404 Server: nginx/1.10.1 Date: Wed, 22 Mar 2017 15:59:55 GMT Content-Type: text/html; charset=UTF-8 Transfer-Encoding: chunked Connection: close Vary: Cookie Expires: Wed, 11 Jan 1984 05:00:00 GMT Cache-Control: no-cache, must-revalidate, max-age=0 Link: &lt;http://cuiqingcai.com/wp-json/&gt;; rel=&quot;https://api.w.org/&quot; import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)except urllib.error.URLError as e: print(type(e.reason)) if isinstance(e.reason, socket.timeout): print('TIME OUT') &lt;class &apos;socket.timeout&apos;&gt; TIME OUT URL解析urlparseurllib.parse.urlparse(urlstring, scheme=’’, allow_fragments=True) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment')print(type(result), result) &lt;class &apos;urllib.parse.ParseResult&apos;&gt; ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) from urllib.parse import urlparseresult = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) ParseResult(scheme=&apos;https&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment', allow_fragments=False)print(result) ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5#comment&apos;, fragment=&apos;&apos;) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html#comment', allow_fragments=False)print(result) ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html#comment&apos;, params=&apos;&apos;, query=&apos;&apos;, fragment=&apos;&apos;) urlunparsefrom urllib.parse import urlunparsedata = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']print(urlunparse(data)) http://www.baidu.com/index.html;user?a=6#comment urljoinfrom urllib.parse import urljoinprint(urljoin('http://www.baidu.com', 'FAQ.html'))print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))print(urljoin('http://www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com#comment', '?category=2')) http://www.baidu.com/FAQ.html https://cuiqingcai.com/FAQ.html https://cuiqingcai.com/FAQ.html https://cuiqingcai.com/FAQ.html?question=2 https://cuiqingcai.com/index.php http://www.baidu.com?category=2#comment www.baidu.com?category=2#comment www.baidu.com?category=2 urlencodefrom urllib.parse import urlencodeparams = &#123; 'name': 'germey', 'age': 22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) http://www.baidu.com?name=germey&amp;age=22 request库使用requestsRequests是Python语言编写，基于urllib，采用Apache2 Licensed开源协议的HTTP库。它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。 实例引入import requestsresponse = requests.get('https://www.baidu.com/')print(type(response))print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 各种请求方式import requestsrequests.post('http://httpbin.org/post')requests.put('http://httpbin.org/put')requests.delete('http://httpbin.org/delete')requests.head('http://httpbin.org/get')requests.options('http://httpbin.org/get') 请求基本GET请求基本写法import requestsresponse = requests.get('http://httpbin.org/get')print(response.text) 带参数GET请求import requestsresponse = requests.get(\"http://httpbin.org/get?name=germey&amp;age=22\")print(response.text) import requestsdata = &#123; 'name': 'germey', 'age': 22&#125;response = requests.get(\"http://httpbin.org/get\", params=data)print(response.text) 解析jsonimport requestsimport jsonresponse = requests.get(\"http://httpbin.org/get\")print(type(response.text))print(response.json())print(json.loads(response.text))print(type(response.json())) 获取二进制数据import requestsresponse = requests.get(\"https://github.com/favicon.ico\")print(type(response.text), type(response.content))print(response.text)print(response.content) import requestsresponse = requests.get(\"https://github.com/favicon.ico\")with open('favicon.ico', 'wb') as f: f.write(response.content) f.close() 添加headersimport requestsresponse = requests.get(\"https://www.zhihu.com/explore\")print(response.text) import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;response = requests.get(\"https://www.zhihu.com/explore\", headers=headers)print(response.text) 基本POST请求import requestsdata = &#123;'name': 'germey', 'age': '22'&#125;response = requests.post(\"http://httpbin.org/post\", data=data)print(response.text) import requestsdata = &#123;'name': 'germey', 'age': '22'&#125;headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;response = requests.post(\"http://httpbin.org/post\", data=data, headers=headers)print(response.json()) 响应response属性import requestsresponse = requests.get('http://www.jianshu.com')print(type(response.status_code), response.status_code)print(type(response.headers), response.headers)print(type(response.cookies), response.cookies)print(type(response.url), response.url)print(type(response.history), response.history) 状态码判断import requestsresponse = requests.get('http://www.jianshu.com/hello.html')exit() if not response.status_code == requests.codes.not_found else print('404 Not Found') import requestsresponse = requests.get('http://www.jianshu.com')exit() if not response.status_code == 200 else print('Request Successfully') 100: ('continue',),101: ('switching_protocols',),102: ('processing',),103: ('checkpoint',),122: ('uri_too_long', 'request_uri_too_long'),200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\\\o/', '✓'),201: ('created',),202: ('accepted',),203: ('non_authoritative_info', 'non_authoritative_information'),204: ('no_content',),205: ('reset_content', 'reset'),206: ('partial_content', 'partial'),207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),208: ('already_reported',),226: ('im_used',),# Redirection.300: ('multiple_choices',),301: ('moved_permanently', 'moved', '\\\\o-'),302: ('found',),303: ('see_other', 'other'),304: ('not_modified',),305: ('use_proxy',),306: ('switch_proxy',),307: ('temporary_redirect', 'temporary_moved', 'temporary'),308: ('permanent_redirect', 'resume_incomplete', 'resume',), # These 2 to be removed in 3.0# Client Error.400: ('bad_request', 'bad'),401: ('unauthorized',),402: ('payment_required', 'payment'),403: ('forbidden',),404: ('not_found', '-o-'),405: ('method_not_allowed', 'not_allowed'),406: ('not_acceptable',),407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),408: ('request_timeout', 'timeout'),409: ('conflict',),410: ('gone',),411: ('length_required',),412: ('precondition_failed', 'precondition'),413: ('request_entity_too_large',),414: ('request_uri_too_large',),415: ('unsupported_media_type', 'unsupported_media', 'media_type'),416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),417: ('expectation_failed',),418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),421: ('misdirected_request',),422: ('unprocessable_entity', 'unprocessable'),423: ('locked',),424: ('failed_dependency', 'dependency'),425: ('unordered_collection', 'unordered'),426: ('upgrade_required', 'upgrade'),428: ('precondition_required', 'precondition'),429: ('too_many_requests', 'too_many'),431: ('header_fields_too_large', 'fields_too_large'),444: ('no_response', 'none'),449: ('retry_with', 'retry'),450: ('blocked_by_windows_parental_controls', 'parental_controls'),451: ('unavailable_for_legal_reasons', 'legal_reasons'),499: ('client_closed_request',),# Server Error.500: ('internal_server_error', 'server_error', '/o\\\\', '✗'),501: ('not_implemented',),502: ('bad_gateway',),503: ('service_unavailable', 'unavailable'),504: ('gateway_timeout',),505: ('http_version_not_supported', 'http_version'),506: ('variant_also_negotiates',),507: ('insufficient_storage',),509: ('bandwidth_limit_exceeded', 'bandwidth'),510: ('not_extended',),511: ('network_authentication_required', 'network_auth', 'network_authentication'), 高级操作文件上传import requestsfiles = &#123;'file': open('favicon.ico', 'rb')&#125;response = requests.post(\"http://httpbin.org/post\", files=files)print(response.text) 获取cookieimport requestsresponse = requests.get(\"https://www.baidu.com\")print(response.cookies)for key, value in response.cookies.items(): print(key + '=' + value) 会话维持模拟登录 import requestsrequests.get('http://httpbin.org/cookies/set/number/123456789')response = requests.get('http://httpbin.org/cookies')print(response.text) import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/number/123456789')response = s.get('http://httpbin.org/cookies')print(response.text) 证书验证import requestsresponse = requests.get('https://www.12306.cn')print(response.status_code) import requestsfrom requests.packages import urllib3urllib3.disable_warnings()response = requests.get('https://www.12306.cn', verify=False)print(response.status_code) import requestsresponse = requests.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))print(response.status_code) 代理设置import requestsproxies = &#123; \"http\": \"http://127.0.0.1:9743\", \"https\": \"https://127.0.0.1:9743\",&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) import requestsproxies = &#123; \"http\": \"http://user:password@127.0.0.1:9743/\",&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) pip3 install 'requests[socks]' import requestsproxies = &#123; 'http': 'socks5://127.0.0.1:9742', 'https': 'socks5://127.0.0.1:9742'&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) 超时设置import requestsfrom requests.exceptions import ReadTimeouttry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout') 认证设置import requestsfrom requests.auth import HTTPBasicAuthr = requests.get('http://120.27.34.24:9001', auth=HTTPBasicAuth('user', '123'))print(r.status_code) import requestsr = requests.get('http://120.27.34.24:9001', auth=('user', '123'))print(r.status_code) 异常处理import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout')except ConnectionError: print('Connection error')except RequestException: print('Error') Connection error selenium使用Selenium自动化测试工具，支持多种浏览器。爬虫中主要来解决JavaScript渲染问题。 基本使用from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com') input = browser.find_element_by_id('kw') input.send_keys('Python') input.send_keys(Keys.ENTER) wait = WebDriverWait(browser, 10) wait.until(EC.presence_of_element_located((By.ID, 'content_left'))) print(browser.current_url) print(browser.get_cookies()) print(browser.page_source)finally: browser.close() 声明浏览器对象from selenium import webdriverbrowser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.Edge()browser = webdriver.PhantomJS()browser = webdriver.Safari() 访问页面from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')print(browser.page_source)browser.close() 查找元素单个元素from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input_first = browser.find_element_by_id('q')input_second = browser.find_element_by_css_selector('#q')input_third = browser.find_element_by_xpath('//*[@id=\"q\"]')print(input_first, input_second, input_third)browser.close() &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input_first = browser.find_element(By.ID, 'q')print(input_first)browser.close() &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;1f209c0d11551c40d9d20ad964fef244&quot;, element=&quot;0.07914603542731591-1&quot;)&gt; 多个元素from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')lis = browser.find_elements_by_css_selector('.service-bd li')print(lis)browser.close() [&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-16&quot;)&gt;] from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get('https://www.taobao.com')lis = browser.find_elements(By.CSS_SELECTOR, '.service-bd li')print(lis)browser.close() [&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-16&quot;)&gt;] find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 元素交互操作对获取的元素调用交互方法 from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input = browser.find_element_by_id('q')input.send_keys('iPhone')time.sleep(1)input.clear()input.send_keys('iPad')button = browser.find_element_by_class_name('btn-search')button.click() 更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement 交互动作将动作附加到动作链中串行执行 from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform() 更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains 执行JavaScriptfrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert(\"To Bottom\")') 获取元素信息获取属性from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)logo = browser.find_element_by_id('zh-top-link-logo')print(logo)print(logo.get_attribute('class')) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;e08c0f28d7f44d75ccd50df6bb676104&quot;, element=&quot;0.7236390660048155-1&quot;)&gt; zu-top-link-logo 获取文本值from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.text) 提问 获取ID、位置、标签名、大小from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.id)print(input.location)print(input.tag_name)print(input.size) 0.6822924344980397-1 {&apos;y&apos;: 7, &apos;x&apos;: 774} button {&apos;height&apos;: 32, &apos;width&apos;: 66} Frameimport timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionbrowser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')print(source)try: logo = browser.find_element_by_class_name('logo')except NoSuchElementException: print('NO LOGO')browser.switch_to.parent_frame()logo = browser.find_element_by_class_name('logo')print(logo)print(logo.text) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.44746093888932004-1&quot;)&gt; NO LOGO &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.13792611320464965-2&quot;)&gt; RUNOOB.COM 等待隐式等待当使用了隐式等待执行测试的时候，如果 WebDriver没有在 DOM中找到元素，将继续等待，超出设定时间后则抛出找不到元素的异常, 换句话说，当查找元素或元素并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是0 from selenium import webdriverbrowser = webdriver.Chrome()browser.implicitly_wait(10)browser.get('https://www.zhihu.com/explore')input = browser.find_element_by_class_name('zu-top-add-question')print(input) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;b29214772d59e912f1ac52e96ed29abe&quot;, element=&quot;0.12886805191194894-1&quot;)&gt; 显式等待from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECbrowser = webdriver.Chrome()browser.get('https://www.taobao.com/')wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, 'q')))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))print(input, button) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-2&quot;)&gt; title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’) visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现Alert 详细内容：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions 前进后退import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com/')browser.get('https://www.taobao.com/')browser.get('https://www.python.org/')browser.back()time.sleep(1)browser.forward()browser.close() Cookiesfrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')print(browser.get_cookies())browser.add_cookie(&#123;'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'&#125;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies()) [{&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_cap_id&apos;, &apos;expiry&apos;: 1494196091.403418}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;YWEyOGY4MmI1MzQ2NGY5MmFiMjgzZGUzZWJjYTgwYjY=|1491604091|ff946847ddb5881245bdb7a5e6401b70dc61013f&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;cap_id&apos;, &apos;expiry&apos;: 1494196091.402855}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;MjcxMDE3YzU1YjI4NDljZjljNTQ4ZDIyOWJjZTBhNmY=|1491604091|8da4722b56a1545c2020dba97394a220c0eca8d9&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;r_cap_id&apos;, &apos;expiry&apos;: 1494196091.402525}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmc&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;AADCo7e1kguPTqvEOMieRUzwkA7ZUBhV-VY=|1491604091&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;d_c0&apos;, &apos;expiry&apos;: 1586212091.344773}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.1491604091.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmz&apos;, &apos;expiry&apos;: 1507372091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;3cc99fc5-8706-43fc-90ac-3ad991bd1a25&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_zap&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;97cb00128ccb46659728f7c69cc191b0|1491604091000|1491604091000&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;q_c1&apos;, &apos;expiry&apos;: 1586212091.401644}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.2.10.1491604091&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmb&apos;, &apos;expiry&apos;: 1491605891}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.000--|3=entry_date=20170408=1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmv&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.669300758.1491604091.1491604091.1491604091.1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utma&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmt&apos;, &apos;expiry&apos;: 1491604691}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;AQAAALCZuwh+dgIAeu3PPHA+csDPnXvT&apos;, &apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: True, &apos;name&apos;: &apos;aliyungf_tc&apos;}] [{&apos;secure&apos;: False, &apos;value&apos;: &apos;germey&apos;, &apos;domain&apos;: &apos;.www.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;name&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_cap_id&apos;, &apos;expiry&apos;: 1494196091.403418}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;YWEyOGY4MmI1MzQ2NGY5MmFiMjgzZGUzZWJjYTgwYjY=|1491604091|ff946847ddb5881245bdb7a5e6401b70dc61013f&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;cap_id&apos;, &apos;expiry&apos;: 1494196091.402855}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;l_n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;MjcxMDE3YzU1YjI4NDljZjljNTQ4ZDIyOWJjZTBhNmY=|1491604091|8da4722b56a1545c2020dba97394a220c0eca8d9&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;r_cap_id&apos;, &apos;expiry&apos;: 1494196091.402525}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmc&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;&quot;AADCo7e1kguPTqvEOMieRUzwkA7ZUBhV-VY=|1491604091&quot;&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;d_c0&apos;, &apos;expiry&apos;: 1586212091.344773}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.1491604091.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmz&apos;, &apos;expiry&apos;: 1507372091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;3cc99fc5-8706-43fc-90ac-3ad991bd1a25&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_zap&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;97cb00128ccb46659728f7c69cc191b0|1491604091000|1491604091000&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;q_c1&apos;, &apos;expiry&apos;: 1586212091.401644}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.2.10.1491604091&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmb&apos;, &apos;expiry&apos;: 1491605891}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;n_c&apos;}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.000--|3=entry_date=20170408=1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmv&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;51854390.669300758.1491604091.1491604091.1491604091.1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utma&apos;, &apos;expiry&apos;: 1554676091}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;, &apos;domain&apos;: &apos;.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;__utmt&apos;, &apos;expiry&apos;: 1491604691}, {&apos;secure&apos;: False, &apos;value&apos;: &apos;AQAAALCZuwh+dgIAeu3PPHA+csDPnXvT&apos;, &apos;domain&apos;: &apos;www.zhihu.com&apos;, &apos;path&apos;: &apos;/&apos;, &apos;httpOnly&apos;: True, &apos;name&apos;: &apos;aliyungf_tc&apos;}] [] 选项卡管理import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.execute_script('window.open()')print(browser.window_handles)browser.switch_to_window(browser.window_handles[1])browser.get('https://www.taobao.com')time.sleep(1)browser.switch_to_window(browser.window_handles[0])browser.get('https://python.org') [&apos;CDwindow-4f58e3a7-7167-4587-bedf-9cd8c867f435&apos;, &apos;CDwindow-6e05f076-6d77-453a-a36c-32baacc447df&apos;] 异常处理from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.find_element_by_id('hello') --------------------------------------------------------------------------- NoSuchElementException Traceback (most recent call last) &lt;ipython-input-23-978945848a1b&gt; in &lt;module&gt;() 3 browser = webdriver.Chrome() 4 browser.get(&apos;https://www.baidu.com&apos;) ----&gt; 5 browser.find_element_by_id(&apos;hello&apos;) /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in find_element_by_id(self, id_) 267 driver.find_element_by_id(&apos;foo&apos;) 268 &quot;&quot;&quot; --&gt; 269 return self.find_element(by=By.ID, value=id_) 270 271 def find_elements_by_id(self, id_): /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in find_element(self, by, value) 750 return self.execute(Command.FIND_ELEMENT, { 751 &apos;using&apos;: by, --&gt; 752 &apos;value&apos;: value})[&apos;value&apos;] 753 754 def find_elements(self, by=By.ID, value=None): /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in execute(self, driver_command, params) 234 response = self.command_executor.execute(driver_command, params) 235 if response: --&gt; 236 self.error_handler.check_response(response) 237 response[&apos;value&apos;] = self._unwrap_value( 238 response.get(&apos;value&apos;, None)) /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/errorhandler.py in check_response(self, response) 190 elif exception_class == UnexpectedAlertPresentException and &apos;alert&apos; in value: 191 raise exception_class(message, screen, stacktrace, value[&apos;alert&apos;].get(&apos;text&apos;)) --&gt; 192 raise exception_class(message, screen, stacktrace) 193 194 def _value_or_default(self, obj, key, default): NoSuchElementException: Message: no such element: Unable to locate element: {&quot;method&quot;:&quot;id&quot;,&quot;selector&quot;:&quot;hello&quot;} (Session info: chrome=57.0.2987.133) (Driver info: chromedriver=2.27.440174 (e97a722caafc2d3a8b807ee115bfb307f7d2cfd9),platform=Mac OS X 10.12.3 x86_64) from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: print('No Element')finally: browser.close() No Element 详细文档：http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions JsonJson 是一种轻量级的数据交换格式字符串是Json的表现形式符合Json格式的字符串叫Json字符串 Json相比与XML的优点 易于阅读 易于解析 网络传输效率高同时也可以跨语言交换数据 反序列化json.loads() # 将Json字符串格式转换成对应的Python相对应数据格式 序列化json.dumps() #将Python数据格式转换成Json相对应的格式 数据类型转换对应关系 Json Python object dict array kist string str number int number float true True false False null None 相关名词辨析 Json对象：是Javascript中的一中叫法，相对于其他语言其实是没有的。 Json：数据交换的标准格式。 Json字符串：符合Json格式的字符串。 正则表达式正则表达式正则表达式是对字符串操作的一中逻辑公示，就是用事先定义好的一些特定字符，以及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一中过滤逻辑。 常见匹配模式 模式 描述 \\w 匹配字母数字及下划线 \\W 匹配非字母数字下划线 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f]. \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0-9] \\D 匹配任意非数字 \\A 匹配字符串开始 \\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结束 \\G 匹配最后匹配完成的位置 \\n 匹配一个换行符 \\t 匹配一个制表符 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ [^…] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。 * 匹配0个或多个的表达式。 + 匹配1个或多个的表达式。 ? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 {n} 精确匹配n个前面表达式。 {n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a&#124;b 匹配a或b ( ) 匹配括号内的表达式，也表示一个组 re.matchre.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。re.match(pattern, string, flags=0) 最常规的匹配import recontent = 'Hello 123 4567 World_This is a Regex Demo'print(len(content))result = re.match('^Hello\\s\\d\\d\\d\\s\\d&#123;4&#125;\\s\\w&#123;10&#125;.*Demo$', content)print(result)print(result.group())print(result.span()) 41 &lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt; Hello 123 4567 World_This is a Regex Demo (0, 41) 泛匹配import recontent = 'Hello 123 4567 World_This is a Regex Demo'result = re.match('^Hello.*Demo$', content)print(result)print(result.group())print(result.span()) &lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt; Hello 123 4567 World_This is a Regex Demo (0, 41) 匹配目标import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^Hello\\s(\\d+)\\sWorld.*Demo$', content)print(result)print(result.group(1))print(result.span()) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 1234567 (0, 40) 贪婪匹配import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^He.*(\\d+).*Demo$', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 7 非贪婪匹配import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^He.*?(\\d+).*Demo$', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 1234567 匹配模式import recontent = '''Hello 1234567 World_Thisis a Regex Demo'''result = re.match('^He.*?(\\d+).*?Demo$', content, re.S)print(result.group(1)) 1234567 转义import recontent = 'price is $5.00'result = re.match('price is $5.00', content)print(result) None import recontent = 'price is $5.00'result = re.match('price is \\$5\\.00', content)print(result) &lt;_sre.SRE_Match object; span=(0, 14), match=&apos;price is $5.00&apos;&gt; 总结：尽量使用泛匹配、使用括号得到匹配目标、尽量使用非贪婪模式、有换行符就用re.S re.searchre.search 扫描整个字符串并返回第一个成功的匹配。 import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'result = re.match('Hello.*?(\\d+).*?Demo', content)print(result) None import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'result = re.search('Hello.*?(\\d+).*?Demo', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(13, 53), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt; 1234567 总结：为匹配方便，能用search就不用match 匹配演练import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;&lt;i class=\"fa fa-user\"&gt;&lt;/i&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?active.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)if result: print(result.group(1), result.group(2)) 齐秦 往事随风 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)if result: print(result.group(1), result.group(2)) 任贤齐 沧海一声笑 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html)if result: print(result.group(1), result.group(2)) beyond 光辉岁月 re.findall搜索字符串，以列表形式返回全部能匹配的子串。 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''results = re.findall('&lt;li.*?href=\"(.*?)\".*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)print(results)print(type(results))for result in results: print(result) print(result[0], result[1], result[2]) [(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;), (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;), (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;), (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;), (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)] &lt;class &apos;list&apos;&gt; (&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;) /2.mp3 任贤齐 沧海一声笑 (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;) /3.mp3 齐秦 往事随风 (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;) /4.mp3 beyond 光辉岁月 (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;) /5.mp3 陈慧琳 记事本 (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;) /6.mp3 邓丽君 但愿人长久 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''results = re.findall('&lt;li.*?&gt;\\s*?(&lt;a.*?&gt;)?(\\w+)(&lt;/a&gt;)?\\s*?&lt;/li&gt;', html, re.S)print(results)for result in results: print(result[1]) [(&apos;&apos;, &apos;一路上有你&apos;, &apos;&apos;), (&apos;&lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;&apos;, &apos;沧海一声笑&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;&apos;, &apos;往事随风&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;&apos;, &apos;光辉岁月&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;&apos;, &apos;记事本&apos;, &apos;&lt;/a&gt;&apos;), (&apos;&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&apos;, &apos;但愿人长久&apos;, &apos;&lt;/a&gt;&apos;)] 一路上有你 沧海一声笑 往事随风 光辉岁月 记事本 但愿人长久 re.sub替换字符串中每一个匹配的子串后返回替换后的字符串。注意：sub函数第二个参数可以传入一个函数。 import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('\\d+', '', content)print(content) Extra stings Hello World_This is a Regex Demo Extra stings import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('\\d+', 'Replacement', content)print(content) Extra stings Hello Replacement World_This is a Regex Demo Extra stings import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('(\\d+)', r'\\1 8910', content)print(content) Extra stings Hello 1234567 8910 World_This is a Regex Demo Extra stings import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;''' import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''html = re.sub('&lt;a.*?&gt;|&lt;/a&gt;', '', html)print(html)results = re.findall('&lt;li.*?&gt;(.*?)&lt;/li&gt;', html, re.S)print(results)for result in results: print(result.strip()) &lt;div id=&quot;songs-list&quot;&gt; &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; &lt;p class=&quot;introduction&quot;&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; &lt;li data-view=&quot;7&quot;&gt; 沧海一声笑 &lt;/li&gt; &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; 往事随风 &lt;/li&gt; &lt;li data-view=&quot;6&quot;&gt;光辉岁月&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt;记事本&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt; 但愿人长久 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; [&apos;一路上有你&apos;, &apos;\\n 沧海一声笑\\n &apos;, &apos;\\n 往事随风\\n &apos;, &apos;光辉岁月&apos;, &apos;记事本&apos;, &apos;\\n 但愿人长久\\n &apos;] 一路上有你 沧海一声笑 往事随风 光辉岁月 记事本 但愿人长久 re.compile将正则字符串编译成正则表达式对象 将一个正则表达式串编译成正则对象，以便于复用该匹配模式 import recontent = '''Hello 1234567 World_Thisis a Regex Demo'''pattern = re.compile('Hello.*Demo', re.S)result = re.match(pattern, content)#result = re.match('Hello.*Demo', content, re.S)print(result) &lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This\\nis a Regex Demo&apos;&gt; 实战练习import requestsimport recontent = requests.get('https://book.douban.com/').textpattern = re.compile('&lt;li.*?cover.*?href=\"(.*?)\".*?title=\"(.*?)\".*?more-meta.*?author\"&gt;(.*?)&lt;/span&gt;.*?year\"&gt;(.*?)&lt;/span&gt;.*?&lt;/li&gt;', re.S)results = re.findall(pattern, content)for result in results: url, name, author, date = result author = re.sub('\\s', '', author) date = re.sub('\\s', '', date) print(url, name, author, date) https://book.douban.com/subject/26925834/?icn=index-editionrecommend 别走出这一步 [英]S.J.沃森 2017-1 https://book.douban.com/subject/26953532/?icn=index-editionrecommend 白先勇细说红楼梦 白先勇 2017-2-1 https://book.douban.com/subject/26959159/?icn=index-editionrecommend 岁月凶猛 冯仑 2017-2 https://book.douban.com/subject/26949210/?icn=index-editionrecommend 如果没有今天，明天会不会有昨天？ [瑞士]伊夫·博萨尔特（YvesBossart） 2017-1 https://book.douban.com/subject/27001447/?icn=index-editionrecommend 人类这100年 阿夏 2017-2 https://book.douban.com/subject/26864566/?icn=index-latestbook-subject 眼泪的化学 [澳]彼得·凯里 2017-2 https://book.douban.com/subject/26991064/?icn=index-latestbook-subject 青年斯大林 [英]西蒙·蒙蒂菲奥里 2017-3 https://book.douban.com/subject/26938056/?icn=index-latestbook-subject 带艾伯特回家 [美]霍默·希卡姆 2017-3 https://book.douban.com/subject/26954757/?icn=index-latestbook-subject 乳房 [美]弗洛伦斯·威廉姆斯 2017-2 https://book.douban.com/subject/26956479/?icn=index-latestbook-subject 草原动物园 马伯庸 2017-3 https://book.douban.com/subject/26956018/?icn=index-latestbook-subject 贩卖音乐 [美]大卫·伊斯曼 2017-3-1 https://book.douban.com/subject/26703649/?icn=index-latestbook-subject 被占的宅子 [阿根廷]胡利奥·科塔萨尔 2017-3 https://book.douban.com/subject/26578402/?icn=index-latestbook-subject 信仰与观看 [法]罗兰·雷希特(RolandRecht) 2017-2-17 https://book.douban.com/subject/26939171/?icn=index-latestbook-subject 妹妹的坟墓 [美]罗伯特·杜格尼(RobertDugoni) 2017-3-1 https://book.douban.com/subject/26972465/?icn=index-latestbook-subject 全栈市场人 Lydia 2017-2-1 https://book.douban.com/subject/26986928/?icn=index-latestbook-subject 终极X战警2 [英]马克·米勒&amp;nbsp;/&amp;nbsp;[美]亚当·库伯特 2017-3-15 https://book.douban.com/subject/26948144/?icn=index-latestbook-subject 格调（修订第3版） [美]保罗·福塞尔（PaulFussell） 2017-2 https://book.douban.com/subject/26945792/?icn=index-latestbook-subject 原谅石 [美]洛里·斯皮尔曼 2017-2 https://book.douban.com/subject/26974207/?icn=index-latestbook-subject 庇护二世闻见录 [意]皮科洛米尼 2017-2 https://book.douban.com/subject/26983143/?icn=index-latestbook-subject 遇见野兔的那一年 [芬]阿托·帕西林纳 2017-3-1 https://book.douban.com/subject/26976429/?icn=index-latestbook-subject 鲍勃·迪伦：诗人之歌 [法]让-多米尼克·布里埃 2017-4 https://book.douban.com/subject/26962860/?icn=index-latestbook-subject 牙医谋杀案 [英]阿加莎·克里斯蒂 2017-3 https://book.douban.com/subject/26923022/?icn=index-latestbook-subject 石挥谈艺录：把生命交给舞台 石挥 2017-2 https://book.douban.com/subject/26897190/?icn=index-latestbook-subject 理想 [美]安·兰德 2017-2 https://book.douban.com/subject/26985981/?icn=index-latestbook-subject 青苔不会消失 袁凌 2017-4 https://book.douban.com/subject/26984949/?icn=index-latestbook-subject 地下铁道 [美]科尔森·怀特黑德（ColsonWhitehead） 2017-3 https://book.douban.com/subject/26944012/?icn=index-latestbook-subject 极简进步史 [英]罗纳德·赖特 2017-4-1 https://book.douban.com/subject/26969002/?icn=index-latestbook-subject 驻马店伤心故事集 郑在欢 2017-2 https://book.douban.com/subject/26854223/?icn=index-latestbook-subject 致薇拉 [美]弗拉基米尔·纳博科夫 2017-3 https://book.douban.com/subject/26841616/?icn=index-latestbook-subject 北方档案 [法]玛格丽特·尤瑟纳尔 2017-2 https://book.douban.com/subject/26980391/?icn=index-latestbook-subject 食帖15：便当灵感集 林江 2017-2 https://book.douban.com/subject/26958882/?icn=index-latestbook-subject 生火 [法]克里斯多夫·夏布特（ChristopheChabouté）编绘 2017-3 https://book.douban.com/subject/26989163/?icn=index-latestbook-subject 文明之光（第四册） 吴军 2017-3-1 https://book.douban.com/subject/26878906/?icn=index-latestbook-subject 公牛山 [美]布赖恩·帕诺威奇 2017-2 https://book.douban.com/subject/26989534/?icn=index-latestbook-subject 几乎消失的偷闲艺术 [加拿大]达尼·拉费里埃 2017-4 https://book.douban.com/subject/26939973/?icn=index-latestbook-subject 散步去 [日]谷口治郎 2017-3 https://book.douban.com/subject/26865333/?icn=index-latestbook-subject 中国1945 [美]理查德·伯恩斯坦(RichardBernstein) 2017-3-1 https://book.douban.com/subject/26989242/?icn=index-latestbook-subject 有匪2：离恨楼 Priest 2017-3 https://book.douban.com/subject/26985790/?icn=index-latestbook-subject 女人、火与危险事物 [美]乔治·莱考夫 2017-3 https://book.douban.com/subject/26972277/?icn=index-latestbook-subject 寻找时间的人 [爱尔兰]凯特·汤普森 2017-3 https://www.douban.com/note/610758170/ 白先勇细说红楼梦【全二册】 白先勇 2017-2-1 https://read.douban.com/ebook/31540864/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 奇爱博士 [英]彼得·乔治 2016-8-1 https://read.douban.com/ebook/31433872/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 在时光中盛开的女子 李筱懿 2017-3 https://read.douban.com/ebook/31178635/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 如何高效记忆（原书第2版） [美]肯尼思•希格比（KennethL.Higbee） 2017-3-5 https://read.douban.com/ebook/31358183/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 愿无岁月可回头 回忆专用小马甲 2016-9 https://read.douban.com/ebook/31341636/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 走神的艺术与科学 [新西兰]迈克尔·C.科尔巴里斯 2017-3-1 https://read.douban.com/ebook/27621094/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 神秘的量子生命 [英]吉姆•艾尔－哈利利/约翰乔•麦克法登 2016-8 https://read.douban.com/ebook/31221966/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 寻找时间的人 [爱尔兰]凯特·汤普森 2017-3 https://read.douban.com/ebook/31481323/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 山之四季 [日]高村光太郎 2017-1 https://read.douban.com/ebook/31154855/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 东北游记 [美]迈克尔·麦尔 2017-1 BeautifulSoup 解析库BeautifulSoup灵活又方便的网页解析库，处理高效，支持多种解析器，利用它不用编写正则表达式即可方便的实现网页信息的提取。 解析库 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, “html.parser”) Python的内置标准库、执行速度适中 、文档容错能力强 Python 2.7.3 or 3.2.2)前的版本中文容错能力差 lxml HTML 解析器 BeautifulSoup(markup, “lxml”) 速度快、文档容错能力强 需要安装C语言库 lxml XML 解析器 BeautifulSoup(markup, “xml”) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, “html5lib”) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢、不依赖外部扩展 基本使用html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string) &lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse&apos;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt; &lt;b&gt; The Dormouse&apos;s story &lt;/b&gt; &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;!-- Elsie --&gt; &lt;/a&gt; , &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt; Lacie &lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt; Tillie &lt;/a&gt; ; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; ... &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; The Dormouse&apos;s story 标签选择器选择元素html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.head)print(soup.p) &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;class &apos;bs4.element.Tag&apos;&gt; &lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt; 获取名称html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title.name) title 获取属性html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.attrs['name'])print(soup.p['name']) dromouse dromouse 获取内容html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p clss=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.string) The Dormouse&apos;s story 嵌套选择html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.head.title.string) The Dormouse&apos;s story 子节点和子孙节点html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.contents) [&apos;\\n Once upon a time there were three little sisters; and their names were\\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt;, &apos;\\n&apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &apos; \\n and\\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;, &apos;\\n and they lived at the bottom of a well.\\n &apos;] html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.children)for i, child in enumerate(soup.p.children): print(i, child) &lt;list_iterator object at 0x1064f7dd8&gt; 0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; 2 3 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; 4 and 5 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; 6 and they lived at the bottom of a well. html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.descendants)for i, child in enumerate(soup.p.descendants): print(i, child) &lt;generator object descendants at 0x10650e678&gt; 0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; 2 3 &lt;span&gt;Elsie&lt;/span&gt; 4 Elsie 5 6 7 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; 8 Lacie 9 and 10 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; 11 Tillie 12 and they lived at the bottom of a well. 父节点和祖先节点html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.a.parent) &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(list(enumerate(soup.a.parents))) [(0, &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;), (1, &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;), (2, &lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;), (3, &lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;)] 兄弟节点html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(list(enumerate(soup.a.next_siblings)))print(list(enumerate(soup.a.previous_siblings))) [(0, &apos;\\n&apos;), (1, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;), (2, &apos; \\n and\\n &apos;), (3, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;), (4, &apos;\\n and they lived at the bottom of a well.\\n &apos;)] [(0, &apos;\\n Once upon a time there were three little sisters; and their names were\\n &apos;)] 标准选择器find_all( name , attrs , recursive , text , **kwargs )可根据标签名、属性、内容查找文档 namehtml='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all('ul'))print(type(soup.find_all('ul')[0])) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;, &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt;] &lt;class &apos;bs4.element.Tag&apos;&gt; html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.find_all('ul'): print(ul.find_all('li')) [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] attrshtml='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\" name=\"elements\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(attrs=&#123;'id': 'list-1'&#125;))print(soup.find_all(attrs=&#123;'name': 'elements'&#125;)) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(id='list-1'))print(soup.find_all(class_='element')) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] texthtml='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(text='Foo')) [&apos;Foo&apos;, &apos;Foo&apos;] find( name , attrs , recursive , text , **kwargs )find返回单个元素，find_all返回所有元素 html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find('ul'))print(type(soup.find('ul')))print(soup.find('page')) &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;class &apos;bs4.element.Tag&apos;&gt; None find_parents() find_parent()find_parents()返回所有祖先节点，find_parent()返回直接父节点。 find_next_siblings() find_next_sibling()find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。 find_previous_siblings() find_previous_sibling()find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。 find_all_next() find_next()find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点 find_all_previous() 和 find_previous()find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点 CSS选择器通过select()直接传入CSS选择器即可完成选择CSS选择器语法： html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.select('.panel .panel-heading'))print(soup.select('ul li'))print(soup.select('#list-2 .element'))print(type(soup.select('ul')[0])) [&lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] &lt;class &apos;bs4.element.Tag&apos;&gt; html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul.select('li')) [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] 获取属性html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul['id']) print(ul.attrs['id']) list-1 list-1 list-2 list-2 获取内容html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for li in soup.select('li'): print(li.get_text()) Foo Bar Jay Foo Bar 总结 推荐使用lxml解析库，必要时使用html.parser 标签选择筛选功能弱但是速度快 建议使用find()、find_all() 查询匹配单个结果或者多个结果 如果对CSS选择器熟悉建议使用select() 记住常用的获取属性和文本值的方法 pyquerypyquery强大又灵活的网页解析库。如果你觉得正则写起来太麻烦，如果你觉得BeautifulSoup语法太难记，如果你熟悉jQuery的语法，那么PyQuery就是你的绝佳选择。主要是使用CSS选择器进行解析。比BeautifulSoup中的select()方法更加强大。 初始化 字符串初始化，直接传入html即可 URL初始化，将URL以参数url传入即可。 文件初始化，将demo.html传入filename即可。 字符串初始化html = '''&lt;div&gt; &lt;ul&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; URL初始化from pyquery import PyQuery as pqdoc = pq(url='http://www.baidu.com')print(doc('head')) &lt;head&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&quot;/&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; 文件初始化from pyquery import PyQuery as pqdoc = pq(filename='demo.html')print(doc('li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 基本CSS选择器 find() 返回符合条件的所有节点。 children() 返回所有子节点。 parent() 返回某个节点的父节点 parents() 返回某个节点的祖先节点。 siblings() 返回某个节点的兄弟节点。 获取到单个节点，可以直接打印或者转换成字符串。获取到多个节点，需要调用items()方法，将结果变成生成器，然后对其遍历，去除所有结果。 attr() 获取属性。 text() 获取文本。 html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('#container .list li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 查找元素子元素html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; lis = items.children()print(type(lis))print(lis) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; lis = items.children('.active')print(lis) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 父元素html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')parents = items.parents()print(type(parents))print(parents) &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; parent = items.parents('.wrap')print(parent) &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 兄弟元素html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings()) &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings('.active')) &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 遍历单个元素html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li) &lt;class &apos;generator&apos;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 获取信息获取属性html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.attr('href'))print(a.attr.href) &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; link3.html link3.html 获取文本html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text()) &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; third item 获取HTMLhtml = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(li.html()) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; DOM操作addClass、removeClasshtml = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.removeClass('active')print(li)li.addClass('active')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; attr、csshtml = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.attr('name', 'link')print(li)li.css('font-size', '14px')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot; name=&quot;link&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot; name=&quot;link&quot; style=&quot;font-size: 14px&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; removehtml = '''&lt;div class=\"wrap\"&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)wrap = doc('.wrap')print(wrap.text())wrap.find('p').remove()print(wrap.text()) Hello, World This is a paragraph. Hello, World 其他DOM方法http://pyquery.readthedocs.io/en/latest/api.html 伪类选择器html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('li:first-child')print(li)li = doc('li:last-child')print(li)li = doc('li:nth-child(2)')print(li)li = doc('li:gt(2)')print(li)li = doc('li:nth-child(2n)')print(li)li = doc('li:contains(second)')print(li) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; 更多CSS选择器可以查看http://www.w3school.com.cn/css/index.asp 官方文档http://pyquery.readthedocs.io/ 数据库Python操作三大主流数据库数据库分类 关系型数据库e.g: MariaDB, SQLite, SQLServer, MySQL, PostgreSQL, ORACLE 非关系型数据库e.g: mongoDB, HBASE, redis, CouchDB, Cassandre, Neo4j分类： 文档型 key-value型 列式数据库 图形数据库 MySQL 数据库","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://lucas0625.github.io/blog/categories/爬虫/"}],"tags":[{"name":"基础知识","slug":"基础知识","permalink":"http://lucas0625.github.io/blog/tags/基础知识/"}]},{"title":"常见排序算法","slug":"常见排序算法","date":"2018-12-10T16:00:00.000Z","updated":"2018-12-11T06:08:52.309Z","comments":true,"path":"2018/12/11/常见排序算法/","link":"","permalink":"http://lucas0625.github.io/blog/2018/12/11/常见排序算法/","excerpt":"本文介绍常见的排序算法。包括冒泡排序，插入排序，选择排序。","text":"本文介绍常见的排序算法。包括冒泡排序，插入排序，选择排序。 冒泡排序from typing import List# 冒泡排序 稳定排序算法def bubble_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(len(a)): made_swap = False for j in range(len(a) - i - 1): if a[j] &gt; a[j + 1]: a[j], a[j + 1] = a[j + 1], a[j] made_swap = True if not made_swap: break 注： 空间复杂度O(1)，原地排序算法。 稳定排序算法。 最优时间复杂度O(n), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 插入排序from typing import List# 插入排序 稳定排序算法def insertion_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(1, len(a)): value = a[i] j = i - 1 while j &gt;= 0 and a[j] &gt; value: a[j + 1] = a[j] j -= 1 a[j + 1] = value 注： 空间复杂度O(1)，原地排序算法。 稳定排序算法。 最优时间复杂度O(n), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 选择排序from typing import List# 选择排序 非稳定排序算法def selection_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(len(a)): min_index = i min_val = a[i] for j in range(i, len(a)): if a[j] &lt; min_val: min_val = a[j] min_index = j a[i], a[min_index] = a[min_index], a[i] 注： 空间复杂度O(1)，原地排序算法。 非稳定排序算法。 最优时间复杂度O(n^2), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2)","categories":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Python 编码规范","slug":"Python 编码规范","date":"2018-10-08T16:00:00.000Z","updated":"2018-10-09T13:42:43.892Z","comments":true,"path":"2018/10/09/Python 编码规范/","link":"","permalink":"http://lucas0625.github.io/blog/2018/10/09/Python 编码规范/","excerpt":"遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。在团队开发中，使用（尽量）统一的编码风格，还可以降低沟通成本。","text":"遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。在团队开发中，使用（尽量）统一的编码风格，还可以降低沟通成本。 缩紧 不要使用 tab 缩进 使用任何编辑器写 Python，请把一个 tab 展开为 4 个空格 绝对不要混用 tab 和空格，否则容易出现 IndentationError 空格 在 list, dict, tuple, set, 参数列表的 , 后面加一个空格 在 dict 的 : 后面加一个空格 在注释符号 # 后面加一个空格，但是 #!/usr/bin/python 的 # 后不能有空格 操作符两端加一个空格，如 +, -, *, /, |, &amp;, = 接上一条，在参数列表里的 =两端不需要空格 括号((), {}, [])内的两端不需要空格 空行 function 和 class 顶上两个空行 class 的 method 之间一个空行 函数内逻辑无关的段落之间空一行，不要过度使用空行 不要把多个语句写在一行，然后用 ; 隔开 if/for/while 语句中，即使执行语句只有一句，也要另起一行 换行 每一行代码控制在80字符以内 使用\\或()控制换行，举例： def foo(first, second, third, fourth, fifth, sixth, and_some_other_very_long_param): user = User.objects.filter_by(first=first, second=second, third=third) \\ .skip(100).limit(100) \\ .all()text = ('Long strings can be made up ' 'of several shorter strings.') 命名 使用有意义的，英文单词或词组，绝对不要使用汉语拼音 package/module 名中不要出现 - 各种类型的命名规范： import 所有 import 尽量放在文件开头，在 docstring 下面，其他变量定义的上面 不要使用 from foo imort * import 需要分组，每组之间一个空行，每个分组内的顺序尽量采用字典序，分组顺序是： 1.标准库 2.第三方库 3.本项目的 package 和 module 不要使用隐式的相对导入（implicit relative imports），可是使用显示的相对导入（explicit relative imports），如 from ..utils import validator，最好使用全路径导入（absolute imports） 对于不同的 package，一个 import 单独一行，同一个 package/module 下的内容可以写一起 为了避免可能出现的命名冲突，可以使用 as 或导入上一级命名空间 不要出现循环导入(cyclic import) 注释 文档字符串 docstring, 是 package, module, class, method, function 级别的注释，可以通过 __doc__ 成员访问到，注释内容在一对 &quot;&quot;&quot; 符号之间 function, method 的文档字符串应当描述其功能、输入参数、返回值，如果有复杂的算法和实现，也需要写清楚 不要写错误的注释，不要无谓的注释 优先使用英文写注释，英文不好全部写中文，否则更加看不懂 异常 不要轻易使用 try/except except 后面需要指定捕捉的异常，裸露的 except 会捕捉所有异常，意味着会隐藏潜在的问题 可以有多个 except 语句，捕捉多种异常，分别做异常处理 使用 finally 子句来处理一些收尾操作 try/except 里的内容不要太多，只在可能抛出异常的地方使用 从 Exception 而不是 BaseException 继承自定义的异常类 Class(类) 显示的写明父类，如果不是继承自别的类，就继承自 object 类 使用 super 调用父类的方法 支持多继承，即同时有多个父类，建议使用 Mixin 编码建议字符串 使用字符串的 join 方法拼接字符串 使用字符串类型的方法，而不是 string 模块的方法 使用 startswith 和 endswith 方法比较前缀和后缀 使用 format 方法格式化字符串 比较 空的 list, str, tuple, set, dict 和 0, 0.0, None 都是 False 使用 if some_list 而不是 if len(some_list) 判断某个 list 是否为空，其他类型同理 使用 is 和 is not 与单例（如 None）进行比较，而不是用 == 和 != 使用 if a is not None 而不是 if not a is None 用 isinstance 而不是 type 判断类型 不要用 == 和 != 与 True 和 False 比较（除非有特殊情况，如在 sqlalchemy 中可能用到） 使用 in 操作: 用 key in dict 而不是 dict.has_key() 用 set 加速 “存在性” 检查，list 的查找是线性的，复杂度 O(n)，set 底层是 hash table, 复杂度 O(1)，但用 set需要比 list 更多内存空间 其他 使用列表表达式（list comprehension），字典表达式(dict comprehension, Python 2.7+) 和生成器(generator) dict 的 get 方法可以指定默认值，但有些时候应该用 [] 操作，使得可以抛出 KeyError 使用 for item in list 迭代 list, for index, item in enumerate(list) 迭代 list 并获取下标 使用内建函数 sorted 和 list.sort 进行排序 适量使用map, reduce, filter 和 lambda，使用内建的 all, any 处理多个条件的判断 使用 defaultdict (Python 2.5+), Counter(Python 2.7+) 等 “冷门” 但好用的标准库算法和数据结构 使用装饰器(decorator) 使用 with 语句处理上下文 有些时候不要对类型做太过严格的限制，利用 Python 的鸭子类型（Duck Type）特性 使用 logging 记录日志，配置好格式和级别 了解 Python 的 Magic Method：A Guide to Python’s Magic Methods, Python 魔术方法指南 阅读优秀的开源代码，如 Flask 框架, Requests for Humans 不要重复造轮子，查看标准库、PyPi、Github、Google 等使用现有的优秀的解决方案","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/blog/categories/编程/"}],"tags":[{"name":"经验","slug":"经验","permalink":"http://lucas0625.github.io/blog/tags/经验/"}]},{"title":"深入理解 Python package","slug":"深入理解Python package","date":"2018-10-07T16:00:00.000Z","updated":"2018-10-09T13:42:17.241Z","comments":true,"path":"2018/10/08/深入理解Python package/","link":"","permalink":"http://lucas0625.github.io/blog/2018/10/08/深入理解Python package/","excerpt":"Python 是通过 module 组织代码的，module 即一个 py 文件，module 又是通过 package 来组织的，package 是一个包含 __init__.py 的文件夹，代码，module，package 它们三者的关系就是：module 包含代码，package 至少包含一个为 __init__.py 的 module。 package├── __init__.py├── submodule.py└── subpackage └── __init__.py","text":"Python 是通过 module 组织代码的，module 即一个 py 文件，module 又是通过 package 来组织的，package 是一个包含 __init__.py 的文件夹，代码，module，package 它们三者的关系就是：module 包含代码，package 至少包含一个为 __init__.py 的 module。 package├── __init__.py├── submodule.py└── subpackage └── __init__.py 空的 __init__.py不包含任何代码的 __init__.py 只用来标识一个文件夹是一个 package，而 package 是可以被导出的。 from package import item 此处的 item 可以是 package 中包含的 submodule 或 subpackage。 from package import submodulefrom package import subpackage 不为空 __init__.py如果 __init__.py 不为空，其中包含的任何变量，包括 function、class、variable 以及 任何被导入的 module 都可以通过 package 导出。 from package import item 此处的 item 可以是 __init__.py中的任何变量 package的初始化工作一个 package 被导入，不管在什么时候 __init__.py 中的代码只执行一次。 &gt;&gt;&gt; import packagehello world&gt;&gt;&gt; import package&gt;&gt;&gt; import package&gt;&gt;&gt; 由于 package 被导入时__init__.py 中的可执行代码会被执行，所以小心在 package 中放置你的代码，尽可能消除它们产生的副作用，比如把代码尽可能的进行封装成函数或类。 从package中倒入变量的顺序from package import item import 语句首先检查 item 是否是 __init__.py 中定义的变量，然后检查其是不是一个 subpackage，如果不是再去检查其是不是一个 module，都不是将抛出 ImportError。 在 import item.subitem.subsubitem 语句时，除了最后一个 subsubitem 之外其他 item 都必须是 package，而最后一个 subsubitem 必须是一个 package 或者 module，不能是他前一个 item 定义的 function、class、variable。 使用*导入在 from package import *语句中，如果 __init__.py 中定义了 __all__ 变量，一个 list，仅仅只有这个 list 中定义的 submodule 或者变量将会被导出。 如果__init__.py中没有__all__变量，导出将按照一下规则执行： 此 package 被导入，并且执行 __init__.py 中可被执行的代码 __init__.py 中定义的 variable 被导入 __init__.py 中被显式导入的 module 被导入","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/blog/categories/编程/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"python 常用技巧","slug":"json","date":"2018-09-16T08:30:18.409Z","updated":"2019-05-29T07:45:24.159Z","comments":true,"path":"2018/09/16/json/","link":"","permalink":"http://lucas0625.github.io/blog/2018/09/16/json/","excerpt":"记录自己常用的一些代码技巧","text":"记录自己常用的一些代码技巧 格式化显示字符串import jsonjson.dumps(dict, indent=4) and, or 的特殊用法 a and b and c 返回第一个False 对象, 反之返回 c a or b or c 返回第一个True 对象, 反之返回 c 删除特殊字符str.replace(u'\\u200b', '') 将列表每个元素作为参数传入函数a = [1, 2, 3, 4]# 将列表a作为整体传入函数funcdef func(a): ...# 将列表每一个元素作为参数传入函数funcdef func(*a): ... 以字符分割字符串，长度不超过n的数字作为一个字符import redef splice_string(string: str, nums: int = 3) -&gt; str: \"\"\" 按字符分割字符串，不超过 nums 个的数字不切分 :param string: :return: \"\"\" pattern = re.compile('((?&lt;!\\d)(\\d&#123;1,%d&#125;)(?!(\\d+))|(\\S))' % (nums)) result = re.sub(pattern, r'\\1\\t', string) return ' '.join(result.strip().split())res = splice_string('人一天要睡20-24小时，一天24小时有1小时可以娱乐，一年365天这样做，30000块钱就到手了')print(res) &apos;人 一 天 要 睡 20 - 24 小 时 ， 一 天 24 小 时 有 1 小 时 可 以 娱 乐 ， 一 年 365 天 这 样 做 ， 3 0 0 0 0 块 钱 就 到 手 了&apos; 判断奇偶# 方法一if nums % 2 == 1: # 奇数else: # 偶数# 方法二if nums &amp; 1 == 1: # 奇数else: # 偶数 不使用第三个变量交换两个数# 方法一# n ^ n = 0, n ^ 0 = nx = x ^ yy = x ^ yx = x ^ y# 方法二# 采用四则运算a = a + bb = a - ba = a - b","categories":[{"name":"编程","slug":"编程","permalink":"http://lucas0625.github.io/blog/categories/编程/"}],"tags":[{"name":"技巧","slug":"技巧","permalink":"http://lucas0625.github.io/blog/tags/技巧/"}]},{"title":"算法和数据结构","slug":"Algorithms and Data Structures","date":"2018-09-11T13:17:13.800Z","updated":"2019-05-15T06:24:14.423Z","comments":true,"path":"2018/09/11/Algorithms and Data Structures/","link":"","permalink":"http://lucas0625.github.io/blog/2018/09/11/Algorithms and Data Structures/","excerpt":"数据结构数据结构就是关系，数据元素相互之间存在的一种或多种特定关系的集合","text":"数据结构数据结构就是关系，数据元素相互之间存在的一种或多种特定关系的集合 逻辑结构和物理结构逻辑结构： 指数据对象中数据元素之间的相互关系，也是今后最需要关注和讨论的问题 逻辑结构指数据的逻辑结构在计算机中的存储形式 四大逻辑结构： 集合结构： 同属一个集合 线性结构： 一对一 树形结构： 一对多 图形结构： 多对多 物理结构 存储器： 主要针对内存而言，想赢盘，光盘 数据存储结构形式： 顺序存储，链式存储 顺序存储： 数据元素存放在地址连续的存储单元里，其数据的逻辑关系和物理关系是一致的链式存储： 把数据元素放在任意的存储单元里，这组存储单元可以是连续的，也可以是不连续的 算法算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作 算法的五个基本特征： 输入：零个或多个输入 输出： 至少有一个或多个输出 有穷行： 算法在执行有限的步骤之后，自动结束而不会出现无限循环 确定性： 每一个步骤都具有确定的含义 可行性： 每一步都必须是可行的 算法设计的要求 正确性 可读性 健壮性 时间效率高和存储量低 算法效率的度量方法运行时间主要取决因素 算法采用的策略，方案 编译产生的代码质量 问题的输入规模 机器执行指令的速度 分析算法的运行时间时，重要的是把基本操作的数量和输入模式关联起来可以忽略的项 常数可以忽略 与最高次项相乘的常数可以忽略 其他次项（除去最高项）时间复杂度定义：在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随n的变化情况并确定T(n)的数量级。算法的时间复杂度，也就是算法的时间度量，记作：T(n)=O(f(n))。它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐进时间复杂度，简称为时间复杂度。其中f(n)是问题规模n的某个函数。记法：O()，大O记法推导大O阶： 用常数1取代运行时间中的所有假发常数 在修改后的运行次数函数中，只保留最高阶项 如果最高阶存在且不是1，则去除与这个项相乘的常数 得到的最后结果就是大O阶最常用的大O阶1.O(1) 常数阶2.O(n) 线形阶3.O(n^2 ) 平方阶4.O(logn) 对数阶5.O(nlogn) nlogn阶6.O(n^3 ) 立方阶7.O(2^n ) 指数阶 空间复杂度算法的空间复杂度通过计算算法所需的存储空间实现，算法的空间复杂度的计算公式：S(n)=O(f(n))，其中，n为问题规模，f(n)为语句关于n所占存储空间的函数通常，我们都是用时间复杂度来指运行时间的需求，用空间复杂度来指空间需求 图搜索深度优先搜索(DFS) 定义：深度优先搜索（Depth First Search，DFS），是最常见的图搜索方法之一。深度优先搜索沿着一条路径一直走下去，无法行进时，回退回退到刚刚访问的结点，似不撞南墙不回头，不到黄河不死心。深度优先遍历是按照深度优先搜索的方式对图进行遍历。 秘籍:后被访问的顶点，其邻接点先被访问。根据深度优先遍历秘籍，后来先服务，可以借助于栈实现。递归本身就是使用栈实现的，因此使用递归方法更方便。 算法步骤: 初始化图中所有顶点未被访问。 从图中的某个顶点v出发，访问v并标记已访问； 依次检查v的所有邻接点w，如果w未被访问，则从w出发进行深度优先遍历（递归调用，重复2—3步）。 算法图解:生成树:包含所有节点的树生成：包含所有节点 。 树：连通m=n-1，即连通无回路。 代码实现: 哈希表两种实现方式 哈希函数，查找的时间复杂度为O(1)，解决哈希冲突的时候可以采取在冲突位置内嵌链表。 优点：查找速度快。 缺点：数据存放完全无序 二叉树，查找时间复杂度O(logN)。 优点: 数据存放相对有序。 缺点: 查找相对较慢 二叉树概念 二叉树：每个节点只有两个孩子，左孩子，右孩子。 完全二叉树: 每个节点都都含有两个子节点。 二叉搜索树：空树或者左子树每个节点都小于根节点，右子树每个节点都大于根节点 代码class TreeNode(): def __init__(self, val): self.val = val self.left, self.right = None, None","categories":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]}]}