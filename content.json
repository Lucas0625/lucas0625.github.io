{"meta":{"title":"Mr.chen","subtitle":null,"description":null,"author":"琛","url":"http://lucas0625.github.io/blog"},"pages":[{"title":"About","date":"2020-08-05T09:36:20.896Z","updated":"2020-08-05T09:36:20.890Z","comments":true,"path":"about/index.html","permalink":"http://lucas0625.github.io/blog/about/index.html","excerpt":"","text":"个人信息 姓名：野生程序员一枚 性别：男 电子邮箱：chenw0625@163.com Wechat：751009328"},{"title":"Categories","date":"2019-02-05T07:55:36.181Z","updated":"2019-02-04T08:28:25.234Z","comments":true,"path":"categories/index.html","permalink":"http://lucas0625.github.io/blog/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-02-04T08:28:25.266Z","updated":"2019-02-04T08:28:25.246Z","comments":true,"path":"tags/index.html","permalink":"http://lucas0625.github.io/blog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Markdown基本语法总结","slug":"MarkDown语法总结","date":"2020-08-05T09:26:47.739Z","updated":"2020-08-05T09:30:17.246Z","comments":true,"path":"2020/08/05/MarkDown语法总结/","link":"","permalink":"http://lucas0625.github.io/blog/2020/08/05/MarkDown语法总结/","excerpt":"在线培训第一周，基础介绍-MarkDown支持","text":"在线培训第一周，基础介绍-MarkDown支持 Markdown基本语法总结快捷键 加粗 Ctrl + B 斜体 Ctrl + I 引用 Ctrl + Q 插入链接 Ctrl + L 插入代码 Ctrl + K 插入图片 Ctrl + G 提升标题 Ctrl + H 有序列表 Ctrl + O 无序列表 Ctrl + U 横线 Ctrl + R 撤销 Ctrl + Z 重做 Ctrl + Y 分级标题第一种写法： 这是一个一级标题============================这是一个二级标题-------------------------------------------------- 第二种写法： # 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 注：# 和「一级标题」之间建议保留一个字符的空格，这是最标准的 Markdown 写法。参考图 1： 图 1 列表列表格式也很常用，在 Markdown 中，你只需要在文字前面加上 - 就可以了，如果你希望有序列表，也可以在文字前面加上 1. 2. 3. 就可以了，参考图 2。 注：-、1. 和文本之间要保留一个字符的空格。 图 2 插入图片或链接在 Markdown 中，插入链接不需要其他按钮，语法为：[显示文本](链接地址) 。在 Markdown 中，插入图片不需要其他按钮，语法为：![](图片链接地址) 。注：插入图片的语法和链接的语法很像，只是前面多了一个 ！插入链接和图片的案例，如图 3： 图 3 引用在我们写作的时候经常需要引用他人的文字，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了。 Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的 HTML 页面。—— [维基百科] 上面引用的写法，如图 4： 图 4 再看一个案例的截图，如图 5： 图 5 粗体和斜体Markdown 的粗体和斜体也非常简单，用两个* 包含一段文本就是粗体的语法，用一个* 包含一段文本就是斜体的语法。如下图： 代码引用需要引用代码时，如果引用的语句只有一段，不分行，可以用 ` 将语句包起来。如果引用的语句为多行，可以将 代码引用的案例截图：![](http://upload-images.jianshu.io/upload_images/11062747-ff0dc8c6f40e2d27)表格--**Markdown Extra** 表格语法： 项目 价格 Computer $1600 Phone $12 Pipe $1 显示效果：| 项目 | 价格 || --- | --- || Computer | $1600 || Phone | $12 || Pipe | $1 |可以使用冒号来定义对齐方式： 项目 价格 数量 Computer 1600 元 5 Phone 12 元 12 Pipe 1 元 234 | 项目 | 价格 | 数量 || --- | --- | --- || Computer | 1600 元 | 5 || Phone | 12 元 | 12 || Pipe | 1 元 | 234 |显示效果：| 项目 | 价格 | 数量 || --- | --- | --- || Computer | 1600 元 | 5 || Phone | 12 元 | 12 || Pipe | 1 元 | 234 |代码块---代码块语法遵循标准 markdown 代码，例如： @requires_authorizationdef somefunc(param1=’’, param2=0): ‘’’A docstring’’’ if param1 &gt; param2: print ‘Greater’ return (param2 - param1 + 1) or Noneclass SomeClass: pass message = ‘’’interpreter… prompt’’’ ``` 脚注生成一个脚注 [1]. 语法是：[^footnote][^footnote]: 这里是 **脚注** 的 *内容*.这句语法的显示效果一直在页面最后出现。","categories":[{"name":"在线培训","slug":"在线培训","permalink":"http://lucas0625.github.io/blog/categories/在线培训/"}],"tags":[{"name":"学习","slug":"学习","permalink":"http://lucas0625.github.io/blog/tags/学习/"}]},{"title":"Java零基础学习笔记","slug":"Java学习笔记","date":"2019-11-13T10:00:59.026Z","updated":"2020-08-05T09:37:19.144Z","comments":true,"path":"2019/11/13/Java学习笔记/","link":"","permalink":"http://lucas0625.github.io/blog/2019/11/13/Java学习笔记/","excerpt":"形势所迫，从零开始学Java。","text":"形势所迫，从零开始学Java。 计算机基础存储 位（bit）， 一个数字0或者1，代表一位。 字节（Byte），每逢8位是一个字节，这是数据存储的最小单位。 1 Byte = 8 bit 100 Mbps 宽带，b代表的是位， 15 MB/S 实际下载速度，B代表的是字节。 基础语法关键字的概念和特征 定义：具有特殊含义的，被保留的，不能随意使用的字符。 关键字的特点： 完全小写的字母。 在增强版记事本中有特殊颜色。 标志符 标志符定义：是指在程序中，我们自己定义的内容。比如类名字，方法的名字和变量的名字等等。 命名规则： 可以包含英文26个(区分大小写)，0-9数字，$，_ 不能以数字开头 不能是关键字 命名规范： 类名规范：首字母大写，后面每个单词首字母大写(大驼峰式) 方法名规范：首字母小写，后面每个单词首字母大写(小驼峰式) 变量名规范：同方法名规范 常量 概念：是指在java程序中固定不变的数据。 分类： 类型 含义 数据举例 整数常量 所有整数 0，1，567，-9 小数常量 所有的小数 0.0, -0.1, 2.55 字符常量 单引号引起来，只能写一个字符，必须有内容 ‘a’, ‘好’ 字符串常量 双引号引起来，可以写多个字符，也可以不写 “A”, “Hello”, “你好”, “” 布尔常量 只有两个值 true, false 空常量 代表没有任何数据，只有一个值 null 数据类型 数据类型分类： 基本数据类型：整数(byte, short, int, long)，浮点数(float, double)，字符(char)，布尔(boolean) 引用数据类型：字符串，数组，类，接口，Lambda 基本数据类型： 数据类型 关键字 内存占用 取值范围 字节型 byte 1个字节 -128 ～ 127 短整型 short 2个字节 -32768 ～ 32767 整型 int(默认) 4个字节 -2^31 ～ 2^31 - 1 长整型 long 8个字节 -2^63 ～ 2^63 - 1 单精度浮点数 float 4个字节 1.4013E-45 ～ 3.4028E+38 双精度浮点数 double(默认) 8个字节 4.9E-324- ～ 1.7977E+308 字符型 char 2个字节 0 ～ 65535 布尔类型 boolean 1个字节 true，false long类型：建议数据后加L表示，1234Lfloat类型：建议数据后加F表示，1234F 变量定义 格式：数据类型 变量名 = 数值; 注意事项： 变量名称：在同一个大括号范围内，变量的名字不可以相同。 变量赋值：定义的变量，不赋值不能使用。 变量使用不能超过作用域的范围。(作用域：从定义变量的一行开始，一直到直接所属的大括号结束为止。) 数据类型转换 自动转换：将取值范围小的类型自动提升为取值范围大的类型。 转换规则：范围小的向范围大的类型提升，byte, short, char运算时直接提升为int 强制转换：将取值范围大的类型强制转换成取值范围小的类型。 转换规则: 数据类型 变量名 = (数据类型) 被转数据值; 注意事项：1). 强制类型转换一般不推荐使用。2). byte/short/char这三种类型在运算的时候，首先提升为int类型，然后再计算。3). boolean类型不能发生数据类型转换。 数字和字符对照关系表(编码表)： ASCII 码表 Unicode 码表，开头0-127和ASCII码表一致，从128开始包含更多字符。 ASCII码： 字符 数值 0 48 9 57 A 65 Z 90 a 97 z 122 运算符 运算符：进行特定操作的符号，如：’+’ 表达式：用运算符连接起来的式子叫做表达式。 算数运算符 符号 解释 + 加法运算，字符串连接运算 - 减法运算 * 乘法运算 / 除法运算 % 取模运算，两个数字相除取余数 ++，— 自增自减运算 自增自减运算注意事项： int a = 1;int b = ++a;int b = a++; 变量前++， 变量a自己加1，将加1后的结果赋值给b，也就是说a先计算，a和b的结果都是2 变量后++，变量a先把自己的值1，赋值给变量b，此时变量b的值就是1，变量a自己再加1，a的结果是2，b的结果是1。 +运算符的注意事项： + 符号在遇到字符串的时候，表示连接，拼接的含义。 + 符号再遇到字符类型的时候，在计算之前char会被提升为int，然后再计算。 任何数据类型和字符串进行连接的时候，结果都会变成字符串 赋值运算符 说明：将右侧的数据交给左侧的变量。 符号 说明 = 等于号 += 加等于 -= 减等于 *= 乘等于 /= 除等于 %= 取模等 比较运算符 说明：两个数据之间进行比较的运算，运算结果是布尔值true或者false 符号 说明 == 两侧是否相等 &lt; 左边数据是否小于右边数据 &gt; 左边数据是否大于右边数据 &lt;= 左边数据是否小于等于右边数据 &gt;= 左边数据是否大于等于右边数据 != 不等于符号 注意事项：不能连续比较，比如 1 &lt; a &lt; 3，比较这种情况需要使用逻辑运算符，1 &lt; a &amp;&amp; a &lt; 3。 逻辑运算符 说明：是用来连接两个布尔类型结果的运算符，运算结果都是布尔值true或者false 符号 说明 &amp;&amp; 与运算 &#124;&#124; 或运算 ! 取反运算 注意事项： &amp;&amp; 和 || 具有短路效果，如果根据左边已经可以判断得出最终结果，那么右边的代码将不再执行，从而节省一定的性能。 三元运算符 格式：数据类型 变量名 = 布尔类型表达式？结果1:结果2 public static void main(String[] args)&#123; int i = (1==2 ? 100 : 200); System.out.println(i); // 200 int j = (2&lt;=4 ? 500 : 600); System.out.println(j); // 500&#125; 方法 定义：就是将一个功能抽取出来，把代码单独定义在一个大括号内，形成一个单独的功能。 格式： 修饰符 返回值类型 方法名 (参数列表)&#123; 代码... reutn ;&#125; JShell的使用 使用场景：当我们编写的代码非常少的时候，而又不愿意编写类，main方法，也不愿意去编译和运行，这个时候就可以使用JShell工具 启动方式：在终端直接输入jshell命令。 流程控制顺序结构 定义：顺序执行，根据编写的顺序，从上到下运行。 判断语句判断语句1 — ifif (关系表达式)&#123; 语句体;&#125; 判断语句2 — if…elseif (关系表达式)&#123; 语句体1;&#125;else&#123; 语句体2;&#125; 判断语句3 — if..else if…elseif (判断条件1)&#123; 执行语句1;&#125;else if (判断条件2)&#123; 执行语句2;&#125;else if (判断条件3)&#123; 执行语句3;&#125;else&#123; 执行语句;&#125; 选择语句 如果所有的case都不满足，则执行default下的语句。 注意事项：case后面不写break的时候会出现穿透现象，程序一直向后走，不会在判断case，直接运行完整体switch或遇到break。鉴于此，在编写switch语句时，必须写上break。 switch(表达式)&#123; case 常量值1: 语句体1; break; case 常量值2: 语句体2; break; ... default: 语句体n+1; break;&#125; 多个case后面的数值不可以重复。 switch后面的小括号只能是下列数据类型： 基本数据类型: byte/short/char/int 引用数据类型: String字符串, enum枚举 循环语句循环语句1 — for 格式： for (初始化表达式; 布尔表达式; 步进表达式)&#123; 循环体;&#125; 循环语句2 — while 格式： while(布尔表达式)&#123; 循环体； 步进表达式;&#125; 循环语句3 — do..while 格式: do&#123; 循环体; 步进表达式;&#125;while(布尔表达式); 循环语句的区别: 控制条件语句所控制的那个变量，在for循环结束后，就不能在被访问到了，而while循环结束还可以继续使用，如果想继续使用，就用while，否则推荐使用for，原因是for循环结束，该变量就从内存中消失了，能够提高内存的使用效率。 在已知循环次数的时候推荐使用for循环，循环次数未知的时候推荐使用while循环。 跳出语句 break: 终止switch或者循环 continue: 结束本次循环，继续下一次循环 数组 格式: 动态初始化： 数据类型[] 数组名称 = new 数据类型[数组长度] 静态初始化： 数据类型[] 数组名称 = new 数据类型[] {元素1, 元素2, …} 数据类型[] 数组名称 = {元素1, 元素2, ... } java内存划分 栈 Stack: 存放的都是方法中的局部变量。方法的运行一定要在栈当中。 局部变量：方法的参数，或者是方法内部的变量。 作用域：一旦超出作用域，立刻从栈内存当中消失。 堆 Heap：凡是new出来的东西，都在堆中。堆内存里面的东西都有一个地址值：16进制，堆内存里面的数据，都有默认值，规则： 整数 默认 0 浮点数 默认 0.0 字符 默认 ‘\\u0000’ 布尔 默认 false 引用类型 默认 null 方法区 Method Area： 存储class相关信息，包含方法的信息。 本地方法栈 Native Method Stack：与操作系统相关。 寄存器 pc Register: 与CPU相关。 面向对象思想面向对象思想概述 面向过程：当需要实现一个功能时，每一个具体的步骤都要亲力亲为，详细处理每一个细节。 面向对象：当需要实现一个功能时，不关心具体的步骤，而是找一个已经具有该功能的人，来帮我做事。 面向对象特点封装，继承，多态 类和对象 类：是一组相关属性和行为和集合，可以看成是一类事物的模版，使用事物的属性特征和行为特征来描述该类事物。是抽象的。 对象：是一类事物的具体体现。对象是类的一个实例，必然具备该类事物的属性和行为。是具体的。 类和对象的关系：类是对象的模版，对象是类的实体。 类的定义 类的定义格式： public class ClassName&#123; // 成员变量 String name; //姓名 // 成员方法 public void study()&#123; System.out.println(\"好好学习！\"); &#125;&#125; 对象的使用格式: 类名 对象名 = new 类名();``` ### 局部变量和成员变量1. 区别：```javapublic class Car&#123; String color; //成员变量 public void drive()&#123; int speed = 80; //局部变量 System.out.println(\"时速\" + speed); &#125;&#125; 封装性 定义：封装就是将一些细节信息隐藏起来，对于外界不可见。 方法就是一种封装。 关键字private也是一种封装。 访问私有成员变量：提供getXxx方法和setXxx方法。 封装优化1 — this关键字 如果需要访问本类中的成员变量，需要使用this.成员变量名 通过谁调用的方法，谁就是this 封装优化2 — 构造方法 格式： public 类名称(参数类型 参数名称)&#123; 方法体;&#125; 作用：专门用来创建对象的方法，当我们通过关键字 new 来创建对象时，就是在调用构造方法。 如果没有编写任何构造方法，那么编译器就会默认写一个构造方法，没有参数，方法体什么都没有。 java语言标准类规范-javaBean类 所有的成员变量都要使用private关键字修饰 为每一个成员变量编写一对儿Getter/Setter方法 编写一个无参数的构造方法 编写一个全参数的构造方法 public class ClassName&#123; // 成员变量 // 构造方法 // 无参构造方法【必须】 // 有参构造方法【建议】 // 成员方法 // getXxx() // setXxx()&#125;public class Student&#123; // 成员变量 private String name; private int age; // 构造方法 public Student()&#123;&#125; public Student(String name, int age)&#123; this.name = name; this.age = age; &#125; // 成员方法 public void setName(String name)&#123; this.name = name; &#125; public String getName()&#123; return name; &#125; public void setAge(int age)&#123; this.age = age; &#125; public int getAge()&#123; return age; &#125;&#125;","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://lucas0625.github.io/blog/categories/JAVA/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://lucas0625.github.io/blog/tags/Java/"}]},{"title":"传统文本相似度算法","slug":"class-text-similarity","date":"2019-08-07T09:46:55.293Z","updated":"2020-08-05T09:39:16.759Z","comments":true,"path":"2019/08/07/class-text-similarity/","link":"","permalink":"http://lucas0625.github.io/blog/2019/08/07/class-text-similarity/","excerpt":"介绍TF-IDF和BM25","text":"介绍TF-IDF和BM25 文本相似度算法TF−IDF 算法理论 TF是指归一化后的词频，IDF是指逆文档频率。给定一个文档集合 D，有 $d_1,d_2,d_3,……,d_n ∈ D$。文档集合总共包含 m 个词（注：一般在计算 TF−IDF时会去除如 “的” 这一类的停用词），有 $w_1,w_2,w_3,……,w_m ∈W $ 。我们现在以计算词 $w_i$ 在文档 $d_j$ 中的 TF−IDF 指为例。TF的计算公式为： T F=\\frac{f r e q(i, j)}{\\max _{l e n}(j)} 在这里 $freq(i,j)$ 为 $wi$ 在 $d_j$ 中出现的频率，$max{len}(j)$ 为 $d_j$ 长度。 TF 只能是描述词在文档中的频率，但假设现在有个词为 ” 我们 “，这个词可能在文档集 D 中每篇文档中都会出现，并且有较高的频率。那么这一类词就不具有很好的区分文档的能力，为了降低这种通用词的作用，引入了 IDF 。 IDF的表达式如下： IDF = \\log (\\frac {len(D)} {n(i)}) 在这里 $len(D)$ 表示文档集合 $D$ 中文档的总数，$n(i)$ 表示含有 $w_i$ 这个词的文档的数量。 得到 TF 和 IDF 之后，我们将这两个值相乘得到 TF−IDF 的值： TF-IDF = TF * IDF TF 可以计算在一篇文档中词出现的频率，而 IDF 可以降低一些通用词的作用。因此对于一篇文档我们可以用文档中每个词的 TF−IDF 组成的向量来表示该文档，再根据余弦相似度这类的方法来计算文档之间的相关性。 代码实现import mathimport jiebaclass TFIDF(object): def __init__(self, docs): self.D = len(docs) self.docs = docs self.f = [] # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数 self.df = &#123;&#125; # 存储每个词及出现了该词的文档数量 self.idf = &#123;&#125; # 存储每个词的idf值 self.init() def init(self): for doc in self.docs: tmp = &#123;&#125; for word in [item for item in jieba.cut(doc, cut_all=True)]: tmp[word] = tmp.get(word, 0) + 1 # 存储每个文档中每个词的出现次数 self.f.append(tmp) for k in tmp.keys(): self.df[k] = self.df.get(k, 0) + 1 for k, v in self.df.items(): self.idf[k] = math.log(self.D) - math.log(v) def get_tfidf(self, index): tf_idf = [] for word, count in self.f[index].items(): tf = count / len(self.f[index].keys()) tf_idf.append(tf * self.idf[word]) return tf_idf BM25 算法理论 BM25 算法通常用来做搜索相关性评分的，也是 ES 中的搜索算法，通常用来计算 query 和文本集合 D 中每篇文本之间的相关性。我们用 Q 表示 query，在这里 Q 一般是一个句子。在这里我们要对 Q 进行语素解析（一般是分词），在这里以分词为例，我们对 Q 进行分词，得到 $q_1, q_2,……, q_t$ 这样一个词序列。给定文本 $d \\in D$，现在以计算 Q 和 d 之间的分数（相关性），其表达式如下： Score(Q, d) = \\sum_{i = 1}^t w_i * R(q_i, d) 上面式子中 $w_i$ 表示 $q_i$ 的权重，$R(q_i, d)$ 为 $q_i$ 和 $d$ 的相关性，$Score(Q, d)$ 就是每个语素 $q_i$ 和 $d$ 的相关性的加权和。 $w_i$ 的计算方法有很多，一般是用 $IDF$ 来表示的，但这里的 $IDF$ 计算和上面的有所不同，具体的表达式如下： IDF(q_i) = \\log \\frac {N - n(q_i) + 0.5} {n(q_i) + 0.5} 上面式子中 $N$ 表示文本集合中文本的总数量，$n(q_i)$ 表示包含 $q_i$ 这个词的文本的数量，$0.5$ 主要是做平滑处理。 $R(q_i, d)$ 的计算公式如下： R(q_i, d) = \\frac {f_i * (k_1 + 1)} {f_i + K} * \\frac {qf_i * (k_2 + 1)} {qf_i + k_2} 其中 K = k_1 * (1 - b + b * \\frac {dl} {avg dl}) 上面式子中 $f_i$ 为 $q_i$ 在文本 $d$ 中出现的频率，$qf_i$ 为 $q_i$ 在 $Q$ 中出现的频率，$k_1, k_2, b$都是可调节的参数，$dl, avg dl$分别为文本 $d$ 的长度和文本集 $D$ 中所有文本的平均长度。 一般 $qf_i = 1$ ，取 $k_2 = 0$ ，则可以去除后一项，将上面式子改写成： R(q_i, d) = \\frac {f_i * (k_1 + 1)} {f_i + K} 通常设置 $k_1 = 2, b = 0.75$。参数 $b$ 的作用主要是调节文本长度对相关性的影响。 代码实现import mathimport jiebaclass BM25(object): def __init__(self, docs): self.D = len(docs) self.avgdl = sum([len(doc) for doc in docs]) / self.D self.docs = docs self.f = [] # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数 self.df = &#123;&#125; # 存储每个词及出现了该词的文档数量 self.idf = &#123;&#125; # 存储每个词的idf值 self.k1 = 2.0 self.b = 0.75 self.init() def init(self): for doc in self.docs: tmp = &#123;&#125; for word in [item for item in jieba.cut(doc, cut_all=True)]: tmp[word] = tmp.get(word, 0) + 1 # 存储每个文档中每个词的出现次数 self.f.append(tmp) for k in tmp.keys(): self.df[k] = self.df.get(k, 0) + 1 for k, v in self.df.items(): self.idf[k] = math.log(self.D-v+0.5) - math.log(v+0.5) def get_score(self, doc, index): ''' :param doc: 待查询的文本 :param index: 已知文档集合的索引，index=0 表示计算待查询文本与第一个文本之间的分数。 :return: 分数 ''' score = 0 for word in [item for item in jieba.cut(doc, cut_all=True)]: if word not in self.f[index]: continue d = len(self.docs[index]) score += (self.idf[word]*self.f[index][word]*(self.k1+1) / (self.f[index][word]+self.k1*(1-self.b+self.b*d / self.avgdl))) # print(score) return score def get_scores(self, doc): scores = [] for index in range(self.D): score = self.get_score(doc, index) scores.append(score) return scores","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"基本循环神经网路-RNN，LSTM，GRU","slug":"classic-RNN-LSTM-GRU","date":"2019-05-31T08:32:59.359Z","updated":"2020-08-05T09:42:15.460Z","comments":true,"path":"2019/05/31/classic-RNN-LSTM-GRU/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/31/classic-RNN-LSTM-GRU/","excerpt":"循环神经网络RNN,LSTM,GRU的使用","text":"循环神经网络RNN,LSTM,GRU的使用 RNN 调用方式： torch.nn.RNNCell() torch.nn.RNN() 区别：RNNCell() 只能接受序列中单步的输入，且必须传入隐藏状态，而 RNN() 可以接受一个序列的输入，默认会传入全 0 的隐藏状态，也可以自己申明隐藏状态传入。 RNNCell() 参数 参数 含义 input_size 表示输入 $x_t$ 的特征维度 hidden_size 表示输出的特征维度 bias 表示是否使用偏置，默认使用 nonlinearity 表示选用的非线性激活函数，默认是 ‘tanh’ RNN() 参数 参数 含义 input_size 表示输入 $x_t$ 的特征维度 hidden_size 表示输出的特征维度 num_layers 表示网络的层数 nonlinearity 表示选用的非线性激活函数，默认是 ‘tanh’ bias 表示是否使用偏置，默认使用 batch_first 表示输入数据的形式，默认是 False，就是这样形式，(seq, batch, feature)，也就是将序列长度放在第一位，batch 放在第二位 dropout 表示是否在输出层应用 dropout bidirectional 表示是否使用双向的 RNN，默认是 False import torchfrom torch.autograd import Variablefrom torch import nn # 定义一个单步的 rnnrnn_single = nn.RNNCell(input_size=100, hidden_size=200) # 访问其中的参数rnn_single.weight_hh Parameter containing: tensor([[-0.0017, -0.0512, -0.0353, ..., 0.0567, 0.0081, 0.0408], [ 0.0489, 0.0315, 0.0233, ..., 0.0467, -0.0075, 0.0536], [ 0.0581, 0.0614, -0.0220, ..., -0.0251, 0.0210, -0.0058], ..., [-0.0100, 0.0681, -0.0473, ..., -0.0219, -0.0165, -0.0567], [-0.0447, 0.0149, 0.0063, ..., -0.0212, -0.0591, -0.0601], [-0.0382, -0.0337, -0.0404, ..., -0.0365, -0.0420, 0.0293]], requires_grad=True) 就是上图的矩阵 A # 构造一个序列，长为 6， batch 是 5， 特征是 100x = Variable(torch.randn(6, 5, 100)) # RNN 的输入格式 x tensor([[[-2.0104, 0.2796, 1.5232, ..., -1.8635, 0.0042, 0.5422], [ 1.2934, -0.4782, -1.1989, ..., -0.6775, 0.4277, -0.3155], [ 0.4077, -0.3299, 0.0967, ..., -1.0189, 0.0975, 0.7678], [-1.6604, -0.7599, -0.3706, ..., -0.2911, 2.4880, 0.4783], [-1.5688, 1.1583, -0.3489, ..., 0.3275, 1.0333, -1.2397]], [[ 0.6300, 0.5252, 0.7385, ..., 0.2579, -0.0397, -1.1270], [ 1.3801, 0.7126, -0.2537, ..., -0.4507, 2.1889, 0.7648], [-0.9598, -0.0850, 0.6514, ..., 0.7625, 1.0525, -0.2042], [ 0.1205, 0.0997, 0.7736, ..., 0.5133, -1.9996, 0.2184], [-2.4407, 0.2622, 0.6602, ..., 0.4392, -0.3943, -0.5903]], [[-2.0361, 0.3635, -0.2484, ..., -0.2929, -1.0250, -0.9464], [-1.1445, -0.7474, 1.3661, ..., -0.0825, -0.9291, -1.2322], [ 1.2757, 0.5088, -0.0305, ..., 1.0552, -1.7565, -0.9720], [ 1.0957, 1.7836, 1.2204, ..., -0.4245, -0.3996, -0.5425], [ 0.2476, 0.2516, -0.9161, ..., 1.0104, 0.8708, -1.5077]], [[-0.5922, -0.1151, -0.8730, ..., 2.0240, 0.0284, 0.4642], [ 0.8369, -0.0199, 1.4351, ..., 0.8096, -1.4119, 0.4124], [ 0.1630, -1.3134, -0.1717, ..., 0.9940, 1.4097, -1.0362], [ 2.1787, 0.8273, -0.4433, ..., -0.6921, -1.7547, 0.8085], [ 0.3586, 0.6565, 1.6782, ..., 0.1528, -1.7319, 0.2215]], [[-1.1259, 1.8154, -0.6593, ..., 1.5540, 1.4334, -1.5127], [ 1.3139, 0.8352, 1.4495, ..., 0.8871, 0.0298, 0.5145], [ 0.3252, 0.3663, 0.3321, ..., -1.9635, -0.0162, 1.8176], [-0.3583, 1.0319, 0.9324, ..., -0.0877, 0.5046, 0.8212], [ 0.0805, -0.5177, 0.6721, ..., -0.2758, -1.7134, 0.6457]], [[ 0.6160, -0.2190, 0.1615, ..., 0.0088, 1.0142, 0.1040], [-0.6514, -0.5932, -0.1138, ..., 2.3709, 0.9282, 1.5059], [ 1.7666, 0.5471, 0.6585, ..., 0.4704, 0.7371, 0.7960], [ 1.2190, -0.4006, 1.7100, ..., -0.5754, 0.5801, 0.9159], [-0.8331, -0.5066, -0.3903, ..., -0.2636, -0.8551, 1.0103]]]) # 定义初始的记忆状态h_t = Variable(torch.zeros(5, 200)) # 传入 RNNout = []# 通过 6 次循环作用在整个序列上for i in range(6): h_t = rnn_single(x[i], h_t) out.append(h_t) len(out) 6 out[0].shape # 每个输出的纬度 torch.Size([5, 200]) # 直接使用 RNNrnn_seq = nn.RNN(100, 200) # 访问其中的参数rnn_seq.weight_hh_l0 Parameter containing: tensor([[ 0.0367, -0.0456, 0.0555, ..., -0.0535, 0.0080, 0.0198], [ 0.0440, 0.0042, 0.0356, ..., -0.0534, -0.0374, 0.0550], [-0.0172, -0.0028, -0.0425, ..., 0.0308, 0.0062, -0.0474], ..., [-0.0336, -0.0054, 0.0188, ..., -0.0328, 0.0329, 0.0065], [-0.0484, -0.0571, 0.0554, ..., 0.0681, 0.0305, -0.0171], [ 0.0591, -0.0093, -0.0581, ..., 0.0669, -0.0276, -0.0002]], requires_grad=True) out, h_t = rnn_seq(x) # 使用默认的全 0 隐藏状态# h_t 是网络最后的隐藏状态 len(out) 6 # 自己定义初始的隐藏状态# 三个纬度 (nums_layers * num_direction, batch, hidden_size)h_0 = Variable(torch.randn(1, 5, 200)) out, h_t = rnn_seq(x, h_0) # 输出结果(seq, batch, feature)out.shape torch.Size([6, 5, 200]) LSTM和 RNN类似 lstm_seq = nn.LSTM(50, 100, num_layers=2) # 输入纬度 100， 输出 200， 两层 lstm_seq.weight_hh_l0 # 第一层的 h_t 权重 Parameter containing: tensor([[ 0.0144, 0.0360, 0.0436, ..., 0.0026, 0.0195, -0.0283], [-0.0724, -0.0119, -0.0976, ..., 0.0060, 0.0651, -0.0557], [ 0.0106, -0.0049, -0.0543, ..., -0.0541, 0.0663, 0.0701], ..., [-0.0481, 0.0589, -0.0284, ..., 0.0673, -0.0895, -0.0900], [ 0.0466, -0.0856, 0.0445, ..., -0.0995, 0.0051, -0.0333], [-0.0520, 0.0953, 0.0334, ..., -0.0382, 0.0038, 0.0300]], requires_grad=True) lstm_input = Variable(torch.randn(10, 3, 50)) # 序列 10， batch 是 3，输入纬度 50 out, (h, c) = lstm_seq(lstm_input) # 使用默认全 0 隐藏状态 LSTM 输出的隐藏状态有两个， h 和 c， 指上图中每个 cell 之间的两个箭头，这两个隐藏状态的大小都是相同的，(num_layers * direction, batch, feature) h.shape # 两层， batch 是 3， 特征是 100 torch.Size([2, 3, 100]) c.shape torch.Size([2, 3, 100]) # 如果不使用默认隐藏状态，需要传入两个张量h_init = Variable(torch.randn(2, 3, 100))c_init = Variable(torch.randn(2, 3, 100)) out, (h, c) = lstm_seq(lstm_input, (h_init, c_init)) h.shape torch.Size([2, 3, 100]) c.shape torch.Size([2, 3, 100]) out.shape torch.Size([10, 3, 100]) GRU gru_seq = nn.GRU(10, 20)gru_input = Variable(torch.randn(3, 32, 10))out, h = gru_seq(gru_input) gru_seq.weight_hh_l0 Parameter containing: tensor([[-0.1429, 0.2029, 0.0808, ..., -0.1313, -0.0468, -0.2004], [ 0.1702, 0.1171, -0.0227, ..., 0.1935, 0.0337, 0.1462], [-0.0190, 0.1536, 0.1593, ..., -0.1864, 0.1081, -0.1402], ..., [ 0.0619, 0.0313, -0.0003, ..., -0.1213, -0.0944, 0.1185], [ 0.0532, -0.1704, 0.1925, ..., -0.1551, 0.0528, -0.1263], [ 0.1027, 0.2233, 0.0538, ..., 0.2153, -0.0864, -0.1121]], requires_grad=True) h.shape torch.Size([1, 32, 20]) out.shape torch.Size([3, 32, 20])","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"循环神经网络基础概念","slug":"RNN-base","date":"2019-05-31T06:46:55.594Z","updated":"2020-08-05T09:42:15.445Z","comments":true,"path":"2019/05/31/RNN-base/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/31/RNN-base/","excerpt":"循环神经网络概念","text":"循环神经网络概念 RNN原理具有记忆效果的网络不断将当前序列的中的数据点传入网络。采用参数共享的方式 公式 h_{t}=\\tanh \\left(w_{i h} x_{t}+b_{i h}+w_{h h} h_{t-1}+b_{h n}\\right)其中 $h_{t-1}$ 表示歉意不的记忆状态，$x_t$ 表示当前步的输入，最后得到输出 $h_t$，同时将其传入后面作为这一步的记忆状态。 存在的问题当参数小于 1 的时候，反向传播越乘越小，造成梯度消失当参数大于 1 的时候，反向传播越乘越大，造成梯度爆炸 LSTM原理LSTM 由三个门来控制，分别是输入门，遗忘门和输出门，输入门和输出门限制着输入和输出的大小，而遗忘门控制着记忆的保留度，对于一个任务，遗忘门能够自己学习到该保留多少以前的记忆，不需要人为进行干扰，所以具备长时记忆的功能。 公式 f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)首先结合$t-1$时刻的网络输出和$t$时刻的网络输入，通过线性变换和 sigmoid 函数得到一个 0-1 之间的值$f_t$，这个值称为衰减系数，表示保留过去信息的多少。 \\begin{aligned} i_{t} &=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\ \\tilde{C}_{t} &=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right) \\end{aligned}然后结合$t-1$时刻的输出和$t$时刻的输入分别计算出另外一个系数 $it$ 和新接受的信息 $\\tilde{C}{t}$,其中 $i_t$ 表示保留接受的新信息的多少。 C_{t}=f_{t} * C_{t-1}+i_{t} * \\tilde{C}_{t}然后将前面得到的两个衰减系数分别乘上过去的信息和现在的新信息来确定$t$时刻真正的记忆状态$C_t$。 \\begin{aligned} o_{t} &=\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right) \\\\ h_{t} &=o_{t} * \\tanh \\left(C_{t}\\right) \\end{aligned}最后使用$t-1$时刻的输出和$t$时刻的输入计算一个系数，这个系数表示$t$时刻到底输出多少的记忆状态$C_t$得到真正的模型输出$h_t$ GRUGRU 和 LSTM 最大的不同是， GRU 将遗忘门和输入门合并成了一个更新门，同时网络不再额外给出记忆状态$C_t$，而是将输出结果作为记忆状态不断往后传。 \\begin{aligned} z_{t} &=\\sigma\\left(W_{z} \\cdot\\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} &=\\sigma\\left(W_{r} \\cdot\\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} &=\\tanh \\left(W \\cdot\\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} &=\\left(1-z_{t}\\right) * h_{t-1}+z_{t} * \\tilde{h}_{t} \\end{aligned}","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"神经网络训练技巧","slug":"network-train","date":"2019-05-29T06:59:17.774Z","updated":"2020-08-05T09:42:26.117Z","comments":true,"path":"2019/05/29/network-train/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/29/network-train/","excerpt":"神经网络训练过程中的一些技巧","text":"神经网络训练过程中的一些技巧 卷积神经网络训练技巧数据增强回译法 数据读取Dataset灵活的数据读入，用到 torch.utils.data.Dataset()。可以先看看 Dataset 的文档 如果我们希望定义自己的数据读入函数，我们只需要定义一个子类继承于 Dataset，然后重新定义 __getitem__() 和 __len__() 这两个函数就可以了，__getitem__() 表示按照下标取出其中一个数据，__len__ 表示所有数据的总数/ 数据格式如下： from torch.utils.data import Dataset # 定义一个子类叫 custom_dataset，继承于 Datasetclass Custom_dataset(Dataset): def __init__(self, txt_path, transform=None): self.transform = transform # 传入数据预处理 with open(txt_path, 'r') as f: lines = f.readlines() self.img_list = [i.split()[0] for i in lines] # 得到所有的图像名字 self.label_list = [i.split()[1] for i in lines] # 得到所有的label def __getitem__(self, idx): # 根据 idx 取出一个 img = self.img_list[idx] label = self.label_list[idx] if self.transform is not None: img = self.transform(img) return img, label def __len__(self): # 总数据是多少 return len(self.label_list) # 测试读取函数txt_dataset = Custom_dataset('./train.txt') # 读入 txt 文件# 取得其中一个数据data, label = txt_dataset[0]print(data)print(label) 1009_2.png YOU 通过上诉方式我们也能够非常方便的定义一个数据读入，同时也能够方便的定义数据预处理 DataLoader实现了一个 batch 的处理数据，DataLoader 是一个多线程迭代器，能够帮助我们一个 batch 的读入模型，同时使用多线程速度更快。DataLoader 的文档 DataLoader 中有几个使用最多的参数，第一个是 dataset，就是我们前面定义的数据读入，可以使用 ImageFolder，可以使用自己定义的数据读入子类，第二个是 batch_size，这就是一批多少个数据，第三个是 shuffle，表示是否打乱数据，第四个是 num_workers，表示使用几个线程，默认使用主线程，第五个是 drop_last，表示是否扔掉最后无法构成一个批次的数据。 除了这些参数之外，还有一个参数叫 collate_fn , 能够方便的处理一个 batch 中的数据。这个是在 DataLoader 中已经有默认定义了，可以看源码如果希望将 batch 输出的 label 补成相同的长度，短的 label 用 0 填充，这其实是在机器翻译中的一个需求，这个时候我们就需要使用 collate_fn 来自定义我们 batch 的处理方式， from torch.utils.data import DataLoader train_data1 = DataLoader(txt_dataset, batch_size=100, shuffle=True) # 将 2 个数据作为一个batchfor im, label in train_data1: print(label) break (&#39;JOHN&#39;, &#39;2009&#39;, &#39;7&#39;, &#39;WRIGHT&#39;, &#39;NELSON&#39;, &#39;IS&#39;, &#39;HOUSE&#39;, &#39;TOM&#39;, &#39;8275&#39;, &#39;MAID&#39;, &#39;AH&#39;, &#39;RESIST&#39;, &#39;SALESDCMOBILEADSCOM&#39;, &#39;ONGC&#39;, &#39;AUR&#39;, &#39;BIG&#39;, &#39;THE&#39;, &#39;OF&#39;, &#39;YOU&#39;, &#39;UP&#39;, &#39;BUTTE&#39;, &#39;YOU&#39;, &#39;CROFT&#39;, &#39;SHIPPING&#39;, &#39;PINE&#39;, &#39;ENEMIES&#39;, &#39;816527&#39;, &#39;18&#39;, &#39;ASSOCIATE&#39;, &#39;DEPOSITPHOTOS&#39;, &#39;KANTOR&#39;, &#39;IS&#39;, &#39;2008&#39;, &#39;APPLE&#39;, &#39;GEBRUIK&#39;, &#39;SWAYAMVARA&#39;, &#39;IN&#39;, &#39;SHORT&#39;, &#39;COFFEE&#39;, &#39;2010&#39;, &#39;55&#39;, &#39;ROGEN&#39;, &#39;TOO&#39;, &#39;RENTON&#39;, &#39;SHORT&#39;, &#39;IS&#39;, &#39;FINAL&#39;, &#39;AWARD&#39;, &#39;AND&#39;, &#39;KURT&#39;, &#39;VS&#39;, &#39;HUDGENS&#39;, &#39;CROFT&#39;, &#39;DAY&#39;, &#39;22&#39;, &#39;SWAN&#39;, &#39;WE&#39;, &#39;SEPTEMBER&#39;, &#39;COM&#39;, &#39;YOUR&#39;, &#39;EN&#39;, &#39;STRATEGIC&#39;, &#39;TOTAL&#39;, &#39;SPOTLIGHT&#39;, &#39;GUILDHALL&#39;, &#39;PERFORMANCE&#39;, &#39;FREE&#39;, &#39;CARL&#39;, &#39;FAST&#39;, &#39;103&#39;, &#39;OPENING&#39;, &#39;BANKS&#39;, &#39;150&#39;, &#39;MEDIA&#39;, &#39;ZAPORIZHIA&#39;, &#39;AC&#39;, &#39;VAL&#39;, &#39;STATE&#39;, &#39;SHOAH&#39;, &#39;SRI&#39;, &#39;GREAT&#39;, &#39;CLEARANCE&#39;, &#39;ONE&#39;, &#39;THE&#39;, &#39;HOSTING&#39;, &#39;REWARDS&#39;, &#39;LIFE&#39;, &#39;SBI&#39;, &#39;SCALES&#39;, &#39;SPACE&#39;, &#39;YOU&#39;, &#39;THERE&#39;, &#39;THE&#39;, &#39;FOUND&#39;, &#39;70304&#39;, &#39;SO&#39;, &#39;HELT&#39;, &#39;7749&#39;, &#39;PRICES&#39;, &#39;UFO&#39;) def collate_fn(batch): batch.sort(key=lambda x: len(x[1]), reverse=True) # 将数据集按照 label 的长度从大到小排序 img, label = zip(*batch) # 将数据和 label 配对取出 pad_label = [] lens = [] max_len = len(label[0]) for i in range(len(label)): temp_label = label[i] temp_label += '0' * (max_len - len(label[i])) pad_label.append(temp_label) lens.append(len(label[i])) return img, pad_label, lens # 输出 label 的真实长度 # 使用自定义的 collate_fntrain_data3 = DataLoader(txt_dataset, 10, True, collate_fn=collate_fn)im, label, lens = next(iter(train_data3)) im (&#39;2292_1.png&#39;, &#39;857_23.png&#39;, &#39;2433_6.png&#39;, &#39;2539_1.png&#39;, &#39;2314_9.png&#39;, &#39;5142_4.png&#39;, &#39;2497_6.png&#39;, &#39;325_16.png&#39;, &#39;2202_1.png&#39;, &#39;940_1.png&#39;) label [&#39;HAZELMERR&#39;, &#39;PORTABLE0&#39;, &#39;PRIVATE00&#39;, &#39;827500000&#39;, &#39;HERE00000&#39;, &#39;BALE00000&#39;, &#39;VIEW00000&#39;, &#39;HOW000000&#39;, &#39;FOR000000&#39;, &#39;300000000&#39;] lens [9, 8, 7, 4, 4, 4, 4, 3, 3, 2] 可以看到一个 batch 中所有的 label 都从长到短进行排列，同时短的 label 都被补长了，所以使用 collate_fn 能够非常方便的处理一个 batch 中的数据，一般情况下，没有特别的要求，使用 pytorch 中内置的 collate_fn 就可以满足要求了 学习率衰减在 pytorch 中学习率衰减非常方便，使用 torch.optim.lr_scheduler，更多的信息可以直接查看文档 也可以用下面这种方式来做学习率衰减，更加直观 import numpy as npimport torchfrom torch import nnimport torch.nn.functional as Ffrom torch.autograd import Variable class Net(nn.Module): def __init__(self): pass def forward(self,): pass net = Net() # optimizer = torch.optim.SGD(net,lr=0.01, weight_decay=1e-4) 可以通过 optimizer.param_groups来得到所有参数组和其对应的属性, 参数组是什么意思呢？就是我们可以将模型的参数分成几个组，每个组定义一个学习率，这里比较复杂，一般来讲如果不做特别修改，就只有一个参数组.这个参数组是一个字典，里面有很多属性，比如学习率，权重衰减等等，我们可以访问以下 # print('learning rate: &#123;&#125;'.format(optimizer.param_groups[0]['lr']))# print('weight decay: &#123;&#125;'.format(optimizer.param_groups[0]['weight_decay'])) # learning rate: 0.01# weight decay: 0.0001 # 可以通过修改这个属性来改变我们训练过程中的学习率# optimizer.param_groups[0]['lr'] = 1e-5# 为了防止有多个参数组，我们可以使用一个循环# for param_group in optimizer.param_groups: # param_group['lr'] = 1e-1 # 在任意位置改变学习率def set_learning_rate(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lr 批标准化# 实现一维的情况import torchdef simple_batch_norm_1d(x, gamma, beta): eps = 1e-5 x_mean = torch.mean(x, dim=0, keepdim=True) # 保留纬度进行 broadcast x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True) x_hat = (x - x_mean) / torch.sqrt(x_var + eps) return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean) # 验证对于任何输入会不会被标准化x = torch.arange(15, dtype=torch.float).view(5, 3)gamma = torch.ones(x.shape[1])beta = torch.zeros(x.shape[1])print('before bn:')print(x)y = simple_batch_norm_1d(x, gamma, beta)print('after bn:')print(y) before bn: tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.], [12., 13., 14.]]) after bn: tensor([[-1.4142, -1.4142, -1.4142], [-0.7071, -0.7071, -0.7071], [ 0.0000, 0.0000, 0.0000], [ 0.7071, 0.7071, 0.7071], [ 1.4142, 1.4142, 1.4142]]) 可以看到这里一共是 5 个数据点，三个特征，每一列表示一个特征的不同数据点，使用批标准化之后，每一列都变成了标准的正态分布 这个时候会出现一个问题，就是测试的时候该使用批标准化吗？ 答案是肯定的，因为训练的时候使用了，而测试的时候不使用肯定会导致结果出现偏差，但是测试的时候如果只有一个数据集，那么均值不就是这个值，方差为 0 吗？这显然是随机的，所以测试的时候不能用测试的数据集去算均值和方差，而是用训练的时候算出的移动平均均值和方差去代替 下面我们实现以下能够区分训练状态和测试状态的批标准化方法 def batch_norm_1d(x, gamma, beta, is_training, moving_mean, moving_var, moving_momentum=0.1): eps = 1e-5 x_mean = torch.mean(x, dim=0, keepdim=True) # 保留维度进行 broadcast x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True) if is_training: x_hat = (x - x_mean) / torch.sqrt(x_var + eps) moving_mean[:] = moving_momentum * moving_mean + (1. - moving_momentum) * x_mean moving_var[:] = moving_momentum * moving_var + (1. - moving_momentum) * x_var else: x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps) return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean) 从上面可以看到，我们自己实现了 2 维情况的批标准化，对应于卷积的 4 维情况的标准化是类似的，只需要沿着通道的维度进行均值和方差的计算，但是我们自己实现批标准化是很累的，pytorch 当然也为我们内置了批标准化的函数，一维和二维分别是 torch.nn.BatchNorm1d() 和 torch.nn.BatchNorm2d()，不同于我们的实现，pytorch 不仅将 $\\gamma$ 和 $\\beta$ 作为训练的参数，也将 moving_mean 和 moving_var 也作为参数进行训练 正则化使用正则化之后，更新参数变为： p_j \\rightarrow p_j - \\eta (\\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j) = p_j - \\eta \\frac{\\partial loss}{\\partial p_j} - 2 \\eta \\lambda p_j如果想在随机梯度下降法中使用正则项，或者说权重衰减，torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4) 就可以了，这个 weight_decay 系数就是上面公式中的 $\\lambda$，非常方便 TensorBoard 可视化tensorboard 是 tensorflow 中非常好用的可视化工具，使用这个工具github 可以在 pytorch 中实现可视化. 需要安装以下模块 pip install tensorflow pip install tensorboradX from tensorboardX import SummaryWriter writer = SummaryWriter() # ================ ֵ使用 tensorboard =============== # writer.add_scalars('Loss', &#123;'train': train_loss / len(train_data), 'valid': valid_loss / len(valid_data)&#125;, epoch) # writer.add_scalars('Acc', &#123;'train': train_acc / len(train_data), 'valid': valid_acc / len(valid_data)&#125;, epoch) # =================================================","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"百宝箱","slug":"kinds","date":"2019-05-17T10:55:02.081Z","updated":"2020-08-05T09:44:24.280Z","comments":true,"path":"2019/05/17/kinds/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/17/kinds/","excerpt":"记录自己平时用到的一些小知识","text":"记录自己平时用到的一些小知识 返回本机IP地址 https://utool.fun/ip/ https://api.ip.la/ https://httpbin.org/ip PYPI源 清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/ 使用：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade numpy","categories":[{"name":"私人收藏","slug":"私人收藏","permalink":"http://lucas0625.github.io/blog/categories/私人收藏/"}],"tags":[{"name":"其他","slug":"其他","permalink":"http://lucas0625.github.io/blog/tags/其他/"}]},{"title":"Linux 常用操作","slug":"linux","date":"2019-05-17T01:26:51.213Z","updated":"2020-06-02T10:31:24.613Z","comments":true,"path":"2019/05/17/linux/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/17/linux/","excerpt":"记录平时用到的一些linux命令","text":"记录平时用到的一些linux命令 查看当前目录下文件夹的大小du -h —max-depth=1 分割，合并文件 分割: sed -n ‘1, 100p’ file &gt; file_part1 合并: cat file1 file2 &gt; file 替换sed -i “s/ //g” test 替换空格为空 文件编码 查看文件编码: file filename 转换文件编码: iconv -f 原编码 -t 新编码 原文件 -o 新文件 批量转换文件编码: 第一步：先克隆原文件夹得目录结构 第二步：开始批量转换 find 文件夹名 -type d -exec sh -c “mkdir -p new/{}” \\; find 文件夹名 -type f -exec sh -c “iconv -f GBK -t UTF-8 {} &gt; new/{}” \\; 文件查找grep -rn ‘words’ * : 表示当前目录所有文件，也可以是某个文件名-r 是递归查找-n 是显示行号-R 查找所有文件包含子目录-i 忽略大小写 统计文件大小 wc -l file 计算文件行数 wc -w file 计算文件中的单词数 wc -c file 计算文件中的字符数 文件操作 cp sourcefile destfile 文件拷贝 mv oldname newname 重命名或者文件移动 rm file 删除文件 grep ‘pattern’ file 在文件内搜索字符串 cut -b colnum file 指定欲显示的文件内容范围 cat file 输出文件内容 file somefile 得到文件类型 抓包操作 tcpdumptcpdump是linux系统自带的抓包工具，主要通过命令行的方式。 命令行格式： tcpdump [ -adeflnNOpqStvx ] [ -c 数量 ] [ -F 文件名 ][ -i 网络接口 ] [ -r 文件名] [ -s snaplen ][ -T 类型 ] [ -w 文件名 ] [表达式 ] 常用的参数： -l 使标准输出变为缓冲行形式； -n 不把网络地址转换成名字； -c 在收到指定的包的数目后，tcpdump就会停止； -i 指定监听的网络接口；(如果没有指定可能在默认网卡上监听，需要指定绑定了特定IP的网卡) -w 直接将包写入文件中，并不分析和打印出来； -s 指定记录package的大小，常见 -s 0 ，代表最大值65535，一半linux传输最小单元MTU为1500，足够了 -X 直接输出package data数据，默认不设置，只能通过-w指定文件进行输出 常用表达式： 关于类型的关键字，主要包括host，net，port 传输方向的关键字，主要包括src , dst ,dst or src, dst and src 协议的关键字，主要包括fddi,ip ,arp,rarp,tcp,udp等类型 逻辑运算，取非运算是 not !, 与运算是and,&amp;&amp;;或运算 是or ,|| 其他重要的关键字如下：gateway, broadcast,less,greater 例子： http数据包抓取 (直接在终端输出package data) tcpdump tcp port 80 -n -X -s 0 指定80端口进行输出 抓取http包数据指定文件进行输出package tcpdump tcp port 80 -n -s 0 -w /tmp/tcp.cap 对应的/tmp/tcp.cap基本靠肉眼已经能看一下信息，比如http Header , content信息等 结合管道流 tcpdump tcp port 80 -n -s 0 -X -l | grep xxxx 这样可以实时对数据包进行字符串匹配过滤 mod_proxy反向代理抓包 线上服务器apache+jetty，通过apache mod_proxy进行一个反向代理，80 apache端口, 7001 jetty端口 apache端口数据抓包：tcpdump tcp port 80 -n -s 0 -X -i eth0 注意：指定eth0网络接口jetty端口数据抓包： tcpdump tcp port 7001 -n -s 0 -X -i lo 注意：指定Loopback网络接口 只监控特定的ip主机tcpdump tcp host 10.16.2.85 and port 2100 -s 0 -X需要使用tcp表达式的组合，这里是host指示只监听该ip","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"vim 常用技巧","slug":"vim","date":"2019-05-17T01:26:29.091Z","updated":"2019-05-17T01:33:37.304Z","comments":true,"path":"2019/05/17/vim/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/17/vim/","excerpt":"记录平时用到的一些vim命令","text":"记录平时用到的一些vim命令 视图模式下常用设置 set invlist 显示不可见字符 set nu / nonu 显示行号，不显示行号 s/old/new/g 替换old为new, 1, 100s/old/new/g 将1-100行之间的old替换为new set hlsearch 查找高亮 set nohlsearch 取消查找高亮 打开多个编辑器vim -O2 file1 file2 打开两个vim编辑器ctrl+w+l/h 左右切换光标 强制写入tabcrtl + v + i 强制写入\\t，即^I","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"CNN 基础","slug":"CNN-base","date":"2019-05-16T06:21:13.849Z","updated":"2020-08-05T09:45:24.713Z","comments":true,"path":"2019/05/16/CNN-base/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/16/CNN-base/","excerpt":"本文介绍了CNN基础。","text":"本文介绍了CNN基础。 卷积概念 卷积模块卷积卷积在 PyTorch 中有两种方式， 一种是torch.nn.Conv2d(), 一种是torch.nn.functional.conv2d()， 这两种形式本质上都是使用一个卷积操作。这两种形式的卷积对于输入要求是一样的，都是输入torch.autograd.Variable()，大小是(batch, channel, H, W), 其中 batch 表示输入的一批数据的数目 channel 表示输入的通道数，一般一张彩色图片的通道数为3，灰度图是1 H 表示输入图片的高度 W 表示输入图片的宽度 # 实际操作import numpy as np import torch from torch import nn from torch.autograd import Variable import torch.nn.functional as F from PIL import Image import matplotlib.pyplot as plt %matplotlib inline im = Image.open('./cat.png').convert('L') # 读入一张灰度图的图片im = np.array(im, dtype='float32') # 将其转换为一个矩阵im array([[122., 122., 121., ..., 93., 90., 88.], [124., 124., 123., ..., 96., 93., 91.], [126., 126., 125., ..., 100., 97., 95.], ..., [161., 161., 161., ..., 0., 0., 0.], [160., 160., 161., ..., 0., 0., 0.], [160., 160., 160., ..., 0., 0., 0.]], dtype=float32) # 可视化图片plt.imshow(im.astype('uint8'), cmap='gray') &lt;matplotlib.image.AxesImage at 0x1183e2518&gt; im.shape (224, 224) # 将图片矩阵转化为 pytorh tensor， 并适配卷积输入的要求im = torch.from_numpy(im.reshape((1, 1, im.shape[0], im.shape[1]))) im tensor([[[[122., 122., 121., ..., 93., 90., 88.], [124., 124., 123., ..., 96., 93., 91.], [126., 126., 125., ..., 100., 97., 95.], ..., [161., 161., 161., ..., 0., 0., 0.], [160., 160., 161., ..., 0., 0., 0.], [160., 160., 160., ..., 0., 0., 0.]]]]) # 定义一个算子，对其进行轮廓检测# 使用 nn.Conv2d'''torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')'''conv1 = nn.Conv2d(1, 1, 3, bias=True) # 定义卷积sobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # 定义轮廓检测算子sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出conv1.weight.data = torch.from_numpy(sobel_kernel) # 给卷积的 kernel 赋值edge1 = conv1(Variable(im)) # 作用在图片上edge1 = edge1.data.squeeze().numpy() # 将输出转换为图片的格式 # 可视化边缘检测之后的结果plt.imshow(edge1, cmap='gray') &lt;matplotlib.image.AxesImage at 0x119dce9e8&gt; # 使用 F.conv2dsobel_kernek = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # 定义轮廓检测算子sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出weight = Variable(torch.from_numpy(sobel_kernel))edge2 = F.conv2d(Variable(im), weight) # 作用在图片上edge2 = edge2.data.squeeze().numpy() # 将输出转化为图片的格式plt.imshow(edge2, cmap='gray') &lt;matplotlib.image.AxesImage at 0x119c46da0&gt; 两种卷积方式的区别 nn.Conv2d() 相当于直接定义了一层卷积网络结构, 默认定义一个随机初始化的 weight, 需要修改的话，取出其中的值对其修改 torch.nn.functional.conv2d() 相当于定义了一个卷积的操作，需要再额外定义一个 weight ，weight 也必须是一个 Varibale 实际使用中，基本都使用 nn.Conv2d() 池化模块池化池化层 可以降低矩阵的大小，非常好地提升计算效率，池化层没有参数，池化的方式有很多种。 最大值池化 均值池化 在卷积网络中一般使用最大值池化。PyTorch 中两种池化方式， nn.MaxPool2d() torch.nn.functional.max_pool2d() 参数和卷积参数一致 # 使用 nn.MaxPool2dpool1 = nn.MaxPool2d(2, 2)print('before max pool, image shape: &#123;&#125; x &#123;&#125;'.format(im.shape[2], im.shape[3]))small_im1 = pool1(Variable(im))small_im1 = small_im1.data.squeeze().numpy()print('after max pool, image shape: &#123;&#125; x &#123;&#125; '.format(small_im1.shape[0], small_im1.shape[1])) before max pool, image shape: 224 x 224 after max pool, image shape: 112 x 112 # 图片大小减少了一般，看一下图像的表现plt.imshow(small_im1, cmap='gray') &lt;matplotlib.image.AxesImage at 0x119e82780&gt; # 第二种方式 F.max_pool2dprint('before max pool, image shape: &#123;&#125; x &#123;&#125;'.format(im.shape[2], im.shape[3]))small_im2 = F.max_pool2d(Variable(im), 2, 2)small_im2 = small_im2.data.squeeze().numpy()print('after max pool, image shape: &#123;&#125; x &#123;&#125; '.format(small_im1.shape[0], small_im1.shape[1]))plt.imshow(small_im2, cmap='gray') before max pool, image shape: 224 x 224 after max pool, image shape: 112 x 112 &lt;matplotlib.image.AxesImage at 0x11a074898&gt; 两种方式结果一样，一般使用 nn.MaxPool2d()","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"多层神经网络","slug":"multilayer-neural-network","date":"2019-05-14T06:20:23.740Z","updated":"2020-08-05T09:45:24.688Z","comments":true,"path":"2019/05/14/multilayer-neural-network/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/14/multilayer-neural-network/","excerpt":"PyTorch 实现多层神经网络","text":"PyTorch 实现多层神经网络 多层神经网络概念demo 多层神经网络实现手动构建模型import torchimport numpy as npfrom torch import nnfrom torch.autograd import Variableimport torch.nn.functional as Fimport matplotlib.pyplot as plt%matplotlib inline def plot_decision_boundary(model, x, y): # Set min and max values and give it some padding x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1 y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1 h = 0.01 # Generate a grid of points with distance h between them xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # Predict the function value for the whole grid Z = model(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Plot the contour and training examples plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.ylabel('x2') plt.xlabel('x1') plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral) # 这次仍处理二分类问题，但是比之前的logistic回归更加复杂np.random.seed(1)m = 400 # 样本数量N = int(m/2) # 每一类的点的个数D = 2 # 纬度x = np.zeros((m, D))y = np.zeros((m, 1), dtype='uint8') # label 向量， 0 表示红色， 1 表示蓝色a = 4 for j in range(2): ix = range(N*j,N*(j+1)) t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius x[ix] = np.c_[r*np.sin(t), r*np.cos(t)] y[ix] = j plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral) &lt;matplotlib.collections.PathCollection at 0x120b352b0&gt; # 先尝试使用 logistic 回归解决这个问题x = torch.from_numpy(x).float()y = torch.from_numpy(y).float()w = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))optimizer = torch.optim.SGD([w, b], 1e-1)def logistic_regression(x): return torch.mm(x, w) + bcriterion = nn.BCEWithLogitsLoss() for e in range(100): out = logistic_regression(Variable(x)) loss = criterion(out, Variable(y)) optimizer.zero_grad() loss.backward() optimizer.step() if (e + 1) % 20 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 20, loss: 0.7704809308052063 epoch: 40, loss: 0.6776716113090515 epoch: 60, loss: 0.6732792854309082 epoch: 80, loss: 0.6731553673744202 epoch: 100, loss: 0.6731488108634949 def plot_logistic(x): x = Variable(torch.from_numpy(x).float()) out = torch.sigmoid(logistic_regression(x)) out = (out &gt; 0.5) * 1 return out.data.numpy() plot_decision_boundary(lambda x: plot_logistic(x), x.numpy(), y.numpy())plt.title('logistic regression') Text(0.5,1,&#39;logistic regression&#39;) 可以看到 logistic 并不能很好的区分开，因为 logistic 是线性分类器， 现在采用 神经网络模型 # 定义两层神经网络w1 = nn.Parameter(torch.randn(2, 4) * 0.01) # 隐藏神经元个数 2b1 = nn.Parameter(torch.zeros(4))w2 = nn.Parameter(torch.randn(4, 1) * 0.01)b2 = nn.Parameter(torch.zeros(1))# 定义模型def two_netword(x): x1 = torch.mm(x, w1) + b1 x1 = torch.tanh(x1) # 使用 PyTorch 自带的 tanh 激活函数 x2 = torch.mm(x1, w2) + b2 return x2optimizer = torch.optim.SGD([w1, w2, b1, b2], 1.)criterion = nn.BCEWithLogitsLoss() # 训练 10000 次for e in range(10000): out = two_netword(Variable(x)) loss = criterion(out, Variable(y)) optimizer.zero_grad() loss.backward() optimizer.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.2847880423069 epoch: 2000, loss: 0.27225854992866516 epoch: 3000, loss: 0.26519426703453064 epoch: 4000, loss: 0.2597326934337616 epoch: 5000, loss: 0.23362228274345398 epoch: 6000, loss: 0.2255781590938568 epoch: 7000, loss: 0.22193390130996704 epoch: 8000, loss: 0.21951931715011597 epoch: 9000, loss: 0.2176773101091385 epoch: 10000, loss: 0.21617043018341064 def plot_network(x): x = Variable(torch.from_numpy(x).float()) x1 = torch.mm(x, w1) + b1 x1 = torch.tanh(x1) x2 = torch.mm(x1, w2) + b2 out = torch.sigmoid(x2) out = (out &gt; 0.5) * 1 return out.data.numpy() plot_decision_boundary(lambda x: plot_network(x), x.numpy(), y.numpy())plt.title('2 layer network') Text(0.5,1,&#39;2 layer network&#39;) 可以看到， 神经网络能很好的分类这个复杂的数据， 和前面的logistic回归相比，神经网络因为有了激活函数的存在，成了一个非线性分类器，所以神经网络分类的边界更加复杂 自动构建一个完整的 PyTorch 模型手动构建模型对于比较小的模型是可行的，但是对于大模型，比如 100 层的神经网络，这个时候手动定义模型就会非常麻烦，接下来使用 PyTorch 提供的模块来构建模型 使用 Sequential 构建模型# Sequentialseq_net = nn.Sequential( nn.Linear(2, 4), # PyTorch 中的线性层， wx + b nn.Tanh(), nn.Linear(4, 1))# 序列模块可以通过索引访问每一层seq_net[0] # 第一层 Linear(in_features=2, out_features=4, bias=True) # 打印出第一层的权重w0 = seq_net[0].weightprint(w0) Parameter containing: tensor([[-0.5508, 0.2882], [ 0.0028, -0.5731], [-0.1803, 0.5680], [-0.5479, 0.0248]], requires_grad=True) # 通过 parameters 可以取得模型的参数param = seq_net.parameters()# 定义优化器optim = torch.optim.SGD(param, 1.) # 我们训练 10000 次for e in range(10000): out = seq_net(Variable(x)) loss = criterion(out, Variable(y)) optim.zero_grad() loss.backward() optim.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.28492191433906555 epoch: 2000, loss: 0.27243772149086 epoch: 3000, loss: 0.2653190791606903 epoch: 4000, loss: 0.25978708267211914 epoch: 5000, loss: 0.2339320182800293 epoch: 6000, loss: 0.22560137510299683 epoch: 7000, loss: 0.22195379436016083 epoch: 8000, loss: 0.21954353153705597 epoch: 9000, loss: 0.2177024632692337 epoch: 10000, loss: 0.2161943018436432 def plot_seq(x): out = torch.sigmoid(seq_net(Variable(torch.from_numpy(x).float()))).data.numpy() out = (out &gt; 0.5) * 1 return out plot_decision_boundary(lambda x: plot_seq(x), x.numpy(), y.numpy()) plt.title('sequential') Text(0.5,1,&#39;sequential&#39;) 模型保存的两种方式： 将模型结构和参数都保存在一起 只将参数保存下来 # 第一种保存方式，将模型和参数保存在一起torch.save(seq_net, 'save_seq_net.model') # 要保存的模型，保存路径 # 读取保存的模型seq_net1 = torch.load('save_seq_net.model')seq_net1 Sequential( (0): Linear(in_features=2, out_features=4, bias=True) (1): Tanh() (2): Linear(in_features=4, out_features=1, bias=True) ) # 第二种保存方式，只保存参数torch.save(seq_net.state_dict(), 'save_seq_net_params.pth') # 模型的读取, 需要先重新定义一次模型seq_net2 = nn.Sequential( nn.Linear(2, 4), nn.Tanh(), nn.Linear(4, 1))seq_net2.load_state_dict(torch.load('save_seq_net_params.pth'))seq_net2 Sequential( (0): Linear(in_features=2, out_features=4, bias=True) (1): Tanh() (2): Linear(in_features=4, out_features=1, bias=True) ) 第二种模型保存方式相比较第一种可移植性更强 使用 Module 构建模型# 使用 Module 的模版 ， 中文的地方是需要自己定义的，任何复杂的操作都可以在 forward 中定义class 网络名字(nn.Module): def __init__(self, 一些定义的参数): super(网络名字， self).__init__() self.layer1 = nn.Linear(num_input, num_hidden) self.layer2 = nn.Sequential(...) ... 定义需要的网络层 def forward(self, x): # 定义前向传播 x1 = self.layer(x) x2 = self.layer(x) x = x1 + x2 ... return x # 利用上述模版实现前面的神经网络class module_net(nn.Module): def __init__(self, num_input, num_hidden, num_output): super(module_net, self).__init__() self.layer1 = nn.Linear(num_input, num_hidden) self.layer2 = nn.Tanh() self.layer3 = nn.Linear(num_hidden, num_output) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x mo_net = module_net(2, 4, 1) # 访问模型中的某层可以直接通过名字# 第一层l1 = mo_net.layer1print(l1) Linear(in_features=2, out_features=4, bias=True) # 打印第一层的权重print(l1.weight) Parameter containing: tensor([[ 0.5183, 0.2359], [ 0.3867, 0.6806], [ 0.4755, -0.4971], [-0.4460, -0.6820]], requires_grad=True) # 定义优化器optim = torch.optim.SGD(mo_net.parameters(), 1.) # 训练 10000 次for e in range(10000): out = mo_net(Variable(x)) loss = criterion(out, Variable(y)) optim.zero_grad() loss.backward() optim.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.283423513174057 epoch: 2000, loss: 0.2715207040309906 epoch: 3000, loss: 0.26463547348976135 epoch: 4000, loss: 0.2599412798881531 epoch: 5000, loss: 0.25658532977104187 epoch: 6000, loss: 0.2540966272354126 epoch: 7000, loss: 0.25219157338142395 epoch: 8000, loss: 0.2506936192512512 epoch: 9000, loss: 0.2494892179965973 epoch: 10000, loss: 0.2485022097826004 可以看到得到了相同的结果，这种使用梯度下降来优化参数的方法，在深度学习中叫做反向传播算法 接下来，定义一个5层的神经网络 class module_net(nn.Module): def __init__(self, num_input, num_hidden1, num_hidden2, num_hidden3, num_hidden4, num_output): super(module_net, self).__init__() self.layer_activation = nn.Tanh() self.layer_input = nn.Linear(num_input, num_hidden1) self.layer_hidden1 = nn.Linear(num_hidden1, num_hidden2) self.layer_hidden2 = nn.Linear(num_hidden2, num_hidden3) self.layer_hidden3 = nn.Linear(num_hidden3, num_hidden4) self.layer_output = nn.Linear(num_hidden4, num_output) def forward(self, x): x = self.layer_input(x) x = self.layer_activation(x) x = self.layer_hidden1(x) x = self.layer_activation(x) x = self.layer_hidden2(x) x = self.layer_activation(x) x = self.layer_hidden3(x) x = self.layer_activation(x) x = self.layer_output(x) return xmo_net5 = module_net(2, 10, 10, 10, 10, 1)optim = torch.optim.SGD(mo_net5.parameters(), 0.1)# 训练 20000 次for e in range(20000): out = mo_net5(Variable(x)) loss = criterion(out, Variable(y)) optim.zero_grad() loss.backward() optim.step() if (e + 1) % 1000 == 0: print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e+1, loss.item())) epoch: 1000, loss: 0.2958005964756012 epoch: 2000, loss: 0.219310462474823 epoch: 3000, loss: 0.19828572869300842 epoch: 4000, loss: 0.18876363337039948 epoch: 5000, loss: 0.18029095232486725 epoch: 6000, loss: 0.16922634840011597 epoch: 7000, loss: 0.16025283932685852 epoch: 8000, loss: 0.1556335687637329 epoch: 9000, loss: 0.15196143090724945 epoch: 10000, loss: 0.14857107400894165 epoch: 11000, loss: 0.1452614665031433 epoch: 12000, loss: 0.13823415338993073 epoch: 13000, loss: 0.13576728105545044 epoch: 14000, loss: 0.13504637777805328 epoch: 15000, loss: 0.13344115018844604 epoch: 16000, loss: 0.13231371343135834 epoch: 17000, loss: 0.1298571079969406 epoch: 18000, loss: 0.1285153031349182 epoch: 19000, loss: 0.12435487657785416 epoch: 20000, loss: 0.1194448247551918 def plot_net5(x): out = torch.sigmoid(mo_net5(Variable(torch.from_numpy(x).float()))).data.numpy() out = (out &gt; 0.5) * 1 return out plot_decision_boundary(lambda x: plot_net5(x), x.numpy(), y.numpy()) plt.title('sequential') Text(0.5,1,&#39;sequential&#39;) 可以看出多层神经网络拟合效果会更好","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"深度学习基础知识","slug":"deeplearning-base","date":"2019-05-09T05:52:55.739Z","updated":"2020-08-05T09:45:24.737Z","comments":true,"path":"2019/05/09/deeplearning-base/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/09/deeplearning-base/","excerpt":"介绍深度学习中常用的一些概念","text":"介绍深度学习中常用的一些概念 梯度下降梯度下降 梯度下降法 梯度下降代码(PyTorch)梯度下降在线性模型中的应用import torchimport numpy as npfrom torch.autograd import Variable torch.manual_seed(2019) &lt;torch._C.Generator at 0x10c9b39f0&gt; # 读取 x 和 yx_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32) # 画出图像import matplotlib.pyplot as plt%matplotlib inlineplt.plot(x_train, y_train, 'bo') [&lt;matplotlib.lines.Line2D at 0x11ca8cfd0&gt;] # 转换成 Tensorx_train = torch.from_numpy(x_train)y_train = torch.from_numpy(y_train)# 定义参数 w 和 bw = Variable(torch.randn(1), requires_grad=True) # 随机初始化 wb = Variable(torch.zeros(1), requires_grad=True) # 使用 0 初始化 b # 构建线性回归模型x_train = Variable(x_train)y_train = Variable(y_train)def liner_model(x): return x * w + b y_ = liner_model(x_train) # 在更新参数之前，模型输出结果plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')plt.legend() &lt;matplotlib.legend.Legend at 0x11cc6beb8&gt; 计算误差函数： \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\hat{y}_{i}-y_{i}\\right)^{2}# 计算误差def get_loss(y_, y): return torch.mean((y_-y) ** 2)loss = get_loss(y_, y_train) # 查看 loss 的大小loss tensor(10.2335, grad_fn=&lt;MeanBackward1&gt;) 计算 w 和 b 的梯度，采用PyTorch的自动求导，不需要手动取计算梯度。w 和 b 的梯度分别是： \\frac{\\partial}{\\partial w}=\\frac{2}{n} \\sum_{i=1}^{n} x_{i}\\left(w x_{i}+b-y_{i}\\right)\\frac{\\partial}{\\partial b}=\\frac{2}{n} \\sum_{i=1}^{n}\\left(w x_{i}+b-y_{i}\\right)# 自动求导loss.backward() # 查看 w 和 b 的梯度print(w.grad)print(b.grad) tensor([-41.1289]) tensor([-6.0890]) # 更新一次参数w.data = w.data - 1e-2 * w.grad.datab.data = b.data - 1e-2 * b.grad.data # 更新一次参数后，模型输出结果y_ = liner_model(x_train)plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')plt.legend() &lt;matplotlib.legend.Legend at 0x11cbd84e0&gt; # 多次更新for e in range(20): y_ = liner_model(x_train) loss = get_loss(y_, y_train) w.grad.zero_() # 梯度归零 b.grad.zero_() # 梯度归零 loss.backward() w.data = w.data - 1e-2 * w.grad.data # 更新 w b.data = b.data - 1e-2 * b.grad.data # 更新 b print('epoch: &#123;&#125;, loss: &#123;&#125;'.format(e, loss.data.item())) epoch: 0, loss: 0.23505009710788727 epoch: 1, loss: 0.23023782670497894 epoch: 2, loss: 0.2298405021429062 epoch: 3, loss: 0.22952641546726227 epoch: 4, loss: 0.22921547293663025 epoch: 5, loss: 0.22890615463256836 epoch: 6, loss: 0.22859837114810944 epoch: 7, loss: 0.22829222679138184 epoch: 8, loss: 0.227987602353096 epoch: 9, loss: 0.2276846319437027 epoch: 10, loss: 0.2273831069469452 epoch: 11, loss: 0.22708319127559662 epoch: 12, loss: 0.22678478062152863 epoch: 13, loss: 0.2264879196882248 epoch: 14, loss: 0.2261926382780075 epoch: 15, loss: 0.22589880228042603 epoch: 16, loss: 0.22560644149780273 epoch: 17, loss: 0.225315660238266 epoch: 18, loss: 0.22502633929252625 epoch: 19, loss: 0.2247384935617447 # 参数更新20次之后的结果y_ = liner_model(x_train)plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')plt.legend() &lt;matplotlib.legend.Legend at 0x11cb5fc18&gt; 经过 20 次更新， 红色的预测结果已经比较好的拟合了蓝色的真实值 梯度下降在多项式模型中的应用# 定义一个多变量函数w_target = np.array([0.5, 3, 2.4]) # 定义参数b_target = np.array([0.9]) # 定义参数f_des = 'y = &#123;:.2f&#125; + &#123;:.2f&#125; * x + &#123;:.2f&#125; * x^2 + &#123;:.2f&#125; * x^3'.format( b_target[0], w_target[0], w_target[1], w_target[2]) # 打印出函数的样式print(f_des) y = 0.90 + 0.50 * x + 3.00 * x^2 + 2.40 * x^3 # 先画出这个函数的图像x_sample = np.arange(-3, 3, 0.1)y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample **2 + w_target[2] * x_sample ** 3plt.plot(x_sample, y_sample, label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x11c8255f8&gt; # 构建训练数据 x 和 y# x 是一个如下矩阵 [x, x^2, x^3]# y 是函数的结果 [y]x_train = np.stack([x_sample **i for i in range(1, 4)], axis=1)x_train = torch.from_numpy(x_train).float() # 转换成 float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # 转换为 列向量 # 定义要优化的参数 w 和 bw = Variable(torch.randn(3,1), requires_grad=True)b = Variable(torch.zeros(1), requires_grad=True)# 将 x 和 y 转换成 Variabelx_train = Variable(x_train)y_train = Variable(y_train)# 定义多元线性模型def multi_model(x): return torch.mm(x, w) + b # torch.mm 矩阵相乘 # 画出没有参数没有更新之前的模型与真实模型之间的对比y_pred = multi_model(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), color='r', label='fitting curve')plt.plot(x_train.data.numpy()[:, 0], y_sample, color='b', label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x11cf3f390&gt; loss = get_loss(y_pred, y_train)print(loss) tensor(142.2003, grad_fn=&lt;MeanBackward1&gt;) # 自动求导loss.backward() # 查看 w 和 b 的梯度print(w.grad)print(b.grad) tensor([[ -36.4598], [ -13.2381], [-236.6262]]) tensor([-3.9543]) # 更新一次参数w.data = w.data - 0.001 * w.grad.datab.data = b.data - 0.001 * b.grad.data # 画出更新一次参数之后模型y_pred = multi_model(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), color='r', label='fitting curve')plt.plot(x_train.data.numpy()[:, 0], y_sample, color='b', label='real curve')plt.legend() &lt;matplotlib.legend.Legend at 0x11ccc1b70&gt; # 更新 100 次for e in range(100): y_pred = multi_model(x_train) loss = get_loss(y_pred, y_train) w.grad.data.zero_() b.grad.data.zero_() loss.backward() # 更新参数 w.data = w.data - 0.001 * w.grad.data b.data = b.data - 0.001 * b.grad.data if (e + 1) % 20 == 0: print('epoch &#123;&#125;, Loss:&#123;:.5f&#125;'.format(e+1, loss.data.item())) epoch 20, Loss:2.84467 epoch 40, Loss:1.08338 epoch 60, Loss:0.61722 epoch 80, Loss:0.48439 epoch 100, Loss:0.43824 # 更新 100 次之后的模型结果y_pred = multi_model(x_train)plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r') plt.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b') plt.legend() &lt;matplotlib.legend.Legend at 0x11cf335c0&gt; # 输出此时的参数 w 和 bprint(w.data)print(b.data) tensor([[1.1381], [3.1028], [2.3013]]) tensor([0.1931]) 可以看到模型迭代100次之后已经非常接近原始数据了 激活函数定义类比人脑神经元，解释如下激活函数在神经网络中非常重要，只有通过激活函数才会进入下一层继续传播，如果不使用激活函数，无论多少层神经网络，最后都会变成单层神经网络，所以每一层必须使用激活函数 常见的激活函数sigmoid 函数f(z)=\\frac{1}{1+\\exp (-z)}导函数: f^{\\prime}(z)=f(z)(1-f(z))图像： tanh 函数f(z)=\\tanh (z)=\\frac{\\mathrm{e}^{z}-\\mathrm{e}^{-z}}{\\mathrm{e}^{z}+\\mathrm{e}^{-z}}导函数： f^{\\prime}(z)=1-(f(z))^{2}图像: ReLU 函数f(z)=\\max (0, z)导函数: f^{\\prime}(z)=\\left\\{\\begin{array}{l}{1, z>0} \\\\ {0, z \\leqslant 0}\\end{array}\\right.图像: 激活函数的选择 现在的神经网络中，90%都使用ReLU激活函数，该激活函数能够加快梯度下降的收敛速度，对比其他激活函数计算更加简单。 Sigmoid和Tanh会导致梯度消失，当z很大时，sigmoid函数的导函数会=趋近于0，造成梯度消失，Tanh相当于Sigmoid函数的平移，原理类似。 RuLU的优势，以及局限性： 优点：计算简单，避免梯度消失，单侧抑制提供了网络的稀疏性表达能力 局限性：在训练过程中会导致神经元死亡问题，这是由于函数负梯度在经过ReLU单元时被置为0，且之后也不被任何数据激活，即改神经元梯度永远为0，不对任何数据产生效应。在实习训练过程中，如果学习率设置较大，会导致一定比例的神经元不可逆死亡。导致参数梯度无法更新，整个训练过程失败。 优化：采用Leaky ReLU 表达式 f(z)=\\left\\{\\begin{array}{cl}{z,} & {z>0} \\\\ {a z,} & {z \\leqslant 0}\\end{array}\\right. 反向传播算法定义反向传播本质上是链式求导法则的一个应用。反向传播算法是一个优雅的局部过程，每次求导只是对当前的运算求导，求解每层网络的参数都是通过链式法则将前面的结果求出不断迭代到这一层，所以说是一个传播过程。 求解过程 求解下图的反向传播过程表达式： \\begin{array}{l}{q=x+y=-2+5=3} \\\\ {f=q z=3(-4)=-12}\\end{array}求解中间过程： \\begin{array}{ll}{\\frac{\\partial f}{\\partial q}=z=-4} & {\\frac{\\partial f}{\\partial z}=q=3} \\\\ {\\frac{\\partial q}{\\partial x}=1} & {\\frac{\\partial q}{\\partial y}=1}\\end{array}求解梯度： \\begin{aligned} \\frac{\\partial f}{\\partial x} &=\\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}=z \\times 1=-4 \\\\ \\frac{\\partial f}{\\partial y} &=\\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y}=z \\times 1=-4 \\end{aligned} Sigmoid 函数求解反向传播表达式： f(w, x)=\\frac{1}{1+e^{-\\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\\right)}}求解： \\frac{\\partial f}{\\partial w_{0}}, \\frac{\\partial f}{\\partial w_{1}}, \\frac{\\partial f}{\\partial w_{2}}构建计算图： \\begin{array}{l}{f(x)=\\frac{1}{x}} \\\\ {f_{c}(x)=1+x} \\\\ {f_{e}(x)=e^{x}} \\\\ {f_{w}(x)=-\\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\\right)}\\end{array}求解过程：上图中绿色数字表示数值，红色数字表示梯度。从后向前计算各个参数的梯度。 最后面梯度为 1 经过 $\\frac{1}{x}$ 这个函数，梯度是 $-\\frac{1}{x^{2}}$ ， 所以往前传播的梯度是 $1 \\times-\\frac{1}{1.37^{2}}=-0.53$。 经过 +1 操作，梯度不变 经过 $e^{x}$, 梯度变为 $-0.53 \\times e^{-1}=-0.2$。 经过 *-1， 梯度不变。 不断向后传播就能够求得每一个参数的梯度。 优化算法定义基于反向传播算法计算出梯度，接下来就会利用各种优化算法对参数进行优化。 常用的优化算法SGD 随机梯度下降定义\\theta_{i+1}=\\theta_{i}-\\eta \\nabla L(\\theta)$\\eta$ 代表学习率，$\\nabla L(\\theta)$ 代表梯度 实现# 自定义实现def sgd_update(parameters, lr): for param in parameters: param.data = param.data - lr * param.grad.data # PyTorch 实现optimzier = torch.optim.SGD(net.parameters(), 1e-2) Momentum 基于动量的梯度下降定义\\begin{array}{c}{v_{i}=\\gamma v_{i-1}+\\eta \\nabla L(\\theta)} \\\\ {\\theta_{i}=\\theta_{i-1}-v_{i}}\\end{array}$v_{i}$ 是当前速度，$\\gamma$ 是动量参数，是一个小于 1 的正数， $\\eta$ 是学习率相当于每次在进行参数更新的时候，都会将之前的速度考虑进来，每个参数在各方向上的移动幅度不仅取决于当前的梯度，还取决于过去各个梯度在各个方向上是否一致，如果一个梯度一致沿着当前方向进行更新，那么每次更新的幅度就越来越大，如果一个梯度在一个方向上不断变化，那么其更新幅度就会被衰减，这样我们就可以使用一个较大的学习率，使得收敛更快，同时梯度比较大的方向就会因为动量的关系每次更新的幅度减少。 实现# 自定义实现def sgd_momentum(parameters, vs, lr, gamma): for param, v in zip(parameters, vs): v[:] = gamma * v + lr * param.grad.data param.data = param.data - v # PyTorch 实现# 加动量optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) Adagrad 自适应梯度下降定义学习率在更新过程中不断改变。学习率为： \\frac{\\eta}{\\sqrt{s+\\epsilon}}每次使用一个 batch size的数据进行参数更新的时候，我们需要计算所有参数的梯度，对于每个参数，初始化一个变量 s 为 0，然后每次将该参数的梯度平方和累加到这个变量 s 上，然后再更新参数的时候，学习率变为上式的样子。$\\epsilon$ 是为了数值稳定性而加上的，因为有可能 s 为 0，那么 0 出现在分母就会出现无穷大的情况，通常取 $10^{-10}$ ,这样不同的参数由于梯度不同，它们对应的 s 大小也就不同，得到的学习率也就不同，实现了自适应的学习率。Adagrad 的 核心想法是：如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡。而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够快速的更新。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。 实现# 自定义实现def sgd_adagrad(parameters, sqrs, lr): ‘’‘ sqrs: 梯度平方和 ’‘’ eps = 1e-10 for param, sqr in zip(parameters, sqrs): sqr[:] = sqr + param.grad.data ** 2 div = lr / torch.sqrt(sqr + eps) * param.grad.data param.data = param.data - div # PyTorch 实现optimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2) RMSProp RMSprop方法定义RMSProp的提出，为了解决Adagrad训练后期，学习率过小的问的。RMSProp 使用一个指数加权移动平均来计算 s，如下式： s_{i}=\\alpha s_{i-1}+(1-\\alpha) g^{2}学习率更新公式： \\frac{\\eta}{\\sqrt{s+\\epsilon}}$g$ 表示当前求出的参数梯度，$\\alpha$ 是一个移动平均的系数，这个系数使用 RMSProp 更新到后期累加的梯度平方较小，从而保证 s 不会太大，也就是使得模型后期依然能找到比较优的结果。 实现# 自定义实现def rmsprop(parameters, sqrs, lr, alpha): eps = 1e-10 for param, sqr in zip(parameters, sqrs): sqr[:] = alpha * sqr + (1 - alpha) * param.grad.data ** 2 div = lr / torch.sqrt(sqr + eps) * param.grad.data param.data = param.data - div # PyTorch 实现optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, alpha=0.9) Adadelta Adaedlta方法定义也是为了解决Adagrad中学习率不断减小的问题。先使用移动平均来计算 s s=\\rho s+(1-\\rho) g^{2}$\\rho$ 移动平均系数，$g$ 参数的梯度，计算需要更新的参数的变化量： g^{\\prime}=\\frac{\\sqrt{\\Delta \\theta+\\epsilon}}{\\sqrt{s+\\epsilon}} g$\\Delta \\theta$ 初始为 0 张量，每一步做如下指数加权移动平均更新： \\Delta \\theta=\\rho \\Delta \\theta+(1-\\rho) g^{\\prime 2}最后参数更新为： \\theta=\\theta-g^{\\prime}实现# 自定义实现def adadelta(parameters, sqrs, deltas, rho): eps = 1e-6 for param, sqr, delta in zip(parameters, sqrs, deltas): sqr[:] = rho * sqr + (1 - rho) * param.grad.data ** 2 cur_delta = torch.sqrt(delta + eps) / torch.sqrt(sqr + eps) * param.grad.data delta[:] = rho * delta + (1 - rho) * cur_delta ** 2 param.data = param.data - cur_delta # PyTorch 实现optimizer = torch.optim.Adadelta(net.parameters(), rho=0.9) Adam Adam方法定义Adam 是一个结合了动量法和RMSProp的优化算法，结合了两者的优点。使用一个动量变量 v 和一个 RMSProp 中的梯度元素平方的移动指数加权平均 s，优先将他们全部初始化为 0，然后再每次迭代中，计算他们的移动加权平均进行更新。 \\begin{aligned} v &=\\beta_{1} v+\\left(1-\\beta_{1}\\right) g \\\\ s &=\\beta_{2} s+\\left(1-\\beta_{2}\\right) g^{2} \\end{aligned}为了减轻 v 和 s 被初始化为 0 的初期对计算指数加权移动平均的影响，每次 v 和 s 都做下面的修正。 \\begin{aligned} \\hat{v} &=\\frac{v}{1-\\beta_{1}^{t}} \\\\ \\hat{s} &=\\frac{s}{1-\\beta_{2}^{t}} \\end{aligned}$t$迭代次数，当$0\\leq\\beta{1},\\beta{2}\\leq1$时，迭代到后期$t$比较大，那么$\\beta{1}^{t}$和$\\beta{2}^{t}$就几乎为 0，不会对 v 和 s 有任何影响了，算法作者建议$\\beta{1}=0.9,\\beta{2}=0.999$。使用修正之后的$\\hat{v}$和$\\hat{s}$进行学习率的重新计算。 g^{\\prime}=\\frac{\\eta\\hat{v}}{\\sqrt{\\hat{s}+\\epsilon}}$\\eta$是学习率， $\\epsilon$是为了数值稳定而添加的常数。更新参数： \\theta_{i}=\\theta_{i-1}-g^{\\prime}实现# 自定义实现def adam(parameters, vs, sqrs, lr, t, beta1=0.9, beta2=0.999): eps = 1e-8 for param, v, sqr in zip(parameters, vs, sqrs): v[:] = beta1 * v + (1 - beta1) * param.grad.data sqr[:] = beta2 * sqr + (1 - beta2) * param.grad.data ** 2 v_hat = v / (1 - beta1 ** t) s_hat = sqr / (1 - beta2 ** t) param.data = param.data - lr * v_hat / torch.sqrt(s_hat + eps) # PyTorch 实现optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"基础概念","slug":"基础概念","permalink":"http://lucas0625.github.io/blog/tags/基础概念/"}]},{"title":"算法面试编程试题集锦","slug":"interviewCode","date":"2019-05-05T08:34:34.929Z","updated":"2020-08-05T09:45:24.721Z","comments":true,"path":"2019/05/05/interviewCode/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/05/interviewCode/","excerpt":"[toc] 面试准备(编程篇)","text":"[toc] 面试准备(编程篇) 位运算leetcode-78.Subsetsdesc: Given a set of distinct integers, nums, return all possible subsets (the power set)Note: The solution set must not contain duplicate subsets.返回列表中的所有子集 thought: N 个数的集合有 2*N 个子集，将枚举子集转换成枚举组合数，对应位置为1，则包含这个数。时间复杂度O(n 2^n), 空间复杂度O(1)。 solution class Solution: # 时间复杂度O(n * 2^n), 空间复杂度O(1) def subsets(self, nums: List[int]) -&gt; List[List[int]]: N = len(nums) res = [] # N 个数, 有 2**N 种组合, 枚举所有子集转化为枚举所有 组合数 for i in range(pow(2, N)): tmp = [] for j in range(N): # 检测 i 的第 j 位是否为 1 # 1 包含 0 不包含 if (i &gt;&gt; j) % 2 == 1: tmp.append(nums[j]) res.append(tmp) return res leetcode-762.Prime Number of Set Bits in Binary Representationdesc:Given two integers L and R, find the count of numbers in the range [L, R] (inclusive) having a prime number of set bits in their binary representation. (Recall that the number of set bits an integer has is the number of 1s present when written in binary. For example, 21 written in binary is 10101 which has 3 set bits. Also, 1 is not a prime.)给定的[L,R]中，转换成二进制后1的个数是质数的个数。 thought:暴力解法，先求出给定数的二进制中1的个数，在判断是否为质数 solution:class Solution &#123;public: int countPrimeSetBits(int L, int R) &#123; int ans=0; for(int n=L; n&lt;=R; ++n)&#123; if(isPrime(bits(n)))&#123; ans++; &#125; &#125; return ans; &#125;private: int bits(int n)&#123; int s=0; while(n)&#123; s += n&amp;1; n &gt;&gt;= 1; &#125; return s; &#125; bool isPrime(int n)&#123; if(n&lt;=1) return false; if(n==2) return true; for(int i=2; i&lt;=sqrt(n); ++i)&#123; if(n%i == 0) return false; &#125; return true; &#125;&#125;; class Solution: def countPrimeSetBits(self, L: int, R: int) -&gt; int: res = 0 for i in range(L, R+1): if self.isPrime(self.bits(i)): res += 1 return res def bits(self, n): s = 0 while n: s += n &amp; 1 n &gt;&gt;=1 return s def isPrime(self, n): if n &lt;= 1: return False if n == 2: return True import math for i in range(2, int(math.sqrt(n))+1): if n % i == 0: return False return True leetcode-136. Single Numberdesc：Given a non-empty array of integers, every element appears twice except for one. Find that single one.Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?一个列表中只有一个元素出现一次，其余都出现两次，找到出现一次的元素。 thought：解法一：使用hashmap计数，返回出现一次的元素，但是用到了额外空间，不符合题目要求。解法二：采用异或运算。利用异或运算的下列性质： n ^ n = 0 0 ^ n = n solution：class Solution: def singleNumber(self, nums: List[int]) -&gt; int: res = 0 for i in nums: res ^= i return res 计算pow(x, y)desc: 实现一个求幂次的运算。 thought:如果直接用循环连乘的话，时间复杂度为O(N)。解法一：采用位运算，当前位为1的乘到结果中，当前位为0的不乘到结果中，时间复杂度为O(logN) solution:def my_pow(x, y): # x ** y ans = 1 while y: if y&amp;1: ans = ans * x x = x * x y &gt;&gt;= 1 return ans 二分搜索leetcode-69. Sqrt(x)desc: Implement int sqrt(int x).Compute and return the square root of x, where x is guaranteed to be a non-negative integer.Since the return type is an integer, the decimal digits are truncated and only the integer part of the result is returned.实现一个开方函数thought:解法一: 给定 y ,求 $y=x^2$ x 的值，可以在[0,y]之间二分搜索，通过设定误差值，来求出满足条件的 x 。解法二: 通过牛顿法逼近。 x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{f’\\left(x_{n}\\right)}。 solution:# 解法一class Solution: def mySqrt(self, x: int) -&gt; int: if x == 0 or x == 1: return x l, r = 0, x while l &lt; r: mid = (l + r) / 2 x_hat = mid * mid if abs(x - x_hat) &lt; 0.01: return int(mid) elif x_hat &lt; x: l = mid else: r = mid 分治数组leetcode-121. Best Time to Buy and Sell Stockdesc:Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.Note that you cannot sell a stock before you buy one.只能进行一次买卖，求出最大的利润。 thought: 只需要遍历一次数组，一个变量记录遍历到该元素之前的最小值，另一个变量记录当前值与最小值的差值作为利润，保留最大的h利润。 solution:def maxProfit(prices): ''' 参数说明: prices_min : 扫过的价格中的最小值 ''' if not prices: return 0 prices_min = float('inf') res = 0 for item in prices: prices_min = min(item, prices_min) res = max(res, item - prices_min) return res 生成新数组desc: 给定一个数组，按要求生成新数组 生成的新数组每一位为原数组不包括此位的和。 不能使用减法。 时间复杂度O(N), 空间复杂度O(1) thought: n从1开始两次循环，前向一次，反向一次。 前向 b[i] = b[i-1] + a[i-1] 反向 b[i] = b[i] + a[i+1:n] leetcode-54.螺旋数组的打印desc:Given a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order. thought: 一共有四种可能前进的方向，发生以下情况改变方向。 数组下标越界。 下一个坐标的值已经被打印过。 solution:class Solution: def __init__(self): ''' self.mark 方向标识位 self.row 行坐标 self.col 列坐标 self.res 输出的结果 self.all_i 已经遍历过的坐标集合 ''' self.mark = 0 self.row = 0 self.col = 0 self.res = list() self.all_i = set() self.all_i.add((0,0)) # 定义四种可能的方向 def derection(self, mark): around = [ [self.row, self.col + 1], [self.row + 1, self.col], [self.row, self.col - 1], [self.row - 1, self.col], ] return around[mark % 4] def next(self): # 元素全部遍历完成，退出 if len(self.all_i) == self.row_len * self.col_len: return # 下一个坐标 i = self.derection(self.mark) # 判断下标是否越界 if -1 not in i and self.row_len != i[0] and self.col_len != i[1]: # 判断当前坐标是否遍历过 if tuple(i) not in self.all_i: self.row, self.col = i self.all_i.add(tuple(i)) return None # 如果下标越界或者当前坐标被遍历过，那么改变方向,继续遍历 self.mark += 1 return self.next() def spiralOrder(self, matrix: List[List[int]]) -&gt; List[int]: if not matrix: return [] self.row_len = len(matrix) self.col_len = len(matrix[0]) for _ in range(self.row_len * self.col_len): self.res.append(matrix[self.row][self.col]) self.next() return self.res leetcode-59.生成螺旋数组desc:Given a positive integer n, generate a square matrix filled with elements from 1 to n2 in spiral order. thought: 一共有四种可能前进的方向，发生以下情况改变方向。 数组下标越界。 下一个坐标的值已经存在。 solution:class Solution: def __init__(self): ''' self.row 行坐标 self.col 列坐标 self.mark 方向改变标识位 ''' self.row = 0 self.col = 0 self.mark = 0 def derection(self, mark): around = [ [self.row, self.col + 1], [self.row + 1, self.col], [self.row, self.col - 1], [self.row - 1, self.col], ] return around[mark % 4] def next(self): i = self.derection(self.mark) # 是否超出边界 if -1 not in i and self.row_max not in i: # 下一个坐标的值是否为空 if not self.matrix[i[0]][i[1]]: self.row, self.col = i return # 超出边界或者下一个坐标值不为空, 改变位置 self.mark += 1 return self.next() def generateMatrix(self, n: int) -&gt; List[List[int]]: if not n: return [] self.matrix = [[None for _ in range(n)] for _ in range(n)] self.row_max = n for i in range(1, n**2 + 1): self.matrix[self.row][self.col] = i # 退出条件 if i == self.row_max ** 2: break self.next() return self.matrix 字符串leetcode-14.Longest Common Prefixdesc: Write a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string “”查找字符串数组中的最长公共前缀 thought：解法一：利用两个指针进行遍历，一个指针遍历每个字符串，另一个指针遍历字符数组。类似于一个长度不相等的二维矩阵，固定每一列，遍历每一行，当发现某一列中对应的某行与其他行元素不相等，终止遍历，返回结果。 解法二：先将字符串数组排序，最长公共前缀肯定存在于首尾字符串中，求出首尾字符串的公共前缀即可。 solution：# 解法一class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if not strs: return '' min_str = min(strs, key=lambda x: len(x)) min_str_len = len(min_str) res = '' flag = True for i in range(min_str_len): for str in strs: if str[i] == min_str[i]: continue else: flag = False break if flag: res += min_str[i] else: break return res # 解法二class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if not strs: return '' strs_sort = sorted(strs) head = strs_sort[0] tail = strs_sort[-1] res = '' str_len = min(len(head), len(tail)) for i in range(str_len): if head[i] == tail[i]: res += head[i] else: break return res leetcode-8.String to Integer (atoi)desc：Implement atoi which converts a string to an integer.Note: Only the space character ‘ ‘ is considered as whitespace character. Assume we are dealing with an environment which could only store integers within the 32-bit signed integer range: [−231, 231 − 1]. If the numerical value is out of the range of representable values, INT_MAX (231 − 1) or INT_MIN (−231) is returned.将字符串转换为整数 thought：主要考虑一些边界条件，这里利用正则来匹配字符串中的数字。 solution：class Solution: def myAtoi(self, str: str) -&gt; int: import re pattern = re.compile('[+-]?\\d+') str = str.strip() res = re.match(pattern, str) if res: res = int(res.group()) if res &gt; pow(2, 31) - 1: return pow(2, 31) - 1 elif res &lt; pow(-2, 31): return pow(-2, 31) else: return res else: return 0 leetcode-227. Basic Calculator IIdesc:Implement a basic calculator to evaluate a simple expression string.The expression string contains only non-negative integers, +, -, , / operators and empty spaces . The integer division should truncate toward zero.实现只包含 + - / 空格的字符串的数值大小，类似于自带函数 eval 的功能。 thought: 采用栈数据结构。操作符为 + ，操作符后一个元素入栈操作符为 - ，操作符后一个元素相反数入栈操作符为 * ，取出栈顶元素，与操作符后一个元素做运算后入栈操作符为 / ，取出栈顶元素，与操作符后一个元素做运算后入栈最后将栈内元素求和。 solution:class Solution: def calculate(self, s: str) -&gt; int: ''' 参数说明： pro_op:记录前一个操作符，为了保证逻辑连续性，人为在第一个数字前面引入操作符 + num: 记录前一个数字 ''' # 将每一个元素顺序入栈 stack = [] pre_op = '+' num = 0 # 最后加 + 保证最后一个数字参与了运算，也可以是任一个字符。 s = s.replace(' ', '') + '+' for c in s: if c.isdigit(): # 考虑连续数字出现的情况 num = num * 10 + int(c) continue if pre_op == '+': stack.append(int(num)) if pre_op == '-': stack.append(int(-num)) if pre_op == '*': val = stack.pop() stack.append(int(val * num)) if pre_op == '/': val = stack.pop() stack.append(int(val / num)) num = 0 pre_op = c return sum(stack) 不重复字符串的所有排列组合desc: 例如输入 abc 输出 abc, acb, bac, bca, cba, cabthought: 采用递归，先固定第一个字母，对后续字符串进行全排列，然后交换字母。solution:def permutation(s, start): s = list(s) if not s or start &lt; 0: return # 完成排列后 输出当前排列的字符串 if start == len(s) - 1: print(''.join(s)) else: i = start while i &lt; len(s): # 交换 i 与 start 所在位置的字符 s[start], s[i] = s[i], s[start] permutation(s, start+1) # 还原 i 与 start 所在位置字符 s[start], s[i] = s[i], s[start] i += 1 含有重复字符串的所有排列组合desc: 输入 abb 输出 abb, bab, bbathought:解法一：求出所有排列组合之后去重。解法二：在交换元素的时候，判断交换的两个元素是否相等，相等则不交换。solution：```## leetcode-46. Permutations**desc**: Given a collection of distinct integers, return all possible permutations.![](http://pangfu.oss-cn-beijing.aliyuncs.com/markdown/2019-07-10-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-07-10%20%E4%B8%8B%E5%8D%884.01.52.png)给定一个不重复元素的列表，返回全排列列表**thought**: 递归实现**solution**：```pythonclass Solution: def __init__(self): self.res = list() def _permute(self, nums, start): if not nums or start &lt; 0: return if start == len(nums) - 1: from copy import deepcopy tmp = deepcopy(nums) self.res.append(tmp) else: i = start while i &lt; len(nums): nums[start], nums[i] = nums[i], nums[start] self._permute(nums, start+1) nums[start], nums[i] = nums[i], nums[start] i += 1 def permute(self, nums: List[int]) -&gt; List[List[int]]: self._permute(nums, 0) return self.res 求两个字符串的最长公共子串desc:Given two strings, find the longest common substring.Return the length of it.求两个字符串的最长公共子串，注意子串是连续的。 thought:动态规划方法。solution:class Solution: \"\"\" @param A: A string @param B: A string @return: the length of the longest common substring. \"\"\" def longestCommonSubstring(self, A, B): # write your code here if not A or not B: return 0 len_A = len(A) len_B = len(B) sb = '' maxs = 0 maxI = 0 M = [([None] * (len_B + 1)) for i in range(len_A + 1)] i = 0 while i &lt; len_A + 1: M[i][0] = 0 i += 1 j = 0 while j &lt; len_B + 1: M[0][j] = 0 j += 1 i = 1 while i &lt; len_A + 1: j = 1 while j &lt; len_B + 1: if list(A)[i-1] == list(B)[j-1]: M[i][j] = M[i-1][j-1] + 1 if M[i][j] &gt; maxs: maxs = M[i][j] maxI = i else: M[i][j] = 0 j += 1 i += 1 i = maxI - maxs while i &lt; maxI: sb = sb + list(A)[i] i += 1 return len(sb) leetcode-72. 编辑距离desc: 给定两个字符串s1和s2，计算出将s1转化为s2所需要使用的最小操作数。可以对一个字符串执行如下操作： 插入一个字符。 删除一个字符。 替换一个字符。 thought：if s1[i] == s2[j]: 啥都不做(skip) i, j 同时向前移动else: 三选一 插入（insert） 删除（delete） 替换（replace） solution:# 递归解法class Solution: def minDistance(self, word1: str, word2: str) -&gt; int: def dp(i, j) -&gt; int: # 返回 s1[:i] 和 s2[:j] 的最小编辑距离 if i == -1: return j + 1 if j == -1: return i + 1 if word1[i] == word2[j]: return dp(i - 1, j - 1) else: return min( dp(i, j - 1) + 1, # 插入 dp(i - 1, j) + 1, # 删除 dp(i - 1, j - 1) + 1, # 替换 ) return dp(len(word1) - 1, len(word2) - 1) # 动态规划优化 几何链表leetcode-2. Add Two Numbersdesc:You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself.将以链表形式表示的两个数加起来 thought: 需要考虑进位，然后依次将两个链表对应的值和进位加起来。solution:class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: ''' 变量解释: left : 表示进位 sm : 表示当前位置求和的结果 ''' left = 0 dummy = cur = ListNode(-1) while l1 or l2 or left: # l and l.val or 0 # l为真，返回l.val # l为假，返回 0 left, sm = divmod(sum(l and l.val or 0 for l in (l1, l2)) + left, 10) # cur.next 中的 cur 代表未赋新值之前的cur cur.next = cur = ListNode(sm) # l = l and l.next # l为真，返回l.next # l为假，返回l l1 = l1 and l1.next l2 = l2 and l2.next return dummy.next leetcode-206. Reverse Linked Listdesc: Reverse a singly linked list反转一个单链表 thought: 采用一个新的节点，每次将当前节点添加至该节点之后 solution：# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head): \"\"\" :type head: ListNode :rtype: ListNode \"\"\" prev = None current = head while current: current.next, prev, current = prev, current, current.next return prev leetcode-141. Linked List Cycledesc: Given a linked list, determine if it has a cycle in it.To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list.判断一个单链表是否有环 thought:解法一：遍历链表，并记录下链表当前的值，判断之后的链表是否在已记录下的链表的集合中。解法二：采用快慢指针，看两个指针是否相等 solution:# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 解法一 def hasCycle(self, head): \"\"\" :type head: ListNode :rtype: bool \"\"\" node_set = set() while head and head.next: if head in node_set: return True node_set.add(head) head = head.next return False #解法二 def hasCycle(self, head): \"\"\" :type head: ListNode :rtype: bool \"\"\" slow, fast = head, head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False 贪心哈希表判断两个字符串是否是换位字符串desc: Given two strings s and t , write a function to determine if t is an anagram of s.给定两个字符串，判断是否是字符相同但是位置不同 thought: 暴力法可以将两个字符串排序，然后比较大小采用散列表可以先统计每个字符串的各字符的个数，然后比较。 solution # 解法一 时间复杂度Nlog(N)calss Solution: def isAnagram(self, s: str, t: str) -&gt; bool: return sorted(s) == sorted(t) # 解法二 时间复杂度O(N)class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: dic1, dic2 = &#123;&#125;, &#123;&#125; for item in s: dic1[item] = dic1.get(item, 0) + 1 for item in t: dic2[item] = dic2.get(item, 0) + 1 return dic1 == dic2 leetcode-1.Two Sumdesc: Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.找到列表中的两个数a， b 使 a+b=target， 返回 a，b 的下标 thought 采用哈希表的思想，枚举列表，判断 target-a 是否在除去此枚举元素的哈希表中 solution class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: hash_map = dict() for i, item in enumerate(nums): if target - item in hash_map: return [hash_map[target - item], i] hash_map[item] = i leetcode-15.3Sumdesc: Given an array nums of n integers, are there elements a, b, c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero.The solution set must not contain duplicate triplets.返回三数之和为0的所有子列表 thought: 一种方法是蛮力法，嵌套三层循环。第二种方法是，哈希函数的思想，嵌套两次循环，外层循环枚举 a ，内层循环判断 -b-c 是否在除枚举元素之外的散列表中。第三种方法是先排序，然后嵌套两次循环，外层循环枚举 a， 内层循环使用首尾指针进行移位判断。 solution: # 解法一 散列表 时间复杂度O(N^2), 空间复杂度O(N)class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: if len(nums) &lt; 3: return [] nums.sort() res = set() for i, v in enumerate(nums[:-2]): if i&gt;= 1 and v == nums[i-1]: continue d = &#123;&#125; for x in nums[i+1:]: if x not in d: d[-v-x] = 1 else: res.add((v, -v-x, x)) return [list(item) for item in res] # 解法二 首尾指针 时间复杂度O(N^2), 空间复杂度O(1)class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: res = [] nums.sort() for i in range(len(nums)-2): if i &gt; 0 and nums[i] == nums[i-1]: continue l, r = i+1, len(nums)-1 while l &lt; r: s = nums[i] + nums[l] + nums[r] if s &lt; 0 :l += 1 elif s &gt; 0 : r -= 1 else: res.append([nums[i], nums[l], nums[r]]) while l &lt; r and nums[l] == nums[l+1]: l += 1 while l&lt; r and nums[r] == nums[r-1]: r -= 1 l += 1 r -= 1 return res leetcode-3.Longest Substring Without Repeating Charactersdesc: Given a string, find the length of the longest substring without repeating characters.求最大无重复子串的长度 thought: 采用滑动窗口，滑动窗口从最大依次变小，利用hash表，判断窗口内的元素数量是否等于hash之后的元素数量，相等即为最大子串的长度 sulution: class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: if len(s) == 0: return 0 s_len = len(s) # 窗口最大值 max_slide = len(set(s)) # 窗口依次减小 while max_slide: if max_slide == 1: return 1 i =0 # 从字符串开始位置滑动窗口 while i + max_slide &lt;= s_len: sub_s = s[i:i+max_slide] sub_s_set = set(sub_s) if len(sub_s) == len(sub_s_set): return len(sub_s) i += 1 max_slide -= 1 图搜索leetcode-22. Generate Parenthesesdesc: Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.给定n对括号，生成所有可能的括号对组合 thought:相当于在大小为 2n 的格子中填入相应的括号，加入括号的时候进行剪枝操作。 如果右括号在前，则不符合 左括号 == 右括号 == n solution:class Solution: def generateParenthesis(self, n: int) -&gt; List[str]: self.list = [] self._gen(0, 0, n, '') return self.list def _gen(self, left, right, n, result): ''' left: 左括号已用次数 right: 右括号已用次数 n: 输入的括号对 result: 此时填充后的结果 ''' if left == n and right == n: self.list.append(result) return if left &lt; n: self._gen(left+1, right, n, result+'(') if right &lt; left and right &lt; n: self._gen(left, right+1, n, result+')') leetcode-51. N-Queensdesc:The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other.Given an integer n, return all distinct solutions to the n-queens puzzle. Each solution contains a distinct board configuration of the n-queens’ placement, where ‘Q’ and ‘.’ both indicate a queen and an empty space respectively.N皇后问题，皇后可以攻击所在的行，所在的列，以及撇和捺。将皇后放在安全的地方 thought: 采用DFS，一层一层的遍历，在每一层中遍历每一列，如果不在皇后的攻击范围内，则将皇后刚下，开始遍历下一层 solution:class Solution: def solveNQueens(self, n: int) -&gt; List[List[str]]: if n &lt; 1: return [] self.result = [] self.cols = set() self.pie = set() self.na = set() self.DFS(n, 0, []) return self._generate_result(n) def DFS(self, n, row, cur_state): if row &gt;= n: self.result.append(cur_state) return for col in range(n): if col in self.cols or row+col in self.pie or row-col in self.na: continue self.cols.add(col) self.pie.add(row+col) self.na.add(row-col) self.DFS(n, row+1, cur_state+[col]) self.cols.remove(col) self.pie.remove(row+col) self.na.remove(row-col) def _generate_result(self, n): board = [] for res in self.result: for i in res: board.append('.' * i + 'Q' + '.' * (n-i-1)) return [board[i:i+n] for i in range(0, len(board), n)] 树leetcode-98.Validate Binary Search Treedesc: Given a binary tree, determine if it is a valid binary search tree (BST).Assume a BST is defined as follows:The left subtree of a node contains only nodes with keys less than the node’s key.The right subtree of a node contains only nodes with keys greater than the node’s key.Both the left and right subtrees must also be binary search trees.验证给定的二叉树是否为二叉搜索树 thought:1. 将给定的二叉树中序遍历，如果遍历之后是升序的，则是二叉搜索树。时间复杂度O(n)2.递归，查找左子树最大值max， 查找右子树最小值min，每次递归都保证 max &lt; root &lt; min，则是二叉搜索树。时间复杂度O(n) solution: # 第一种方法# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isValidBST(self, root: TreeNode) -&gt; bool: inorder = self.inorder(root) return inorder == list(sorted(set(inorder))) def inorder(self, root): if root is None: return [] return self.inorder(root.left) + [root.val] + self.inorder(root.right) # 第二种方法 二叉树遍历leetcode-144. Binary Tree Preorder Traversaldesc: Given a binary tree, return the preorder traversal of its nodes’ values.二叉树前序遍历 thought: 遍历顺序 root &gt; left &gt; right solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.l = [] # 时间复杂度 O(N)， 空间复杂度O(1) def preorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用递归实现前序遍历''' if not root: return [] self.l.append(root.val) self.preorderTraversal(root.left) self.preorderTraversal(root.right) return self.l # 时间复杂度 O(N), 空间复杂度O(N) def preorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用循环实现前序遍历''' if not root: return [] mystack = [] # 存储根节点 out = [] cur = root while cur or mystack: # 从根节点开始，一直找它的左子树 while cur: mystack.append(cur) out.append(cur.val) cur = cur.left # 循环结束，表示前一个节点没有左子树了，开始查看右子树 cur = mystack.pop().right return out leetcode-94. Binary Tree Inorder Traversaldesc: Given a binary tree, return the inorder traversal of its nodes’ values.二叉树中序遍历 thought: 遍历顺序 left &gt; root &gt; right solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.l = [] # 时间复杂度 O(N)， 空间复杂度O(1) def inorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用递归实现前序遍历''' if not root: return [] self.inorderTraversal(root.left) self.l.append(root.val) self.inorderTraversal(root.right) return self.l # 时间复杂度 O(N), 空间复杂度O(N) def inorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用循环实现中序遍历''' if not root: return [] mystack = [] out = [] cur = root while cur or mystack: while cur: mystack.append(cur) cur = cur.left tmp = mystack.pop() out.append(tmp.val) cur = tmp.right return out leetcode-145. Binary Tree Postorder Traversaldesc: Given a binary tree, return the postorder traversal of its nodes’ values.二叉树后序遍历 thought: 遍历顺序 left &gt; right &gt; root, 左右根 是 根右左的反转，可以参考前序遍历的做法 solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.l = [] # 时间复杂度 O(N)， 空间复杂度O(1) def postorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用递归实现后序遍历''' if not root: return [] self.postorderTraversal(root.left) self.postorderTraversal(root.right) self.l.append(root.val) return self.l # 时间复杂度 O(N), 空间复杂度O(N) def postorderTraversal(self, root: TreeNode) -&gt; List[int]: '''利用循环实现后序遍历''' if not root: return [] cur = root mystack = [] out = [] # 参考前序遍历的做法，先遍历出右左根，最后反转结果-&gt;根左右 while cur or mystack: while cur: out.append(cur.val) mystack.append(cur) cur = cur.right cur = mystack.pop().left return out[::-1] leetcode-102. Binary Tree Level Order Traversaldesc: Given a binary tree, return the level order traversal of its nodes’ values. (ie, from left to right, level by level).二叉树层次遍历 thought: 遍历顺序一层一层的遍历，可以采用BFS, 也可以采用DFS solution:# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# 引入双端队列from collections import deque# BFSclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] queue = deque() result = [] queue.append(root) # 如果不是树，是图的话，就需要判断是否访问过 # visited = set(root) while queue: level_size = len(queue) current_level = [] # 循环遍历每一层的元素 for _ in range(level_size): # 双端队列 右进 左出 node = queue.popleft() current_level.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) result.append(current_level) return result # DFSclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] self.result = [] self._dfs(root, 0) return self.result def _dfs(self, node, level): if not node: return if len(self.result) &lt; level + 1: self.result.append([]) self.result[level].append(node.val) self._dfs(node.left, level+1) self._dfs(node.right, level+1) leetcode-236. Lowest Common Ancestor of a Binary Treedesc: Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree. According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).” Given the following binary tree: root = [3,5,1,6,2,0,8,null,null,7,4]求二叉树最小公共父节点 thought:采用递归 solution:# Definition for a binary tree node.class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def lowestCommonAncestor(self, root: 'TreeNode', p: 'TreeNode', q: 'TreeNode') -&gt; 'TreeNode': if any((not root, root==p, root==q)): return root left = self.lowestCommonAncestor(root.left, p, q) right = self.lowestCommonAncestor(root.right, p, q) if not left or not right: return left if left else right return root 递归动态规划生成斐波那契数列desc: thought:解法一：回溯法(递归)，包含大量重复计算，时间复杂度O(N*N)解法二: 动态规划，时间复杂度O(N),空间复杂度O(N)解法三：动态规划优化，只记录前两个值，时间复杂度O(N) solution:#解法一def fib(n): if n &lt;= 1: return n return fib(n-1) + fib(n-2) #解法二def fib(n): if n &lt;= 1 :return n nums = [0] * (n+1) nums[0] = 0 nums[1] = 1 for i in range(2, n+1): nums[i] = nums[i-1] + nums[i-2] return nums[-1] #解法三def fib(n): if n &lt;= 1: return n a, b = 0, 1 for i in range(2, n+1): sum = a + b a = b b = sum return b leetcode-62. Unique Pathsdesc: A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below). How many possible unique paths are there?机器人只能向右或者向下走，问到达终点有多少种走法。 thought: 采用DP方法，从终点往回递归，时间复杂度O(N*M) solution:class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: res = [[0 for _ in range(n)] for _ in range(m)] for i in range(m): for j in range(n): if i == 0 or j == 0: res[i][j] = 1 else: res[i][j] = res[i-1][j] + res[i][j-1] return res[m-1][n-1] 数学题其他leetcode-703.Kth Largest Element in a Streamdesc: Design a class to find the kth largest element in a stream. Note that it is the kth largest element in the sorted order, not the kth distinct element.Your KthLargest class will have a constructor which accepts an integer k and an integer array nums, which contains initial elements from the stream. For each call to the method KthLargest.add, return the element representing the kth largest element in the stream.返回数据流中第K大的元素 thought:采用优先队列，维护一个大小为K的最小堆 solution class KthLargest &#123;public: KthLargest(int k, vector&lt;int&gt;&amp; nums):k_(k) &#123; for(int num : nums) &#123; add(num); &#125; &#125; int add(int val) &#123; s_.push(val); if(s_.size() &gt; k_) &#123; s_.pop(); &#125; return s_.top(); &#125;private: const int k_; priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; s_;&#125;;/** * Your KthLargest object will be instantiated and called as such: * KthLargest* obj = new KthLargest(k, nums); * int param_1 = obj-&gt;add(val); */ 摩尔计数法desc: 找出一个列表中出现次数大于列表长度一半的数，这个数一定存在。thought: 不采用额外空间的话，有一种技巧是采用摩尔计数法，因为这个数只有一个，遍历数组，每次遍历两个数，两个数不相同，则删除。 高频考点如何在海量数据中快速查找最相似的文本逆序双向链表去掉字符串中违法的单括号leetcode-7. Reverse Integerdesc: Given a 32-bit signed integer, reverse digits of an integer.反转一个整数thought: 依次对10取余入栈，然后依次乘10出栈solution:class Solution: def reverse(self, x: int) -&gt; int: y = -1 * x if x &lt; 0 else x tmp = [] while y: tmp.append(y%10) y //= 10 res = 0 i = 1 while tmp: res += tmp.pop() * i i *= 10 res = -1 * res if x &lt; 0 else res if res &lt; pow(-2, 31) or res &gt; pow(2, 31): return 0 else: return res 反转字符串desc: 反转一个字符串，不能使用系统内置方法。thought: 采用两个指针，分别指向头尾，交换头为指针元素，并将头尾指针向中间靠拢。交换的时候可以采用两种方法：方法一：采用中间变量。方法二：采用异或的方式。solution:class Solution: # 方法一 def reverseString(self, s: List[str]) -&gt; None: \"\"\" Do not return anything, modify s in-place instead. \"\"\" i = 0 j = len(s) - 1 while i &lt; j: tmp = s[i] s[i] = s[j] s[j] = tmp i += 1 j -= 1 return s # 方法二 def reverseString(self, s: List[str]) -&gt; None: \"\"\" Do not return anything, modify s in-place instead. \"\"\" i = 0 j = len(s) - 1 while i &lt; j: s[i] = chr(ord(s[i]) ^ ord(s[j])) s[j] = chr(ord(s[i]) ^ ord(s[j])) s[i] = chr(ord(s[i]) ^ ord(s[j])) i += 1 j -= 1 return s 实现单词反转desc: 实现一句话中单词反转。how are you 反转后为 you are howthought: 采用两次字符串反转，第一次反转完为 uoy era woh 然后对每一个单词再进行一次字符串反转即可。soulution:class Solution: def reverseWords(self, s: str) -&gt; str: def reverseStr(s_list, front, end): while front &lt; end: tmp = s_list[front] s_list[front] = s_list[end] s_list[end] = tmp front += 1 end -= 1 # 删除首尾空格，替换多个空格为一个 s = ' '.join(s.strip().split()) lens = len(s) s_list = list(s) # 反转全部字符串 reverseStr(s_list, 0, lens-1) # 反转每个单词 begin = 0 i = 1 while i &lt; lens: if s_list[i] == ' ': reverseStr(s_list, begin, i-1) begin = i + 1 i += 1 reverseStr(s_list, begin, lens-1) return ''.join(s_list) 判断两个字符串的包含关系desc: 判断两个字符串是否有包含关系。s1=’abcdfg’, s2=’bfg’，则s1包含s2thought: 遍历一次短字符串，记录不同字符出现的次数,遍历长字符串，对短字符串中出现的字符总数减一，结果为零则包含。solution:def strContain(s1, s2): len1 = len(s1) len2 = len(s2) short_str = s1 if len1 &lt; len2 else s2 long_str = s1 if len1 &gt; len2 else s2 flag = [0 for _ in range(52)] count = 0 i = 0 while i &lt; len(short_str): if ord(list(short_str)[i]) &gt;= ord('A') and ord(list(short_str)[i]) &lt;= ord('Z'): k = ord(list(short_str)[i]) - ord('A') else: k = ord(list(short_str)[i]) - ord('a') + 26 if flag[k] == 0: flag[k] = 1 count += 1 i += 1 j = 0 while j &lt; len(long_str): if ord(list(long_str)[j]) &gt; ord('A') and ord(list(long_str)[j]) &lt;= ord('Z'): k = ord(list(long_str)[j]) - ord('A') else: k = ord(list(long_str)[j]) - ord('a') + 26 if flag[k] == 1: flag[k] = 0 count -= 1 if count == 0: return True j += 1 return False 消除字符串中的内嵌括号desc:给定一个如下格式的字符串:(1,(2,3,(4,(5,6),7))， 括号内的元素可以是数字，也可能是另一个括号，实现一个算法消除嵌套的括号 ，例如把上面的表达式变成(1,2,3,4,5,6,7),如果表达式有误，那么报错。 thought: 用一个变量记录括号，遇到’(‘ 加一，遇到’)’减一，如果变量最后为零，则括号合法。 solution:def removeNestedPare(s): if not s: return None if s[0] != '(' or s[-1] != ')': return None parantheses_num = 0 res = '(' for char in s: if char == '(': parantheses_num += 1 elif char == ')': parantheses_num -= 1 else: res = res + char if parantheses_num == 0: return res + ')' else: return None 将字符串转换成整数desc:给定一个字符串，将其转化为整数thought: 需要考虑正负，整数0对应当ascii码为48，依次判断每一位的值，并计算。solution:def str2int(s): if not s: return -1 res, flag, mult = 0, 1, 1 if s[0] == '+' or s[0] == '-': if s[0] == '-': flag = -1 s = s[1:] for char in range(len(s)-1, -1, -1): if s[char] &gt;= '0' and s[char] &lt;= '9': res += (ord(s[char]) - 48) * mult mult *= 10 else: return -1 return flag * res","categories":[{"name":"面试锦囊","slug":"面试锦囊","permalink":"http://lucas0625.github.io/blog/categories/面试锦囊/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://lucas0625.github.io/blog/tags/面试/"}]},{"title":"算法面试理论试题集锦","slug":"interviewTheory","date":"2019-05-05T08:31:43.372Z","updated":"2020-08-05T09:45:24.682Z","comments":true,"path":"2019/05/05/interviewTheory/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/05/interviewTheory/","excerpt":"面试准备(理论篇)","text":"面试准备(理论篇) Python自然语言处理LR 为什么使用 sigmoid简述 Dropout 的原理Dropout 是指在深度网络的训练中，以一定的概率随机地‘临时丢弃’一部分神经元节点。Dropout 作用于每份小批量训练数据，由于随机丢去部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。Dropout 可被认为是一种大规模深度神经网络的集成算法，在包含 N 个神经元节点的网络中，Dropout 可以看作是 $2^n$ 个模型的集成。一定程度上抑制过拟合。方法是前向传播的时候以概率 P 生成 0 或 1 作用于神经元上。 LSTMLSTM 参数量LSTM 结构： 第一层是一个 embedding 层，输出是 100 维的。 第二层是一个 LSTM 层，输出是 512 维的。 设 LSTM 输入维度为 x_dim， 输出维度为 y_dim，那么参数个数 n 为：n = 4 x ((x_dim + y_dim) * y_dim + y_dim)对应上述网络结构就是：n = 4 x ((100 + 512) * 512 + 512) = 1255424 LSTM 能解决 RNN 什么问题，并通过数学公式推导解决 RNN 梯度消失，梯度爆炸的问题。 双向 LSTM 比 LSTM 好在哪介绍一下 LSTM 的各个门优化方法Adam的特点word2vec介绍一下词嵌入，one-hot和word2vec的区别，word2vec如何实现的word2vec 哈夫曼方式时的词向量怎么获取介绍哈夫曼树与负采样CBOW 的损失函数bert如何改进 在预训练中，先用工具识别出句子中的短语，然后mask短语，然后预测mask掉的短语中的word，以word为预测单元。 ‘放你一马’ 在预训练中，先用工具识别出句子中的实体，然后mask实体，然后预测mask掉的短语中的word，以word为预测单元。 ‘黑龙江第十届冰雪大世界’. 为了使模型能表示多轮对话，可以采用QQR，QRQ，QRR作为输入。 使用多源预料。 Transformer vs LSTM 并性能力。Transformer是并行运行，LSTM是串行运行，对于大规模的语料，如果无法并行运行，那么时间就是一个噩梦，这是RNN无法落地的主要原因。 LSTM 依旧无法摆脱长距离依赖的问题，主要是因为其是通过维护一个全局的向量信息来做信息融合，可能一些重要信息在融合过程中丢失了，距离越长，这种问题可能越严重。 LSTM 不是真正的双向，会损失一部分信息，还是因为单一向量维护全局信息，这使得双向LSTM在特征提取方面不如Transformer。 ElasticSearchelasticsearch 的倒排索引是什么倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典 + 映射表即为倒排索引。有了倒排索引，就能实现o（1）时间复杂度 的效率检索文章了，极大的提高了检索效率。 倒排索引底层实现方式倒排索引的底层实现是基于：FST（Finite State Transducer）有限状态转移机，数据结构。lucene 从 4 + 版本后开始大量使用的数据结构是 FST。FST 有两个优点： 1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间； 2）查询速度快。O(len(str)) 的查询时间复杂度。 elasticsearch 中_score的评分机制5.0 版本之前标准的算法是 Term Frequency/Inverse Document Frequency, 简写为 TF/IDF。5.0 版本之后标准的算法是 BM25(Best Match 25), 基于 TF/IDF 进化来的。 elasticsearch 索引数据多了怎么办，如何调优，部署索引数据的规划，应在前期做好规划，正所谓 “设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。 1）动态索引，建立blogindex时间戳的形式，每天递增数据。这样的好处是不至于数据量激增导致单个索引数据量非常大。 2）冷热数据分离。定期压缩冷数据。 3）动态扩容。","categories":[{"name":"面试锦囊","slug":"面试锦囊","permalink":"http://lucas0625.github.io/blog/categories/面试锦囊/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://lucas0625.github.io/blog/tags/面试/"}]},{"title":"算法面试开放试题集锦","slug":"interviewQues","date":"2019-04-30T16:00:00.000Z","updated":"2020-08-05T09:40:30.596Z","comments":true,"path":"2019/05/01/interviewQues/","link":"","permalink":"http://lucas0625.github.io/blog/2019/05/01/interviewQues/","excerpt":"面试准备(开放问题篇)","text":"面试准备(开放问题篇) 反问技术面试官 对新人的培养方式以及对新人的期望，培养方式，学习空间，上升渠道。 组里目前主要的业务以及最大的挑战。 希望了解一下组内的技术栈，比如说使用的技术，框架，语言。 感觉自己已经被挂掉了，可以问需要着重学习哪一方面的知识。 反问HR面试官 薪资构成。 公司是否有激励机制。 预计多久会收到面试结果。 早上上班时间。 个人职业规划旅行去过的最喜欢城市两个词形容自己介绍两个朋友说一下自己的缺点","categories":[{"name":"面试锦囊","slug":"面试锦囊","permalink":"http://lucas0625.github.io/blog/categories/面试锦囊/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://lucas0625.github.io/blog/tags/面试/"}]},{"title":"ElasticSearch","slug":"ElasticSearch","date":"2019-04-29T07:18:55.059Z","updated":"2020-08-05T09:45:24.676Z","comments":true,"path":"2019/04/29/ElasticSearch/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/29/ElasticSearch/","excerpt":"ElasticSearch 原理与应用","text":"ElasticSearch 原理与应用 第一节 ElasticSearch概述1.1 ElasticSearch简介ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTfulweb接口。ElasticSearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。构建在全文检索开源软件Lucene之上的Elasticsearch，不仅能对海量规模的数据完成分布式索引与检索，还能提供数据聚合分析。据国际权威的数据库产品评测机构的统计，在2016年1月，Elasticsearch已超过Solr等，成为排名第一的搜索引擎类应用概括：基于Restful标准的高扩展高可用的实时数据分析的全文搜索工具 1.2 ElasticSearch的基本概念 Index:类似于mysql数据库中的database Type: 类似于mysql数据库中的table表，es中可以在Index中建立type（table），通过mapping进行映射。 Document: 由于es存储的数据是文档型的，一条数据对应一篇文档即相当于mysql数据库中的一行数据row，一个文档中可以有多个字段也就是mysql数据库一行可以有多列。 Field: es中一个文档中对应的多个列与mysql数据库中每一列对应 Mapping: 可以理解为mysql或者solr中对应的schema，只不过有些时候es中的mapping增加了动态识别功能，感觉很强大的样子，其实实际生产环境上不建议使用，最好还是开始制定好了对应的schema为主。 indexed: 就是名义上的建立索引。mysql中一般会对经常使用的列增加相应的索引用于提高查询速度，而在es中默认都是会加上索引的，除非你特殊制定不建立索引只是进行存储用于展示，这个需要看你具体的需求和业务进行设定了。 Query DSL: 类似于mysql的sql语句，只不过在es中是使用的json格式的查询语句，专业术语就叫：QueryDSL GET/PUT/POST/DELETE: 分别类似与mysql中的select/update/delete…… 1.3 Elasticsearch的架构 Gateway层 es用来存储索引文件的一个文件系统且它支持很多类型，例如：本地磁盘、共享存储（做snapshot的时候需要用到）、hadoop的hdfs分布式存储、亚马逊的S3。它的主要职责是用来对数据进行长持久化以及整个集群重启之后可以通过gateway重新恢复数据。 Distributed Lucene Directory Gateway上层就是一个lucene的分布式框架，lucene是做检索的，但是它是一个单机的搜索引擎，像这种es分布式搜索引擎系统，虽然底层用lucene，但是需要在每个节点上都运行lucene进行相应的索引、查询以及更新，所以需要做成一个分布式的运行框架来满足业务的需要。 四大模块组件 districted lucene directory之上就是一些es的模块 Index Module是索引模块，就是对数据建立索引也就是通常所说的建立一些倒排索引等； Search Module是搜索模块，就是对数据进行查询搜索； Mapping模块是数据映射与解析模块，就是你的数据的每个字段可以根据你建立的表结构通过mapping进行映射解析，如果你没有建立表结构，es就会根据你的数据类型推测你的数据结构之后自己生成一个mapping，然后都是根据这个mapping进行解析你的数据； River模块在es2.0之后应该是被取消了，它的意思表示是第三方插件，例如可以通过一些自定义的脚本将传统的数据库（mysql）等数据源通过格式化转换后直接同步到es集群里，这个river大部分是自己写的，写出来的东西质量参差不齐，将这些东西集成到es中会引发很多内部bug，严重影响了es的正常应用，所以在es2.0之后考虑将其去掉。 Discovery、Script es4大模块组件之上有 Discovery模块：es是一个集群包含很多节点，很多节点需要互相发现对方，然后组成一个集群包括选主的，这些es都是用的discovery模块，默认使用的是 Zen，也可是使用EC2；es查询还可以支撑多种script即脚本语言，包括mvel、js、python等等。 Transport协议层 再上一层就是es的通讯接口Transport，支持的也比较多：Thrift、Memcached以及Http，默认的是http，JMX就是java的一个远程监控管理框架，因为es是通过java实现的。 RESTful接口层 最上层就是es暴露给我们的访问接口，官方推荐的方案就是这种Restful接口，直接发送http请求，方便后续使用nginx做代理、分发包括可能后续会做权限的管理，通过http很容易做这方面的管理。如果使用java客户端它是直接调用api，在做负载均衡以及权限管理还是不太好做。 1.4 RESTfull API一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。在目前主流的三种Web服务交互方案中，REST相比于SOAP（Simple Object Access protocol，简单对象访问协议）以及XML-RPC更加简单明了 (Representational State Transfer 意思是：表述性状态传递) 它使用典型的HTTP方法，诸如GET,POST.DELETE,PUT来实现资源的获取，添加，修改，删除等操作。即通过HTTP动词来实现资源的状态扭转复制代码 GET 用来获取资源 POST 用来新建资源（也可以用于更新资源） PUT 用来更新资源 DELETE 用来删除资源 1.5 CRUL命令以命令的方式执行HTTP协议的请求GET/POST/PUT/DELETE 示例： 访问一个网页 curl www.baidu.com curl -o tt.html www.baidu.com 显示响应的头信息 curl -i www.baidu.com 显示一次HTTP请求的通信过程 curl -v www.baidu.com 执行GET/POST/PUT/DELETE操作 curl -X GET/POST/PUT/DELETE url 1.6 CentOS7下安装ElasticSearch6.2.4(1)配置JDK环境 配置环境变量 export JAVA_HOME=&quot;/opt/jdk1.8.0_144&quot;export PATH=&quot;$JAVA_HOME/bin:$PATH&quot;export CLASSPATH=&quot;.:$JAVA_HOME/lib&quot; (2)安装ElasticSearch6.2.4 下载地址：https://www.elastic.co/cn/downloads/elasticsearch 启动报错： 解决方式：bin/elasticsearch -Des.insecure.allow.root=true 或者修改bin/elasticsearch，加上ES_JAVA_OPTS属性：ES_JAVA_OPTS=”-Des.insecure.allow.root=true” 再次启动： 这是出于系统安全考虑设置的条件。由于ElasticSearch可以接收用户输入的脚本并且执行，为了系统安全考 虑，建议创建一个单独的用户用来运行ElasticSearch。 创建用户组和用户： groupadd esgroup useradd esuser -g esgroup -p espassword 更改elasticsearch文件夹及内部文件的所属用户及组： cd /opt chown -R esuser:esgroup elasticsearch-6.2.4 切换用户并运行： su esuser ./bin/elasticsearch 再次启动显示已杀死： 需要调整JVM的内存大小： vi bin/elasticsearch ES_JAVA_OPTS=”-Xms512m -Xmx512m” 再次启动：启动成功 如果显示如下类似信息： [INFO ][o.e.c.r.a.DiskThresholdMonitor] [ZAds5FP] low disk watermark [85%] exceeded on [ZAds5FPeTY-ZUKjXd7HJKA][ZAds5FP][/opt/elasticsearch-6.2.4/data/nodes/0] free: 1.2gb[14.2%], replicas will not be assigned to this node 需要清理磁盘空间。 后台运行：./bin/elasticsearch -d 测试连接：curl 127.0.0.1:9200 会看到一下JSON数据： [root@localhost ~]# curl 127.0.0.1:9200 &#123; &quot;name&quot; : &quot;rBrMTNx&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;-noR5DxFRsyvAFvAzxl07g&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.1.1&quot;, &quot;build_hash&quot; : &quot;5395e21&quot;, &quot;build_date&quot; : &quot;2016-12-06T12:36:15.409Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.3.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 实现远程访问：需要对config/elasticsearch.yml进行 配置： network.host: 192.168.25.131 再次启动报错： 处理第一个错误： vim /etc/security/limits.conf //文件最后加入 esuser soft nofile 65536 esuser hard nofile 65536 esuser soft nproc 4096 esuser hard nproc 4096 处理第二个错误： 进入limits.d目录下修改配置文件。 vim /etc/security/limits.d/20-nproc.conf修改为 esuser soft nproc 4096 处理第三个错误： vim /etc/sysctl.conf vm.max_map_count=655360 执行以下命令生效：sysctl -p 关闭防火墙：systemctl stop firewalld.service 再次启动成功！ 1.7 安装Head插件Head是elasticsearch的集群管理工具，可以用于数据的浏览和查询 (1)elasticsearch-head是一款开源软件，被托管在github上面，所以如果我们要使用它，必须先安装git，通过git获取elasticsearch-head (2)运行elasticsearch-head会用到grunt，而grunt需要npm包管理器，所以nodejs是必须要安装的 (3)elasticsearch5.0之后，elasticsearch-head不做为插件放在其plugins目录下了。使用git拷贝elasticsearch-head到本地 cd /usr/local/ git clone git://github.com/mobz/elasticsearch-head.git (4)安装elasticsearch-head依赖包 [root@localhost local]# npm install -g grunt-cli [root@localhost _site]# cd /usr/local/elasticsearch-head/ [root@localhost elasticsearch-head]# cnpm install (5)修改Gruntfile.js [root@localhost _site]# cd /usr/local/elasticsearch-head/ [root@localhost elasticsearch-head]# vi Gruntfile.js 在connect—&gt;server—&gt;options下面添加：hostname:’*’，允许所有IP可以访问 (6)修改elasticsearch-head默认连接地址[root@localhost elasticsearch-head]# cd /usr/local/elasticsearch-head/_site/ [root@localhost _site]# vi app.js 将this.base_uri = this.config.base_uri || this.prefs.get(“app-base_uri”) || “http://localhost:9200&quot;;中的localhost修改成你es的服务器地址 (7)配置elasticsearch允许跨域访问 打开elasticsearch的配置文件elasticsearch.yml，在文件末尾追加下面两行代码即可： http.cors.enabled: true http.cors.allow-origin: “*” (8)打开9100端口 [root@localhost elasticsearch-head]# firewall-cmd —zone=public —add-port=9100/tcp —permanent 重启防火墙 [root@localhost elasticsearch-head]# firewall-cmd —reload (9)启动elasticsearch (10)启动elasticsearch-head [root@localhost _site]# cd /usr/local/elasticsearch-head/ [root@localhost elasticsearch-head]# node_modules/grunt/bin/grunt server (11)访问elasticsearch-head 关闭防火墙：systemctl stop firewalld.service 浏览器输入网址：http://192.168.25.131:9100/ 1.8 安装KibanaKibana是一个针对Elasticsearch的开源分析及可视化平台，使用Kibana可以查询、查看并与存储在ES索引的数据进行交互操作，使用Kibana能执行高级的数据分析，并能以图表、表格和地图的形式查看数据 (1)下载Kibanahttps://www.elastic.co/downloads/kibana (2)把下载好的压缩包拷贝到/soft目录下 (3)解压缩，并把解压后的目录移动到/user/local/kibana (4)编辑kibana配置文件 [root@localhost /]# vi /usr/local/kibana/config/kibana.yml 将server.host,elasticsearch.url修改成所在服务器的ip地址 (5)开启5601端口 Kibana的默认端口是5601 开启防火墙:systemctl start firewalld.service 开启5601端口:firewall-cmd —permanent —zone=public —add-port=5601/tcp 重启防火墙：firewall-cmd –reload (6)启动Kibana [root@localhost /]# /usr/local/kibana/bin/kibana 浏览器访问：http://192.168.25.131:5601 1.9 安装中文分词器(1)下载中文分词器https://github.com/medcl/elasticsearch-analysis-ik 下载elasticsearch-analysis-ik-master.zip (2)解压elasticsearch-analysis-ik-master.zip unzip elasticsearch-analysis-ik-master.zip (3)进入elasticsearch-analysis-ik-master，编译源码 mvn clean install -Dmaven.test.skip=true (4)在es的plugins文件夹下创建目录ik (5)将编译后生成的elasticsearch-analysis-ik-版本.zip移动到ik下，并解压 (6)解压后的内容移动到ik目录下 第二节 ElasticSearch基本操作2.1 倒排索引Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 示例： (1)：假设文档集合包含五个文档，每个文档内容如图所示，在图中最左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。 (2):中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统将文档自动切分成单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引“单词ID”一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表 (3):索引系统还可以记录除此之外的更多信息,下图还记载了单词频率信息（TF）即这个单词在某个文档中的出现次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。 (4):倒排列表中还可以记录单词在某个文档出现的位置信息 (1,,1),(2,,1),(3,,2) 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“Facebook”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。 2.1.1 倒排索引原理1.The quick brown fox jumped over the lazy dog 2.Quick brown foxes leap over lazy dogs in summer 倒排索引： Term Doc_1 Doc_2 Quick X The X brown X X dog X dogs X fox X foxes X in X jumped X lazy X X leap X over X X quick X summer X the X 搜索quick brown ： Term Doc_1 Doc_2 brown X X quick X Total 2 1 计算相关度分数时，文档1的匹配度高，分数会比文档2高 问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。 fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。 jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。 搜索含有 Quick fox的文档是搜索不到的 使用标准化规则(normalization)：建立倒排索引的时候，会对拆分出的各个单词进行相应的处理，以提升后面搜索的时候能够搜索到相关联的文档的概率 Term Doc_1 Doc_2 brown X X dog X X fox X X in X jump X X lazy X X over X X quick X X summer X the X X 2.1.2 分词器介绍及内置分词器分词器：从一串文本中切分出一个一个的词条，并对每个词条进行标准化 包括三部分： character filter：分词之前的预处理，过滤掉HTML标签，特殊符号转换等 tokenizer：分词 token filter：标准化 内置分词器： standard 分词器：(默认的)他会将词汇单元转换成小写形式，并去除停用词和标点符号，支持中文采用的方法为单字切分 simple 分词器：首先会通过非字母字符来分割文本信息，然后将词汇单元统一为小写形式。该分析器会去掉数字类型的字符。 Whitespace 分词器：仅仅是去除空格，对字符没有lowcase化,不支持中文；并且不对生成的词汇单元进行其他的标准化处理。 language 分词器：特定语言的分词器，不支持中文 2.2 使用ElasticSearch API 实现CRUD添加索引： PUT /lib/ &#123; &quot;settings&quot;:&#123; &quot;index&quot;:&#123; &quot;number_of_shards&quot;: 5, &quot;number_of_replicas&quot;: 1 &#125; &#125;&#125; PUT lib 查看索引信息: GET /lib/_settings GET _all/_settings 添加文档: PUT /lib/user/1 &#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125; POST /lib/user/ &#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 23, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ] &#125; 查看文档: GET /lib/user/1 GET /lib/user/ GET /lib/user/1?_source=age,interests 更新文档: PUT /lib/user/1 &#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 36, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125; POST /lib/user/1/_update &#123; &quot;doc&quot;:&#123; &quot;age&quot;:33 &#125;&#125; 删除一个文档: DELETE /lib/user/1 删除一个索引: DELETE /lib 2.3 批量获取文档使用es提供的Multi Get API： 使用Multi Get API可以通过索引名、类型名、文档id一次得到一个文档集合，文档可以来自同一个索引库，也可以来自不同索引库 使用curl命令： curl &apos;http://192.168.25.131:9200/_mget&apos; -d &apos;&#123;&quot;docs&quot;：[ &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 1 &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2 &#125; ]&#125;&apos; 在客户端工具中： GET /_mget &#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 1 &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2 &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 3 &#125; ]&#125; 可以指定具体的字段： GET /_mget &#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 1, &quot;_source&quot;: &quot;interests&quot; &#125;, &#123; &quot;_index&quot;: &quot;lib&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2, &quot;_source&quot;: [&quot;age&quot;,&quot;interests&quot;] &#125; ]&#125; 获取同索引同类型下的不同文档： GET /lib/user/_mget &#123; &quot;docs&quot;:[ &#123; &quot;_id&quot;: 1 &#125;, &#123; &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: 2, &#125; ]&#125; GET /lib/user/_mget &#123; &quot;ids&quot;: [&quot;1&quot;,&quot;2&quot;] &#125; 2.4 使用Bulk API 实现批量操作bulk的格式： {action:{metadata}}\\n {requstbody}\\n action:(行为) create：文档不存在时创建 update:更新文档 index:创建新文档或替换已有文档 delete:删除一个文档 metadata：_index,_type,_id create和index的区别 如果数据存在，使用create操作失败，会提示文档已经存在，使用index则可以成功执行。 示例： {“delete”:{“_index”:”lib”,”_type”:”user”,”_id”:”1”}} 批量添加: POST /lib2/books/_bulk &#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;&#123;&quot;title&quot;:&quot;Java&quot;,&quot;price&quot;:55&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;&#123;&quot;title&quot;:&quot;Html5&quot;,&quot;price&quot;:45&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;&#123;&quot;title&quot;:&quot;Php&quot;,&quot;price&quot;:35&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:4&#125;&#125;&#123;&quot;title&quot;:&quot;Python&quot;,&quot;price&quot;:50&#125; 批量获取: GET /lib2/books/_mget &#123;&quot;ids&quot;: [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;]&#125; 删除：没有请求体 POST /lib2/books/_bulk &#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;lib2&quot;,&quot;_type&quot;:&quot;books&quot;,&quot;_id&quot;:4&#125;&#125;&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;tt&quot;,&quot;_type&quot;:&quot;ttt&quot;,&quot;_id&quot;:&quot;100&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;lisi&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;tt&quot;,&quot;_type&quot;:&quot;ttt&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;zhaosi&quot;&#125;&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;lib2&quot;,&quot;_type&quot;:&quot;books&quot;,&quot;_id&quot;:&quot;4&quot;&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;price&quot;:58&#125;&#125; bulk一次最大处理多少数据量: bulk会把将要处理的数据载入内存中，所以数据量是有限制的，最佳的数据量不是一个确定的数值，它取决于你的硬件，你的文档大小以及复杂性，你的索引以及搜索的负载。 一般建议是1000-5000个文档，大小建议是5-15MB，默认不能超过100M，可以在es的配置文件（即$ES_HOME下的config下的elasticsearch.yml）中。 2.5 版本控制ElasticSearch采用了乐观锁来保证数据的一致性，也就是说，当用户对document进行操作时，并不需要对该document作加锁和解锁的操作，只需要指定要操作的版本即可。当版本号一致时，ElasticSearch会允许该操作顺利执行，而当版本号存在冲突时，ElasticSearch会提示冲突并抛出异常（VersionConflictEngineException异常）。 ElasticSearch的版本号的取值范围为1到2^63-1。 内部版本控制：使用的是_version 外部版本控制：elasticsearch在处理外部版本号时会与对内部版本号的处理有些不同。它不再是检查version是否与请求中指定的数值相同_,而是检查当前的_version是否比指定的数值小。如果请求成功，那么外部的版本号就会被存储到文档中的_version中。 为了保持_version与外部版本控制的数据一致使用version_type=external 2.6 什么是MappingPUT /myindex/article/1 &#123; &quot;post_date&quot;: &quot;2018-05-10&quot;, &quot;title&quot;: &quot;Java&quot;, &quot;content&quot;: &quot;java is the best language&quot;, &quot;author_id&quot;: 119&#125; PUT /myindex/article/2 &#123; &quot;post_date&quot;: &quot;2018-05-12&quot;, &quot;title&quot;: &quot;html&quot;, &quot;content&quot;: &quot;I like html&quot;, &quot;author_id&quot;: 120&#125; PUT /myindex/article/3 &#123; &quot;post_date&quot;: &quot;2018-05-16&quot;, &quot;title&quot;: &quot;es&quot;, &quot;content&quot;: &quot;Es is distributed document store&quot;, &quot;author_id&quot;: 110&#125; GET /myindex/article/_search?q=2018-05 GET /myindex/article/_search?q=2018-05-10 GET /myindex/article/_search?q=html GET /myindex/article/_search?q=java 2.6.1 查看es自动创建的mappingGET /myindex/article/_mapping es自动创建了index，type，以及type对应的mapping(dynamic mapping) 什么是映射：mapping定义了type中的每个字段的数据类型以及这些字段如何分词等相关属性 &#123; &quot;myindex&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 创建索引的时候,可以预先定义字段的类型以及相关属性，这样就能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理字符串值等 支持的数据类型： (1)核心数据类型（Core datatypes） 字符型：string，string类型包括 text 和 keyword text类型被用来索引长文本，在建立索引前会将这些文本进行分词，转化为词的组合，建立索引。允许es来检索这些词语。text类型不能用来排序和聚合。 Keyword类型不需要进行分词，可以被用来检索过滤、排序和聚合。keyword 类型字段只能用本身来进行检索 数字型：long, integer, short, byte, double, float 日期型：date 布尔型：boolean 二进制型：binary (2)复杂数据类型（Complex datatypes） 数组类型（Array datatype）：数组类型不需要专门指定数组元素的type，例如： 字符型数组: [ &quot;one&quot;, &quot;two&quot; ] 整型数组：[ 1, 2 ] 数组型数组：[ 1, [ 2, 3 ]] 等价于[ 1, 2, 3 ] 对象数组：[ { &quot;name&quot;: &quot;Mary&quot;, &quot;age&quot;: 12 }, { &quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 10 }] 对象类型（Object datatype）：_ object _ 用于单个JSON对象； 嵌套类型（Nested datatype）：_ nested _ 用于JSON数组； (3)地理位置类型（Geo datatypes） 地理坐标类型（Geo-point datatype）：_ geo_point _ 用于经纬度坐标； 地理形状类型（Geo-Shape datatype）：_ geo_shape _ 用于类似于多边形的复杂形状； (4)特定类型（Specialised datatypes） IPv4 类型（IPv4 datatype）：_ ip _ 用于IPv4 地址； Completion 类型（Completion datatype）：_ completion _提供自动补全建议； Token count 类型（Token count datatype）：_ token_count _ 用于统计做了标记的字段的index数目，该值会一直增加，不会因为过滤条件而减少。 mapper-murmur3 类型：通过插件，可以通过 _ murmur3 _ 来计算 index 的 hash 值； 附加类型（Attachment datatype）：采用 mapper-attachments 插件，可支持_ attachments _ 索引，例如 Microsoft Office 格式，Open Document 格式，ePub, HTML 等。 支持的属性： “store”:false//是否单独设置此字段的是否存储而从_source字段中分离，默认是false，只能搜索，不能获取值 “index”: true//分词，不分词是：false ，设置成false，字段将不会被索引 “analyzer”:”ik”//指定分词器,默认分词器为standard analyzer “boost”:1.23//字段级别的分数加权，默认值是1.0 “doc_values”:false//对not_analyzed字段，默认都是开启，分词字段不能使用，对排序和聚合能提升较大性能，节约内存 “fielddata”:{“format”:”disabled”}//针对分词字段，参与排序或聚合时能提高性能，不分词字段统一建议使用doc_value “fields”:{“raw”:{“type”:”string”,”index”:”not_analyzed”}} //可以对一个字段提供多种索引模式，同一个字段的值，一个分词，一个不分词 “ignore_above”:100 //超过100个字符的文本，将会被忽略，不被索引 “include_in_all”:ture//设置是否此字段包含在_all字段中，默认是true，除非index设置成no选项 “index_options”:”docs”//4个可选参数docs（索引文档号） ,freqs（文档号+词频），positions（文档号+词频+位置，通常用来距离查询），offsets（文档号+词频+位置+偏移量，通常被使用在高亮字段）分词字段默认是position，其他的默认是docs “norms”:{“enable”:true,”loading”:”lazy”}//分词字段默认配置，不分词字段：默认{“enable”:false}，存储长度因子和索引时boost，建议对需要参与评分字段使用 ，会额外增加内存消耗量 “null_value”:”NULL”//设置一些缺失字段的初始化值，只有string可以使用，分词字段的null值也会被分词 “position_increament_gap”:0//影响距离查询或近似查询，可以设置在多值字段的数据上火分词字段上，查询时可指定slop间隔，默认值是100 “search_analyzer”:”ik”//设置搜索时的分词器，默认跟ananlyzer是一致的，比如index时用standard+ngram，搜索时用standard用来完成自动提示功能 “similarity”:”BM25”//默认是TF/IDF算法，指定一个字段评分策略，仅仅对字符串型和分词类型有效 “term_vector”:”no”//默认不存储向量信息，支持参数yes（term存储），with_positions（term+位置）,with_offsets（term+偏移量），with_positions_offsets(term+位置+偏移量) 对快速高亮fast vector highlighter能提升性能，但开启又会加大索引体积，不适合大数据量用 映射的分类： (1)动态映射： 当ES在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动地对该字段添加映射。 可以通过dynamic设置来控制这一行为，它能够接受以下的选项： true：默认值。动态添加字段 false：忽略新字段 strict：如果碰到陌生字段，抛出异常 dynamic设置可以适用在根对象上或者object类型的任意字段上。 POST /lib2 2.6.2 给索引lib2创建映射类型&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;books&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:false&#125;, &quot;publish_date&quot;:&#123;&quot;type&quot;:&quot;date&quot;,&quot;index&quot;:false&#125;, &quot;price&quot;:&#123;&quot;type&quot;:&quot;double&quot;&#125;, &quot;number&quot;:&#123;&quot;type&quot;:&quot;integer&quot;&#125; &#125; &#125; &#125;&#125; POST /lib2 2.6.3 给索引lib2创建映射类型&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;books&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;index&quot;:false&#125;, &quot;publish_date&quot;:&#123;&quot;type&quot;:&quot;date&quot;,&quot;index&quot;:false&#125;, &quot;price&quot;:&#123;&quot;type&quot;:&quot;double&quot;&#125;, &quot;number&quot;:&#123; &quot;type&quot;:&quot;object&quot;, &quot;dynamic&quot;:true &#125; &#125; &#125; &#125;&#125; 2.7基本查询(Query查询)2.7.1数据准备PUT /lib3 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;age&quot;: &#123;&quot;type&quot;:&quot;integer&quot;&#125;, &quot;interests&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;birthday&quot;: &#123;&quot;type&quot;:&quot;date&quot;&#125; &#125; &#125; &#125;&#125; GET /lib3/user/_search?q=name:lisi GET /lib3/user/_search?q=name:zhaoliu&amp;sort=age:desc 2.7.2 term查询和terms查询term query会去倒排索引中寻找确切的term，它并不知道分词器的存在。这种查询适合keyword 、numeric、date。 term:查询某个字段里含有某个关键词的文档 GET /lib3/user/_search/ &#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123;&quot;interests&quot;: &quot;changge&quot;&#125; &#125;&#125; terms:查询某个字段里含有多个关键词的文档 GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; 2.7.3 控制查询返回的数量from：从哪一个文档开始size：需要的个数 GET /lib3/user/_search &#123; &quot;from&quot;:0, &quot;size&quot;:2, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; 2.7.4 返回版本号GET /lib3/user/_search &#123; &quot;version&quot;:true, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; 2.7.5 match查询match query知道分词器的存在，会对filed进行分词操作，然后再查询 GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;name&quot;: &quot;zhaoliu&quot; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;age&quot;: 20 &#125; &#125;&#125; match_all:查询所有文档 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; multi_match:可以指定多个字段 GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;lvyou&quot;, &quot;fields&quot;: [&quot;interests&quot;,&quot;name&quot;] &#125; &#125;&#125; match_phrase:短语匹配查询 ElasticSearch引擎首先分析（analyze）查询字符串，从分析后的文本中构建短语查询，这意味着必须匹配短语中的所有分词，并且保证各个分词的相对位置不变： GET lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;interests&quot;: &quot;duanlian，shuoxiangsheng&quot; &#125; &#125;&#125; 2.7.6 指定返回的字段GET /lib3/user/_search &#123; &quot;_source&quot;: [&quot;address&quot;,&quot;name&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;&#125; 2.7.7控制加载的字段GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot;: &#123; &quot;includes&quot;: [&quot;name&quot;,&quot;address&quot;], &quot;excludes&quot;: [&quot;age&quot;,&quot;birthday&quot;] &#125;&#125; 使用通配符* GET /lib3/user/_search &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: &quot;addr*&quot;, &quot;excludes&quot;: [&quot;name&quot;,&quot;bir*&quot;] &#125;, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 2.7.8 排序使用sort实现排序：desc:降序，asc:升序 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;:&quot;asc&quot; &#125; &#125; ] &#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;age&quot;: &#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125; ] &#125; 2.7.9 前缀匹配查询GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;zhao&quot; &#125; &#125; &#125;&#125; 2.7.10 范围查询range:实现范围查询 参数：from,to,include_lower,include_upper,boost include_lower:是否包含范围的左边界，默认是true include_upper:是否包含范围的右边界，默认是true GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;birthday&quot;: &#123; &quot;from&quot;: &quot;1990-10-10&quot;, &quot;to&quot;: &quot;2018-05-01&quot; &#125; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;from&quot;: 20, &quot;to&quot;: 25, &quot;include_lower&quot;: true, &quot;include_upper&quot;: false &#125; &#125; &#125;&#125; 2.7.11 wildcard查询允许使用通配符* 和 ?来进行查询 *代表0个或多个字符 ？代表任意一个字符 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;name&quot;: &quot;zhao*&quot; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;name&quot;: &quot;li?i&quot; &#125; &#125;&#125; 2.7.12 fuzzy实现模糊查询value：查询的关键字 boost：查询的权值，默认值是1.0 min_similarity:设置匹配的最小相似度，默认值为0.5，对于字符串，取值为0-1(包括0和1);对于数值，取值可能大于1;对于日期型取值为1d,1m等，1d就代表1天 prefix_length:指明区分词项的共同前缀长度，默认是0 max_expansions:查询中的词项可以扩展的数目，默认可以无限大 GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;interests&quot;: &quot;chagge&quot; &#125; &#125;&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;interests&quot;: &#123; &quot;value&quot;: &quot;chagge&quot; &#125; &#125; &#125;&#125; 2.7.13 高亮搜索结果GET /lib3/user/_search &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;interests&quot;: &#123;&#125; &#125; &#125;&#125; 2.8 Filter查询filter是不计算相关性的，同时可以cache。因此，filter速度要快于query。 POST /lib4/items/_bulk &#123;&quot;index&quot;: &#123;&quot;_id&quot;: 1&#125;&#125;&#123;&quot;price&quot;: 40,&quot;itemID&quot;: &quot;ID100123&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 2&#125;&#125;&#123;&quot;price&quot;: 50,&quot;itemID&quot;: &quot;ID100124&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 3&#125;&#125;&#123;&quot;price&quot;: 25,&quot;itemID&quot;: &quot;ID100124&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 4&#125;&#125;&#123;&quot;price&quot;: 30,&quot;itemID&quot;: &quot;ID100125&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 5&#125;&#125;&#123;&quot;price&quot;: null,&quot;itemID&quot;: &quot;ID100127&quot;&#125; 2.8.1 简单的过滤查询GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;term&quot;: &#123; &quot;price&quot;: 40 &#125; &#125;&#125; GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;price&quot;: [25,40] &#125; &#125;&#125; GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;term&quot;: &#123; &quot;itemID&quot;: &quot;ID100123&quot; &#125; &#125;&#125; 查看分词器分析的结果： GET /lib4/_mapping 不希望商品id字段被分词，则重新创建映射 DELETE lib4 PUT /lib4 &#123; &quot;mappings&quot;: &#123; &quot;items&quot;: &#123; &quot;properties&quot;: &#123; &quot;itemID&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false &#125; &#125; &#125; &#125;&#125; 2.8.2 bool过滤查询可以实现组合过滤查询 格式： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [], &quot;should&quot;: [], &quot;must_not&quot;: [] &#125;&#125; must:必须满足的条件—-and should：可以满足也可以不满足的条件—or must_not:不需要满足的条件—not GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;: &#123;&quot;price&quot;:25&#125;&#125;, &#123;&quot;term&quot;: &#123;&quot;itemID&quot;: &quot;id100123&quot;&#125;&#125; ], &quot;must_not&quot;: &#123; &quot;term&quot;:&#123;&quot;price&quot;: 30&#125; &#125; &#125; &#125;&#125; 嵌套使用bool： GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;: &#123;&quot;itemID&quot;: &quot;id100123&quot;&#125;&#125;, &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123;&quot;itemID&quot;: &quot;id100124&quot;&#125;&#125;, &#123;&quot;term&quot;: &#123;&quot;price&quot;: 40&#125;&#125; ] &#125; &#125; ] &#125; &#125;&#125; 2.8.3 范围过滤gt: &gt; lt: &lt; gte: &gt;= lte: &lt;= GET /lib4/items/_search &#123; &quot;post_filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gt&quot;: 25, &quot;lt&quot;: 50 &#125; &#125; &#125;&#125; 2.8.5 过滤非空GET /lib4/items/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;exists&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125; &#125;&#125; GET /lib4/items/_search &#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot;: &#123; &quot;exists&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125; &#125;&#125; 2.8.6 过滤器缓存ElasticSearch提供了一种特殊的缓存，即过滤器缓存（filter cache），用来存储过滤器的结果，被缓存的过滤器并不需要消耗过多的内存（因为它们只存储了哪些文档能与过滤器相匹配的相关信息），而且可供后续所有与之相关的查询重复使用，从而极大地提高了查询性能。 注意：ElasticSearch并不是默认缓存所有过滤器，以下过滤器默认不缓存： numeric_range script geo_bbox geo_distance geo_distance_range geo_polygon geo_shape and or not exists,missing,range,term,terms默认是开启缓存的 开启方式：在filter查询语句后边加上“_catch”:true 2.9 聚合查询(1)sum GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_of_sum&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (2)min GET /lib4/items/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;price_of_min&quot;: &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (3)max GET /lib4/items/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;price_of_max&quot;: &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (4)avg GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_of_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (5)cardinality:求基数 GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_of_cardi&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; (6)terms:分组 GET /lib4/items/_search &#123; &quot;size&quot;:0, &quot;aggs&quot;: &#123; &quot;price_group_by&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; 对那些有唱歌兴趣的用户按年龄分组GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;, &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;age_group_by&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;order&quot;: &#123; &quot;avg_of_age&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_of_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 2.10 复合查询将多个基本查询组合成单一查询的查询 2.10.1 使用bool查询接收以下参数： must： 文档 必须匹配这些条件才能被包含进来。must_not： 文档 必须不匹配这些条件才能被包含进来。should： 如果满足这些语句中的任意语句，将增加 _score，否则，无任何影响。它们主要用于修正每个文档的相关性得分。filter： 必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。 相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 两者 都满足，那么它排名将更高： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125; ] &#125;&#125; 如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。但，如果存在至少一条 must 语句，则对 should 语句的匹配没有要求。如果我们不想因为文档的时间而影响得分，可以用 filter 语句来重写前面的例子： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125; &#125; &#125;&#125; 通过将 range 查询移到 filter 语句中，我们将它转成不评分的查询，将不再影响文档的相关性排名。由于它现在是一个不评分的查询，可以使用各种对 filter 查询有效的优化手段来提升性能。 bool 查询本身也可以被用做不评分的查询。简单地将它放置到 filter 语句中并在内部构建布尔逻辑： &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;lte&quot;: 29.99 &#125;&#125;&#125; ], &quot;must_not&quot;: [ &#123; &quot;term&quot;: &#123; &quot;category&quot;: &quot;ebooks&quot; &#125;&#125; ] &#125; &#125; &#125;&#125; 2.10.2 constant_score查询它将一个不变的常量评分应用于所有匹配的文档。它被经常用于你只需要执行一个 filter 而没有其它查询（例如，评分查询）的情况下。 &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;category&quot;: &quot;ebooks&quot; &#125; &#125; &#125;&#125; term 查询被放置在 constant_score 中，转成不评分的filter。这种方式可以用来取代只有 filter 语句的 bool 查询。 第三节 ElasticSearch原理3.1 解析es的分布式架构3.1.1 分布式架构的透明隐藏特性 ElasticSearch是一个分布式系统，隐藏了复杂的处理机制 分片机制：我们不用关心数据是按照什么机制分片的、最后放入到哪个分片中 分片的副本：集群发现机制(cluster discovery)：比如当前我们启动了一个es进程，当启动了第二个es进程时，这个进程作为一个node自动就发现了集群，并且加入了进去shard负载均衡：比如现在有10shard，集群中有3个节点，es会进行均衡的进行分配，以保持每个节点均衡的负载请求 请求路由 3.1.2 扩容机制垂直扩容：购置新的机器，替换已有的机器 水平扩容：直接增加机器 3.1.3 rebalance增加或减少节点时会自动均衡 3.1.4 master节点主节点的主要职责是和集群操作相关的内容，如创建或删除索引，跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点。稳定的主节点对集群的健康是非常重要的。 3.1.5 节点对等每个节点都能接收请求每个节点接收到请求后都能把该请求路由到有相关数据的其它节点上接收原始请求的节点负责采集数据并返回给客户端 3.2 分片和副本机制 index包含多个shard 每个shard都是一个最小工作单元，承载部分数据；每个shard都是一个lucene实例，有完整的建立索引和处理请求的能力 增减节点时，shard会自动在nodes中负载均衡 primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard replica shard是primary shard的副本，负责容错，以及承担读请求负载 primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改 primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上 3.3 单节点环境下创建索引分析PUT /myindex &#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的（一个shard的副本replica，他们两个是不能在同一个节点的）。集群可以正常工作，但是一旦出现节点宕机，数据全部丢失，而且集群不可用，无法接收任何请求。 3.4 两个节点环境下创建索引分析将3个primary shard分配到一个node上去，另外3个replica shard分配到另一个节点上 primary shard 和replica shard 保持同步 primary shard 和replica shard 都可以处理客户端的读请求 3.5 水平扩容的过程 扩容后primary shard和replica shard会自动的负载均衡 扩容后每个节点上的shard会减少，那么分配给每个shard的CPU，内存，IO资源会更多，性能提高 扩容的极限，如果有6个shard，扩容的极限就是6个节点，每个节点上一个shard，如果想超出扩容的极限，比如说扩容到9个节点，那么可以增加replica shard的个数 6个shard，3个节点，最多能承受几个节点所在的服务器宕机？(容错性)任何一台服务器宕机都会丢失部分数据。为了提高容错性，增加shard的个数：9个shard，(3个primary shard，6个replicashard)，这样就能容忍最多两台服务器宕机了 总结：扩容是为了提高系统的吞吐量，同时也要考虑容错性，也就是让尽可能多的服务器宕机还能保证数据不丢失 3.6ElasticSearch的容错机制以9个shard，3个节点为例： 1.如果master node 宕机，此时不是所有的primary shard都是Active status，所以此时的集群状态是red。 容错处理的第一步:是选举一台服务器作为master容错处理的第二步:新选举出的master会把挂掉的primary shard的某个replica shard 提升为primary shard,此时集群的状态为yellow，因为少了一个replica shard，并不是所有的replica shard都是active status 容错处理的第三步：重启故障机，新master会把所有的副本都复制一份到该节点上，（同步一下宕机后发生的修改），此时集群的状态为green，因为所有的primary shard和replica shard都是Active status 3.7文档的核心元数据 _index: 说明了一个文档存储在哪个索引中 同一个索引下存放的是相似的文档(文档的field多数是相同的) 索引名必须是小写的，不能以下划线开头，不能包括逗号 _type: 表示文档属于索引中的哪个类型 一个索引下只能有一个type 类型名可以是大写也可以是小写的，不能以下划线开头，不能包括逗号 _id: 文档的唯一标识，和索引，类型组合在一起唯一标识了一个文档 可以手动指定值，也可以由es来生成这个值 3.8 文档id生成方式 手动指定 put /index/type/66 通常是把其它系统的已有数据导入到es时 由es生成id值 post /index/type es生成的id长度为20个字符，使用的是base64编码，URL安全，使用的是GUID算法，分布式下并发生成id值时不会冲突 3.9 _source元数据分析其实就是我们在添加文档时request body中的内容 指定返回的结果中含有哪些字段： get /index/type/1?_source=name 3.10 改变文档内容原理解析替换方式： PUT /lib/user/4 &#123; &quot;first_name&quot; : &quot;Jane&quot;,&quot;last_name&quot; : &quot;Lucy&quot;,&quot;age&quot; : 24,&quot;about&quot; : &quot;I like to collect rock albums&quot;,&quot;interests&quot;: [ &quot;music&quot; ]&#125; 修改方式(partial update)： POST /lib/user/2/_update &#123; &quot;doc&quot;:&#123; &quot;age&quot;:26 &#125;&#125; 删除文档：标记为deleted，随着数据量的增加，es会选择合适的时间删除掉 3.11 基于groovy脚本执行partial updatees有内置的脚本支持，可以基于groovy脚本实现复杂的操作 修改年龄 POST /lib/user/4/_update &#123; &quot;script&quot;: &quot;ctx._source.age+=1&quot;&#125; 修改名字 POST /lib/user/4/_update &#123; &quot;script&quot;: &quot;ctx._source.last_name+=&apos;hehe&apos;&quot;&#125; 添加爱好 POST /lib/user/4/_update &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.interests.add(params.tag)&quot;, &quot;params&quot;: &#123; &quot;tag&quot;:&quot;picture&quot; &#125; &#125;&#125; 删除爱好 POST /lib/user/4/_update &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.interests.remove(ctx._source.interests.indexOf(params.tag))&quot;, &quot;params&quot;: &#123; &quot;tag&quot;:&quot;picture&quot; &#125; &#125;&#125; 删除文档 POST /lib/user/4/_update &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx.op=ctx._source.age==params.count?&apos;delete&apos;:&apos;none&apos;&quot;, &quot;params&quot;: &#123; &quot;count&quot;:29 &#125; &#125;&#125; upsert POST /lib/user/4/_update &#123; &quot;script&quot;: &quot;ctx._source.age += 1&quot;, &quot;upsert&quot;: &#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Lucy&quot;, &quot;age&quot; : 20, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125;&#125; 3.12 partial update 处理并发冲突使用的是乐观锁:_version retry_on_conflict: POST /lib/user/4/_update?retry_on_conflict=3 重新获取文档数据和版本信息进行更新，不断的操作，最多操作的次数就是retry_on_conflict的值 3.13 文档数据路由原理解析 文档路由到分片上： 一个索引由多个分片构成，当添加(删除，修改)一个文档时，es就需要决定这个文档存储在哪个分片上，这个过程就称为数据路由(routing) 路由算法： shard=hash(routing) % number_of_pirmary_shards 示例：一个索引，3个primary shard (1)每次增删改查时，都有一个routing值，默认是文档的_id的值 (2)对这个routing值使用哈希函数进行计算 (3)计算出的值再和主分片个数取余数 余数肯定在0—-（number_of_pirmary_shards-1）之间，文档就在对应的shard上 routing值默认是文档的_id的值，也可以手动指定一个值，手动指定对于负载均衡以及提高批量读取的性能都有帮助 primary shard个数一旦确定就不能修改了 3.14 文档增删改内部原理1:发送增删改请求时，可以选择任意一个节点，该节点就成了协调节点(coordinating node) 2.协调节点使用路由算法进行路由，然后将请求转到primary shard所在节点，该节点处理请求，并把数据同步到它的replica shard 3.协调节点对客户端做出响应 3.15 写一致性原理和quorum机制 任何一个增删改操作都可以跟上一个参数consistency 可以给该参数指定的值： one: (primary shard)只要有一个primary shard是活跃的就可以执行 all: (all shard)所有的primary shard和replica shard都是活跃的才能执行 quorum: (default) 默认值，大部分shard是活跃的才能执行 （例如共有6个shard，至少有3个shard是活跃的才能执行写操作） quorum机制：多数shard都是可用的， int((primary+number_of_replica)/2)+1 例如：3个primary shard，1个replica int((3+1)/2)+1=3 至少3个shard是活跃的 注意：可能出现shard不能分配齐全的情况 比如：1个primary shard,1个replicaint((1+1)/2)+1=2但是如果只有一个节点，因为primary shard和replica shard不能在同一个节点上，所以仍然不能执行写操作 再举例：1个primary shard,3个replica,2个节点 int((1+3)/2)+1=3 最后:当活跃的shard的个数没有达到要求时，es默认会等待一分钟，如果在等待的期间活跃的shard的个数没有增加，则显示timeout put /index/type/id?timeout=60s 3.16 文档查询内部原理第一步：查询请求发给任意一个节点，该节点就成了coordinating node，该节点使用路由算法算出文档所在的primary shard 第二步：协调节点把请求转发给primary shard也可以转发给replica shard(使用轮询调度算法(Round-Robin Scheduling，把请求平均分配至primary shard 和replica shard) 第三步：处理请求的节点把结果返回给协调节点，协调节点再返回给应用程序 特殊情况：请求的文档还在建立索引的过程中，primary shard上存在，但replica shar上不存在，但是请求被转发到了replica shard上，这时就会提示找不到文档 3.17 bulk批量操作的json格式解析bulk的格式： {action:{metadata}}\\n {requstbody}\\n 为什么不使用如下格式： [&#123;&quot;action&quot;: &#123;&#125;,&quot;data&quot;: &#123;&#125;&#125;] 这种方式可读性好，但是内部处理就麻烦了： 将json数组解析为JSONArray对象，在内存中就需要有一份json文本的拷贝，另外还有一个JSONArray对象。 解析json数组里的每个json，对每个请求中的document进行路由 为路由到同一个shard上的多个请求，创建一个请求数组 将这个请求数组序列化 将序列化后的请求数组发送到对应的节点上去 耗费更多内存，增加java虚拟机开销 不用将其转换为json对象，直接按照换行符切割json，内存中不需要json文本的拷贝 对每两个一组的json，读取meta，进行document路由 直接将对应的json发送到node上去 3.18 查询结果分析&#123; &quot;took&quot;: 419, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.6931472, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lib3&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.6931472, &quot;_source&quot;: &#123; &quot;address&quot;: &quot;bei jing hai dian qu qing he zhen&quot;, &quot;name&quot;: &quot;lisi&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lib3&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.47000363, &quot;_source&quot;: &#123; &quot;address&quot;: &quot;bei jing hai dian qu qing he zhen&quot;, &quot;name&quot;: &quot;zhaoming&quot; &#125; &#125; took：查询耗费的时间，单位是毫秒 _shards：共请求了多少个shard total：查询出的文档总个数 max_score：本次查询中，相关度分数的最大值，文档和此次查询的匹配度越高，_score的值越大，排位越靠前 hits：默认查询前10个文档 timed_out： GET /lib3/user/_search?timeout=10ms &#123; &quot;_source&quot;: [&quot;address&quot;,&quot;name&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;interests&quot;: &quot;changge&quot; &#125; &#125;&#125; 3.19 多index，多type查询模式GET _search GET /lib/_search GET /lib,lib3/_search GET /3,4/_search GET /lib/user/_search GET /lib,lib4/user,items/_search GET /_all/_search GET /_all/user,items/_search 3.20 分页查询中的deep paging问题GET /lib3/user/_search &#123; &quot;from&quot;:0, &quot;size&quot;:2, &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;interests&quot;: [&quot;hejiu&quot;,&quot;changge&quot;] &#125; &#125;&#125; GET /_search?from=0&amp;size=3 deep paging:查询的很深，比如一个索引有三个primary shard，分别存储了6000条数据，我们要得到第100页的数据(每页10条)，类似这种情况就叫deep paging 如何得到第100页的10条数据？ 在每个shard中搜索990到999这10条数据，然后用这30条数据排序，排序之后取10条数据就是要搜索的数据，这种做法是错的，因为3个shard中的数据的_score分数不一样，可能这某一个shard中第一条数据的_score分数比另一个shard中第1000条都要高，所以在每个shard中搜索990到999这10条数据然后排序的做法是不正确的。 正确的做法是每个shard把0到999条数据全部搜索出来（按排序顺序），然后全部返回给coordinate node，由coordinate node按_score分数排序后，取出第100页的10条数据，然后返回给客户端。 deep paging性能问题 耗费网络带宽，因为搜索过深的话，各shard要把数据传送给coordinate node，这个过程是有大量数据传递的，消耗网络， 消耗内存，各shard要把数据传送给coordinate node，这个传递回来的数据，是被coordinate node保存在内存中的，这样会大量消耗内存。 消耗cpu coordinate node要把传回来的数据进行排序，这个排序过程很消耗cpu. 鉴于deep paging的性能问题，所以应尽量减少使用。 3.21 query string查询及copy_to解析GET /lib3/user/_search?q=interests:changge GET /lib3/user/_search?q=+interests:changge GET /lib3/user/_search?q=-interests:changge copy_to字段是把其它字段中的值，以空格为分隔符组成一个大字符串，然后被分析和索引，但是不存储，也就是说它能被查询，但不能被取回显示。 注意:copy_to指向的字段字段类型要为：text 当没有指定field时，就会从copy_to字段中查询GET /lib3/user/_search?q=changge 3.22字符串排序问题对一个字符串类型的字段进行排序通常不准确，因为已经被分词成多个词条了 解决方式：对字段索引两次，一次索引分词（用于搜索），一次索引不分词(用于排序) GET /lib3/_search GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;interests&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; GET /lib3/user/_search &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;interests.raw&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125; DELETE lib3 PUT /lib3 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;age&quot;: &#123;&quot;type&quot;:&quot;integer&quot;&#125;, &quot;birthday&quot;: &#123;&quot;type&quot;:&quot;date&quot;&#125;, &quot;interests&quot;: &#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125;, &quot;fielddata&quot;: true &#125; &#125; &#125; &#125;&#125; 3.23 如何计算相关度分数使用的是TF/IDF算法(Term Frequency&amp;Inverse Document Frequency) Term Frequency:我们查询的文本中的词条在document本中出现了多少次，出现次数越多，相关度越高 搜索内容： hello world Hello，I love china. Hello world,how are you! Inverse Document Frequency：我们查询的文本中的词条在索引的所有文档中出现了多少次，出现的次数越多，相关度越低 搜索内容：hello world hello，what are you doing? I like the world. hello 在索引的所有文档中出现了500次，world出现了100次 Field-length(字段长度归约) norm:field越长，相关度越低 搜索内容：hello world {“title”:”hello,what’s your name?”,”content”:{“owieurowieuolsdjflk”}} {“title”:”hi,good morning”,”content”:{“lkjkljkj…….world”}} 查看分数是如何计算的： GET /lib3/user/_search?explain=true &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;interests&quot;: &quot;duanlian,changge&quot; &#125; &#125;&#125; 查看一个文档能否匹配上某个查询： GET /lib3/user/2/_explain &#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;interests&quot;: &quot;duanlian,changge&quot; &#125; &#125;&#125; 3.24 Doc Values 解析DocValues其实是Lucene在构建倒排索引时，会额外建立一个有序的正排索引(基于document =&gt; field value的映射列表) {“birthday”:”1985-11-11”,age:23} {“birthday”:”1989-11-11”,age:29} document age birthday doc1 23 1985-11-11 doc2 29 1989-11-11 存储在磁盘上，节省内存 对排序，分组和一些聚合操作能够大大提升性能 注意：默认对不分词的字段是开启的，对分词字段无效（需要把fielddata设置为true） PUT /lib3 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;age&quot;: &#123; &quot;type&quot;:&quot;integer&quot;, &quot;doc_values&quot;:false &#125;, &quot;interests&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;birthday&quot;: &#123;&quot;type&quot;:&quot;date&quot;&#125; &#125; &#125; &#125;&#125; 3.25 基于scroll技术滚动搜索大量数据如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用scoll滚动查询，一批一批的查，直到所有数据都查询完为止。 scoll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的 采用基于_doc(不使用_score)进行排序的方式，性能较高 每次发送scroll请求，我们还需要指定一个scoll参数，指定一个时间窗口，每次搜索请求只要在这个时间窗口内能完成就可以了 GET /lib3/user/_search?scroll=1m &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;:[&quot;_doc&quot;], &quot;size&quot;:3&#125; GET /_search/scroll &#123; &quot;scroll&quot;: &quot;1m&quot;, &quot;scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoAwAAAAAAAAAdFkEwRENOVTdnUUJPWVZUd1p2WE5hV2cAAAAAAAAAHhZBMERDTlU3Z1FCT1lWVHdadlhOYVdnAAAAAAAAAB8WQTBEQ05VN2dRQk9ZVlR3WnZYTmFXZw==&quot;&#125; 3.26 dynamic mapping策略dynamic: true:遇到陌生字段就 dynamic mapping false:遇到陌生字段就忽略 strict:约到陌生字段就报错 PUT /lib8 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;dynamic&quot;:strict, &quot;properties&quot;:&#123; &quot;name&quot;: &#123;&quot;type&quot;:&quot;text&quot;&#125;, &quot;address&quot;:&#123; &quot;type&quot;:&quot;object&quot;, &quot;dynamic&quot;:true &#125;, &#125; &#125; &#125;&#125; 会报错 PUT /lib8/user/1 &#123; &quot;name&quot;:&quot;lisi&quot;, &quot;age&quot;:20, &quot;address&quot;:&#123; &quot;province&quot;:&quot;beijing&quot;, &quot;city&quot;:&quot;beijing&quot; &#125;&#125; date_detection:默认会按照一定格式识别date，比如yyyy-MM-dd 可以手动关闭某个type的date_detection PUT /lib8 &#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 0 &#125;, &quot;mappings&quot;:&#123; &quot;user&quot;:&#123; &quot;date_detection&quot;: false, &#125; &#125;&#125; 定制 dynamic mapping template(type) PUT /my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;en&quot;: &#123; &quot;match&quot;: &quot;*_en&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; ] &#125; &#125; &#125; 使用了模板 PUT /my_index/my_type/3 &#123; &quot;title_en&quot;: &quot;this is my dog&quot; &#125; 没有使用模板 PUT /my_index/my_type/5 &#123; &quot;title&quot;: &quot;this is my cat&quot;&#125; GET my_index/my_type/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;is&quot; &#125; &#125;&#125; 3.27重建索引一个field的设置是不能修改的，如果要修改一个field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入到index中。 批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scroll就查询指定日期的一段数据，交给一个线程即可。 PUT /index1/type1/4 &#123; &quot;content&quot;:&quot;1990-12-12&quot;&#125; GET /index1/type1/_search GET /index1/type1/_mapping 报错PUT /index1/type1/4 &#123; &quot;content&quot;:&quot;I am very happy.&quot;&#125; 修改content的类型为string类型,报错，不允许修改 PUT /index1/_mapping/type1 &#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;&#125; 创建一个新的索引，把index1索引中的数据查询出来导入到新的索引中但是应用程序使用的是之前的索引，为了不用重启应用程序，给index1这个索引起个#别名 PUT /index1/_alias/index2 创建新的索引，把content的类型改为字符串 PUT /newindex &#123; &quot;mappings&quot;: &#123; &quot;type1&quot;:&#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 使用scroll批量查询 GET /index1/type1/_search?scroll=1m &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [&quot;_doc&quot;], &quot;size&quot;: 2&#125; 使用bulk批量写入新的索引POST /_bulk{“index”:{“_index”:”newindex”,”_type”:”type1”,”_id”:1}}{“content”:”1982-12-12”} 将别名index2和新的索引关联，应用程序不用重启 POST /_aliases &#123; &quot;actions&quot;: [ &#123;&quot;remove&quot;: &#123;&quot;index&quot;:&quot;index1&quot;,&quot;alias&quot;:&quot;index2&quot;&#125;&#125;, &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;newindex&quot;,&quot;alias&quot;: &quot;index2&quot;&#125;&#125;]&#125; GET index2/type1/_search 3.28 索引不可变的原因倒排索引包括： 文档的列表，文档的数量，词条在每个文档中出现的次数，出现的位置，每个文档的长度，所有文档的平均长度 索引不变的原因： 不需要锁，提升了并发性能 可以一直保存在缓存中（filter） 节省cpu和io开销 第四节 在Java应用中访问ElasticSearch4.1在Java应用中实现查询文档pom中加入ElasticSearch6.2.4的依赖： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.2.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- java编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 4.2 在Java应用中实现添加文档 \"&#123;\" + \"\\\"id\\\":\\\"1\\\",\" + \"\\\"title\\\":\\\"Java设计模式之装饰模式\\\",\" + \"\\\"content\\\":\\\"在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能。\\\",\" + \"\\\"postdate\\\":\\\"2018-05-20 14:38:00\\\",\" + \"\\\"url\\\":\\\"csdn.net/79239072\\\"\" + \"&#125;\"XContentBuilder doc1 = XContentFactory.jsonBuilder() .startObject() .field(\"id\",\"3\") .field(\"title\",\"Java设计模式之单例模式\") .field(\"content\",\"枚举单例模式可以防反射攻击。\") .field(\"postdate\",\"2018-02-03\") .field(\"url\",\"csdn.net/79247746\") .endObject(); IndexResponse response = client.prepareIndex(\"index1\", \"blog\", null) .setSource(doc1) .get(); System.out.println(response.status()); 4.3在Java应用中实现删除文档DeleteResponse response=client.prepareDelete(\"index1\",\"blog\",\"SzYJjWMBjSAutsuLRP_P\").get();//删除成功返回OK，否则返回NOT_FOUNDSystem.out.println(response.status()); 4.4在Java应用中实现更新文档 UpdateRequest request=new UpdateRequest(); request.index(\"index1\") .type(\"blog\") .id(\"2\") .doc( XContentFactory.jsonBuilder().startObject() .field(\"title\",\"单例模式解读\") .endObject() );UpdateResponse response=client.update(request).get();//更新成功返回OK，否则返回NOT_FOUNDSystem.out.println(response.status());upsert方式：IndexRequest request1 =new IndexRequest(\"index1\",\"blog\",\"3\") .source( XContentFactory.jsonBuilder().startObject() .field(\"id\",\"3\") .field(\"title\",\"装饰模式\") .field(\"content\",\"动态地扩展一个对象的功能\") .field(\"postdate\",\"2018-05-23\") .field(\"url\",\"csdn.net/79239072\") .endObject() ); UpdateRequest request2=new UpdateRequest(\"index1\",\"blog\",\"3\") .doc( XContentFactory.jsonBuilder().startObject() .field(\"title\",\"装饰模式解读\") .endObject() ).upsert(request1); UpdateResponse response=client.update(request2).get(); //upsert操作成功返回OK，否则返回NOT_FOUNDSystem.out.println(response.status()); 4.5在Java应用中实现批量操作 MultiGetResponse mgResponse = client.prepareMultiGet() .add(\"index1\",\"blog\",\"3\",\"2\") .add(\"lib3\",\"user\",\"1\",\"2\",\"3\") .get(); for(MultiGetItemResponse response:mgResponse)&#123; GetResponse rp=response.getResponse(); if(rp!=null &amp;&amp; rp.isExists())&#123; System.out.println(rp.getSourceAsString()); &#125; &#125; bulk： BulkRequestBuilder bulkRequest = client.prepareBulk();bulkRequest.add(client.prepareIndex(\"lib2\", \"books\", \"4\") .setSource(XContentFactory.jsonBuilder() .startObject() .field(\"title\", \"python\") .field(\"price\", 68) .endObject() ) );bulkRequest.add(client.prepareIndex(\"lib2\", \"books\", \"5\") .setSource(XContentFactory.jsonBuilder() .startObject() .field(\"title\", \"VR\") .field(\"price\", 38) .endObject() ) ); //批量执行BulkResponse bulkResponse = bulkRequest.get(); System.out.println(bulkResponse.status());if (bulkResponse.hasFailures()) &#123; System.out.println(\"存在失败操作\"); &#125; 第五节 在Python应用中访问ElasticSearch5.1 创建 Indexfrom elasticsearch import Elasticsearches = Elasticsearch()result = es.indices.create(index='news', ignore=400)print(result) 5.2 删除 Indexfrom elasticsearch import Elasticsearches = Elasticsearch()result = es.indices.delete(index='news', ignore=[400, 404])print(result) 5.3 插入数据from elasticsearch import Elasticsearches = Elasticsearch()es.indices.create(index='news', ignore=400)data = &#123;'title': '美国留给伊拉克的是个烂摊子吗', 'url': 'http://view.news.qq.com/zt2011/usa_iraq/index.htm'&#125;result = es.create(index='news', doc_type='politics', id=1, body=data)print(result) 5.4 更新数据from elasticsearch import Elasticsearches = Elasticsearch()data = &#123; 'title': '美国留给伊拉克的是个烂摊子吗', 'url': 'http://view.news.qq.com/zt2011/usa_iraq/index.htm', 'date': '2011-12-16'&#125;result = es.update(index='news', doc_type='politics', body=data, id=1)print(result) 5.5 删除数据from elasticsearch import Elasticsearches = Elasticsearch()result = es.delete(index='news', doc_type='politics', id=1)print(result) 5.6 查询数据from elasticsearch import Elasticsearches = Elasticsearch()mapping = &#123; 'properties': &#123; 'title': &#123; 'type': 'text', 'analyzer': 'ik_max_word', 'search_analyzer': 'ik_max_word' &#125; &#125;&#125;es.indices.delete(index='news', ignore=[400, 404])es.indices.create(index='news', ignore=400)result = es.indices.put_mapping(index='news', doc_type='politics', body=mapping)print(result)","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"es","slug":"es","permalink":"http://lucas0625.github.io/blog/tags/es/"}]},{"title":"正则表达式 简要手册","slug":"regex","date":"2019-04-29T03:32:03.363Z","updated":"2019-05-09T09:52:51.447Z","comments":true,"path":"2019/04/29/regex/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/29/regex/","excerpt":"正则表达式的简单梳理","text":"正则表达式的简单梳理 正则表达式基本语法 python中使用正则表达式import re pattern = re.compile(expression) 编译匹配模式 re.match(pattern, str) 从字符串开头开始匹配，返回match对象，使用group(0)取出匹配到的字符 re.search(pattern, str) 搜索字符串，返回第一个匹配到的match对象，使用group(0)取出匹配到的字符 re.findall(pattern, str) 以列表的形式返回所有匹配到的字符 re.finditer(pattern, str) 以迭代器的形式返回所有匹配到的字符 re.sub(pattern, repl_str, str) 将str中匹配到的字符替换成repl_str 正则表达式练习题正则表达式在线练习网址: HackerRank Regex Golf s1 = “get-element-by-id”; 给定这样一个连字符串，写一个function转换为驼峰命名法形式的字符串 getElementById import rea = 'get-element-by-id'pattern = re.compile('-[a-z]')for item in re.findall(pattern, a): a = a.replace(item, item[-1].upper())print(a) getElementById 判断字符串是否包含数字 import repattern = re.compile('\\d')for i in ['asdfdsf', 'asdfdsfs43', '343']: print(True if re.search(pattern, i) else False) False True True 判断电话号码 import repattern = re.compile('^1[34578]\\d&#123;9&#125;$')for i in ['1233456', '13366789980', '18678900875']: print(True if re.search(pattern, i) else False) False True True 判断是否符合指定格式，给定字符串str，检查其是否符合如下格式 XXX-XXX-XXXX 其中X为Number类型 import repattern = re.compile('^(\\d&#123;3&#125;-)&#123;2&#125;\\d&#123;4&#125;$')for i in ['XXX-XXX-XXXX', '123-456-6889', '1232-434-1314']: print(True if re.search(pattern, i) else False) False True False 判断是否符合USD格式，给定字符串 str，检查其是否符合美元书写格式 以 $ 开始 整数部分，从个位起，满 3 个数字用 , 分隔 如果为小数，则小数部分长度为 2 正确的格式如：$1,023,032.03 或者 $2.03，错误的格式如：$3,432,12.12 或者 $34,344.3 import repattern = re.compile('^\\$\\d&#123;1,3&#125;(,\\d&#123;3&#125;)*(\\.\\d&#123;2&#125;)?$')for i in ['$1,023,032.03', '$2.03', '$3,432,12.12', '$34,344.3']: print(True if re.search(pattern, i) else False) True True False False JS实现千位分隔符 import redef formatNum(num): if isinstance(num, int): num = str(num) pattern = re.compile(r'(\\d+)(\\d&#123;3&#125;)((,\\d&#123;3&#125;)*)') while True: num, count = re.subn(pattern, r'\\1,\\2\\3', num) if count == 0: break return numprint(formatNum(43532412245353)) 43,532,412,245,353 验证邮箱 import repattern = re.compile('^[a-zA-Z0-9_\\-]+@[a-zA-Z0-9_\\-]+\\.[a-zA-Z0-9_\\-]+$')for i in ['12@163.com', '244@qq.com', 'dsf@dsf']: print(True if re.search(pattern, i) else False) True True False 验证身份证号码, 身份证号码可能为15位或18位，15位为全数字，18位中前17位为数字，最后一位为数字或者X import repattern = re.compile('^\\d&#123;15&#125;$|^\\d&#123;17&#125;[0-9Xx]$')for i in ['34345345', '123456789012345', '12345678901234567X']: print(True if re.search(pattern, i) else False) False True True 匹配汉字 import repattern = re.compile('^[\\u4E00-\\u9FA5]*$')for i in ['34sd', '', '我们']: print(True if re.search(pattern, i) else False) False True True 去除首尾的’/‘ a = '/sdf/'a.strip('/')print(a) sdf 判断日期格式是否符合 ‘2017-05-11’的形式，简单判断，只判断格式 import repattern = re.compile('^\\d&#123;4&#125;\\-\\d&#123;1,2&#125;\\-\\d&#123;1,2&#125;$') 十六进制颜色正则 import repattern = re.compile('^#?([a-fA-F0-9]&#123;6&#125;|[a-fA-F0-9]&#123;3&#125;)$') 车牌号正则 import repattern = re.compile('^[京津沪渝冀豫云辽黑湘皖鲁新苏浙赣鄂桂甘晋蒙陕吉闽贵粤青藏川宁琼使领A-Z]&#123;1&#125;[A-Z]&#123;1&#125;[A-Z0-9]&#123;4&#125;[A-Z0-9挂学警港澳]&#123;1&#125;$')re.search(pattern , str) 密码强度正则，最少6位，包括至少1个大写字母，1个小写字母，1个数字，1个特殊字符 import repattern = re.compile('^.*(?=.&#123;6,&#125;)(?=.*\\d)(?=.*[A-Z])(?=.*[a-z])(?=.*[!@#$%^&amp;*? ]).*$') 匹配浮点数 import repattern = re.compile('^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$')","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"re","slug":"re","permalink":"http://lucas0625.github.io/blog/tags/re/"}]},{"title":"Git 常用操作指南","slug":"Git常用操作指南","date":"2019-04-28T08:11:53.115Z","updated":"2019-04-28T08:11:53.115Z","comments":true,"path":"2019/04/28/Git常用操作指南/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/28/Git常用操作指南/","excerpt":"本文介绍自己平时工作中常用的一些Git操作","text":"本文介绍自己平时工作中常用的一些Git操作 Git 常用操作指南.gitignore规范 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。[注]:所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（*）匹配零个或多个任意字符 [abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c） 问号（?）只匹配一个任意字符 如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 example: # 此为注释 – 将被 Git 忽略 # 忽略所有 .a 结尾的文件 *.a # 但 lib.a 除外 !lib.a # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO /TODO # 忽略 build/ 目录下的所有文件 build/ # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt doc/*.txt 查看文件之间的差别 git diff file 查看工作目录中当前文件和暂存区域快照之间的差异 git diff —cached / —staged 查看已经暂存起来的文件和上次提交时的快照之间的差异 提交文件 git commit -m ‘up’ 将暂存区的文件提交 git commit -a -m ‘up’ 跳过add步骤，将工作区的文件直接提交 删除文件 git rm file 从暂存区删除，并连带从工作目录中删除指定的文件 git rm -f file 如何删除之前修改过，并且已加入暂存区，则必须强制删除 git rm —cached file 将文件从git仓库中删除，但是仍保留在工作目录中 修改文件名 git mv file_form file_to 查看提交历史 git log 按照提交历史列出所有更新，最近的更新排在最上面 git log -p -n 显示每次提交的内容差异，显示最近的n次提交 git log —stat 显示每次提交的简要增改行统计 git log —pretty=oneline 将每个提交放在一行显示 git log —pretty=format:”%h - %an, %ar : %s” 定制要显示的记录格式 选项 说明 %H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字. 实际作出修改的人 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 -date= 选项定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字. 最后将此工作成果提交到仓库的人 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期，按多久以前的方式显示 %s 提交说明 git log —pretty=format:”%h %s” —graph 增加ASCII字符串表示的简单图形，展示每个提交所在的分支及其分化衍合情况 其他选项 选项 说明 -p 按补丁格式显示每个更新之间的差异。 --stat 显示每次更新的文件修改统计信息。 --shortstat 只显示 --stat 中最后的行数修改添加移除统计。 --name-only 仅在提交信息后显示已修改的文件清单。 --name-status 显示新增、修改、删除的文件清单。 --abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 --relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 --graph 显示 ASCII 图形表示的分支合并历史。 --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 git log —since=2.weeks 显示最近两周的提交 选项 说明 -(n) 仅显示最近的 n 条提交 --since, --after 仅显示指定时间之后的提交。 --until, --before 仅显示指定时间之前的提交。 --author 仅显示指定作者相关的提交。 --committer 仅显示指定提交者相关的提交。 撤销操作 git commit —amend 撤销刚才的提交操作，使用当前的暂存区域快照提交，如果刚才提交完没有作任何改动，相当于有机会重新编辑提交说明 git reset HEAD file 取消已暂存的文件 git checkout — file 取消工作区对文件的修改!!!!!!!!!!(此过程不可逆) 管理远程仓库 git remote 列出远程仓库的名字 git remote -v 显示对应的克隆地址 git pull 拉取远程仓库的更新 git push 推送数据到远程仓库 分支操作 git branch testing 新建testing分支 git branch 显示当前分支 git branch -a 显示所有分支 git checkout testing 转换到testing分支 git checkout -b testing 新建testing分支，并转向testing分支 git branch -d testing 删除testing分支 git merge testing 合并testing分支到当前分支 合并方式包括简单的指针前进操作 fast forward 或者多方合并 recursive， 当合并有冲突时，任何包含未解决冲突的文件都会以unmerged状态存在。&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 当前分支============&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;要合并的分支解决完冲突之后 执行add, commit 提交到仓库 git branch -v 查看各个分支最后一个提交的对象的信息 git branch —merged 筛选出与当前分支合并的分支，一般来说列表中没有*的分支通常都可以删除 git branch —no-merged 筛选出尚未与当前分支合并的分支 git rebase master 把当前分支里提交的改变移动到master分支里 git rebase —onto master server client 取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它在 master 上重演一遍 git rebase master server 取出特性分支 server，然后在主分支 master 上重演 储藏(stashing)操作“‘储藏”“可以获取你工作目录的中间状态——也就是你修改过的被追踪的文件和暂存的变更——并将它保存到一个未完结变更的堆栈中，随时可以重新应用。 git stash 往堆栈推送一个新的储藏， git stash list 查看现有的储藏 git stash apply 恢复最近一次储藏，但是不从堆栈中删除该储藏 git stash apply stash@{2} 恢复指定储藏，但是不从堆栈中删除该储藏 git stash pop 恢复最新一次储藏，并从堆栈中删除它 git stash drop stash@{2} 删除指定储藏","categories":[{"name":"工具","slug":"工具","permalink":"http://lucas0625.github.io/blog/categories/工具/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://lucas0625.github.io/blog/tags/Git/"}]},{"title":"Python 虚拟环境的使用","slug":"Python 虚拟环境的使用","date":"2019-04-28T07:14:06.892Z","updated":"2020-08-05T09:45:24.707Z","comments":true,"path":"2019/04/28/Python 虚拟环境的使用/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/28/Python 虚拟环境的使用/","excerpt":"简要介绍python中虚拟环境的使用","text":"简要介绍python中虚拟环境的使用 Python 虚拟环境的使用安装virtualenvpip install virtualenv 创建虚拟环境在当前目录中创建一个文件夹，实际上就是将python环境克隆了一份，包括python解释器，setuptools， pip， wheel， 以及python标准库virtualenv venv 使用虚拟环境激活后，系统提示符左侧会显示虚拟环境的名字，(venv)source venv/bin/activate 退出虚拟环境deactivate 在虚拟环境中安装包pip install -r requirements.txt 将虚拟环境中的依赖库打包pip freeze &gt; requirements.txt 删除虚拟环境rm -rf venv","categories":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://lucas0625.github.io/blog/categories/PYTHON/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"Word2Vec 原理与实现","slug":"Word2Vec","date":"2019-04-25T02:30:06.908Z","updated":"2020-08-05T09:45:24.703Z","comments":true,"path":"2019/04/25/Word2Vec/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/Word2Vec/","excerpt":"word2vec 详细推导与代码实现","text":"word2vec 详细推导与代码实现 基础知识逻辑回归sigmoid 函数定义： \\sigma(x)=\\frac{1}{1+e^{-x}}函数定义域为$(-\\infty,+\\infty)$, 值域为$(0.1)$，图像如下图所示：导函数： \\sigma^{\\prime}(x)=\\sigma(x)[1-\\sigma(x)]其中，以下两个导函数在后续推导中会用到。$\\log \\sigma(x)$ 的导函数为： [\\log \\sigma(x)]^{\\prime}=1-\\sigma(x)$\\log (1-\\sigma(x))$ 的导函数为： [\\log (1-\\sigma(x))]^{\\prime}=-\\sigma(x)分类过程$\\mathbf{x}{i} \\in \\mathbb{R}^{n}, y{i} \\in{0,1}$，而分类问题可写为： h_{\\theta}(\\mathbf{x})=\\sigma\\left(\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\cdots+\\theta_{n} x_{n}\\right)h_{\\theta}(\\mathbf{x})=\\sigma\\left(\\theta^{\\top} \\mathbf{x}\\right)=\\frac{1}{1+e^{-\\theta^{\\top} \\mathbf{x}}}而分类判别公式： y(\\mathrm{x})=\\left\\{\\begin{array}{ll}{1,} & {h_{\\theta}(\\mathrm{x}) \\geq 0.5} \\\\ {0,} & {h_{\\theta}(\\mathrm{x})","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"BERT 原理与实现","slug":"BERT","date":"2019-04-25T02:29:55.578Z","updated":"2020-08-05T09:45:24.605Z","comments":true,"path":"2019/04/25/BERT/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/BERT/","excerpt":"Bert 是 Google 在 2018 年 10 月提出的一种新的语言模型，全称为 Bidirectional Encoder Representations from Transformers（Bert）。和近年来的一些语言模型譬如 ELMo 不同，BERT 通过在所有层联合调节左右两个上下文来预训练深层双向表示，此外还通过组装长句作为输入增强了对长程语义的理解。Bert 可以被微调以广泛用于各类任务，仅需额外添加一个输出层，无需进行针对任务的模型结构调整，就在文本分类，语义理解等一些任务上取得了 state-of-the-art 的成绩。","text":"Bert 是 Google 在 2018 年 10 月提出的一种新的语言模型，全称为 Bidirectional Encoder Representations from Transformers（Bert）。和近年来的一些语言模型譬如 ELMo 不同，BERT 通过在所有层联合调节左右两个上下文来预训练深层双向表示，此外还通过组装长句作为输入增强了对长程语义的理解。Bert 可以被微调以广泛用于各类任务，仅需额外添加一个输出层，无需进行针对任务的模型结构调整，就在文本分类，语义理解等一些任务上取得了 state-of-the-art 的成绩。 Bert 的两种用法Bert 的论文中对预训练好的 Bert 模型设计了两种应用于具体领域任务的用法，一种是 fine-tune（微调） 方法，一种是 feature extract（特征抽取） 方法。 fine tune（微调）方法指的是加载预训练好的 Bert 模型，其实就是一堆网络权重的值，把具体领域任务的数据集喂给该模型，在网络上继续反向传播训练，不断调整原有模型的权重，获得一个适用于新的特定任务的模型。这很好理解，就相当于利用 Bert 模型帮我们初始化了一个网络的初始权重，是一种常见的迁移学习手段。 feature extract（特征抽取）方法指的是调用预训练好的 Bert 模型，对新任务的句子做句子编码，将任意长度的句子编码成定长的向量。编码后，作为你自己设计的某种模型（例如 LSTM、SVM 等都由你自己定）的输入，等于说将 Bert 作为一个句子特征编码器，这种方法没有反向传播过程发生，至于如果后续把定长句子向量输入到 LSTM 种继续反向传播训练，那就不关 Bert 的事了。这也是一种常见的语言模型用法，同类的类似 ELMo。 两种方式的适用场景两种方法各有优劣。首先。fine tune 的使用是具有一定限制的。预训练模型的模型结构是为预训练任务设计的，所以显然的，如果我们要在预训练模型的基础上进行再次的反向传播，那么我们做的具体领域任务对网络的设计要求必然得和预训练任务是一致的。那么 Bert 预训练过程究竟在做什么任务呢？Bert 一共设计了两个任务。 任务一：屏蔽语言模型（Masked LM）该任务类似于高中生做的英语完形填空，将语料中句子的部分单词进行遮盖，使用 [MASK] 作为屏蔽符号，然后预测被遮盖词是什么。例如对于原始句子 my dog is hairy ，屏蔽后 my dog is [MASK]。该任务中，隐层最后一层的 [MASK] 标记对应的向量会被喂给一个对应词汇表的 softmax 层，进行单词分类预测。当然具体实现还有很多问题，比如 [MASK] 会在训练集的上下文里出现，而测试集里永远没有，参见论文，此处不做详细介绍。 任务二：相邻句子判断（Next Sentence Prediction）语料中的句子都是有序邻接的，我们使用 [SEP] 作为句子的分隔符号，[CLS] 作为句子的分类符号，现在对语料中的部分句子进行打乱并拼接，形成如下这样的训练样本： Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]Label = IsNextInput = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight Label = NotNext 输入网络后，针对隐层最后一层 [CLS] 符号的词嵌入做 softmax 二分类，做一个预测两个句子是否是相邻的二分类任务。 可以看出，这两种任务都在训练过程中学习输入标记符号的 embedding，再基于最后一层的 embedding 仅添加一个输出层即可完成任务。该过程还可能引入一些特殊词汇符号，通过学习特殊符号譬如 [CLS] 的 embedding 来完成具体任务。 fine tune所以，如果我们要来 fine-tune 做任务，也是这个思路：首先对原有模型做适当构造，一般仅需加一输出层完成任务。Bert 的论文对于若干种常见任务做了模型构造的示例，如下： 图 a 和 b 是序列级别的任务，c 和 d 是词级别的任务。a 做句子对分类任务，b 做单句分类任务，构造非常简单，将图中红色箭头指的 [CLS] 对应的隐层输出接一个 softmax 输出层。c 做的是阅读理解问题，d 做的是命名实体识别（NER），模型构造也类似，取图中箭头指出的部分词对应的隐层输出分别接一个分类输出层完成任务。 类似以上这些任务的设计，可以将预训练模型 fine-tuning 到各类任务上，但也不是总是适用的，有些 NLP 任务并不适合被 Transformer encoder 架构表示，而是需要适合特定任务的模型架构。因此基于特征的方法就有用武之地了。 基于特征的方法和 ELMo 这类动生成态语境向量的模型是相同的使用方法：将输入序列的一层或多层的隐层状态拼接起来，作为序列的表示，然后作为下游任务模型的输入。这样带来的另一个好处是，可以把学习训练数据的表示这样一个复杂度很高的过程一次运行完成，然后基于序列的良好表示，进行大量低成本的模型实验。Bert 论文做了一些实验，对比了选取不同层数对模型性能的影响。 可以看出尽管基于 feature 的方法性能都不如全部层 fine tune 的方法，但拼接最后四个隐藏层的性能已经足够接近了。 如何 Coding？Bert 官方提供了 tensorflow 版本的代码，可以 fine tune 和 feature extract。第三方还提供了 Pytorch 版本的代码。此外，第三方 bert-as-service 项目封装了 feature extract 的方法，能以 web 接口的形式提供句子编码服务。建议如果是 fine tune，可以采用官方项目的代码，如果是特征抽取，则可以使用 bert-as-service 项目。 以下分别介绍基于 Bert 官方代码做 fine tune，以及利用 bert-as-service 该项目做句子编码。 基于 Bert 官方代码 fine tune 做句子分类任务下载预训练模型预训练过程代价很高，Google 声称目前放出的 12 层的 Base 版模型是 4 块 Cloud TPU 训练四天完成的，而 24 层的 Large 版本的模型则是 16 块 TPU 训练的。由于内存不足问题，普通 GPU 例如 GTX 1080 Ti、Titan X 都不能做预训练。因此我们下载 Google 提供的 预训练模型，提供了多语版本的和中文版本的模型。 中文版的模型是由维基百科语料训练而成的，并且与英文基于词的模型不同，中文是基于字做的。下载后解压如下，这些文件待会儿都要用到： $ tree chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/├── bert_config.json &lt;- 模型配置文件├── bert_model.ckpt.data-00000-of-00001 ├── bert_model.ckpt.index├── bert_model.ckpt.meta└── vocab.txt &lt;- 模型词汇表文件0 directories, 5 files 硬件资源要求Google 官方表示用给定的超参对 Bert-Base 进行 fine tune 至少需要 12GB RAM。结合源码和我的测试，大概情况如下： train_batch_size=32, max_seq_len=128, memory=11615MBtrain_batch_size=32, max_seq_len=64, memory=8460MB 如果想消耗更少的内存，可以减少 batch size，我目前没做更多测试。 代码纵览下载官方的 tensorflow 版本的代码 如下： $ tree bertbert├── CONTRIBUTING.md├── create_pretraining_data.py├── extract_features.py &lt;- feature extract 做句子编码的例子├── __init__.py├── LICENSE├── modeling.py├── modeling_test.py├── multilingual.md├── optimization.py├── optimization_test.py├── predicting_movie_reviews_with_bert_on_tf_hub.ipynb├── README.md├── requirements.txt├── run_classifier.py &lt;- fine-tune 做句子分类的例子├── run_classifier_with_tfhub.py├── run_pretraining.py &lt;- 预训练脚本├── run_squad.py &lt;- fine-tune 做完形填空的例子├── sample_text.txt├── tokenization.py &lt;- 分词└── tokenization_test.py0 directories, 20 files 包含了预训练脚本以及调用预训练模型 fine-tune 做句子分类的脚本、fine-tune 做 SQuAD 任务的脚本，以及用于 feature extract 的脚本等。 run_classifier.py 是我们主要关注和修改的代码 ，这是一个句子分类的脚本，可以用来进行文本分类模型的训练（train）、评估（dev）、预测（predict）。文本分类包括单句分类，输入一个句子，给出一个类标；句子对分类，即输入两个句子，给出一个类标。run_classifier.py 将他们都封装成了同一个任务，将多个句子中间用 [SEP] 分隔符连接，因此统一成了单句分类任务。 领域分类任务我们做一个公安领域的问题分类任务，假设数据集如下： train.tsv 场景 问题户政管理 我和男朋友分生的时候发现自己已经怀孕了，我想把孩子生下来，可孩子的户口怎么办户政管理 改一下户主快不快的要几天？出入境管理 你好,我想咨询一下:12月21日我们一家在曲靖办理了港澳通行证,当天就收到云南省公安厅出入境管理局发来的&quot;已受理完成&quot;的短信通知,可在12月27日我和我父亲又收到云南省公安厅出入境管理局发来的与21日相同的短信!所以想咨询一下办理港澳通行证的15个工作日是从21日还是27号开始计算时间啊?因已报团参加港澳游,所以特别急,请尽快回复,谢谢!请问我户口不在昆明，但我身份证是昆明的，是在昆明读大学的时候办的，我要办理港澳通行证，在昆明可以办理吗？还是必须要回户口所在地办理。户政管理 想把户口迁到外省去，大概几天可以办好？消防管理 如何预防吸烟引发的火灾?道路交通 换领机动车登记证书，要带车一起去吗道路交通 增加客运班线也要提出申请吗治安管理 一般多久能把危险化学品处置方案备案管理下来治安管理 周六能办理公章刻制备案吗？禁毒管理 非法集资的立案金额治安管理 娱乐场所备案申请要几天能办好道路交通 校车标牌核发也需要到车管所吗？ 我们将数据划分为 train.tsv、dev.tsv、test.tsv。 定义数据读取类run_classifier.py 声明了 DataProcessor 基类作为任务的数据处理基类，并实现了 XNLI、MultiNLI、MRPC、CoLA 这几个任务数据集的读取方式作为样例。该类很简单，仅需子类实现实现父类定义的如下四个方法： class DataProcessor(object): &quot;&quot;&quot;Base class for data converters for sequence classification data sets.&quot;&quot;&quot; def get_train_examples(self, data_dir): &quot;&quot;&quot;Gets a collection of `InputExample`s for the train set.&quot;&quot;&quot; raise NotImplementedError() def get_dev_examples(self, data_dir): &quot;&quot;&quot;Gets a collection of `InputExample`s for the dev set.&quot;&quot;&quot; raise NotImplementedError() def get_test_examples(self, data_dir): &quot;&quot;&quot;Gets a collection of `InputExample`s for prediction.&quot;&quot;&quot; raise NotImplementedError() def get_labels(self): &quot;&quot;&quot;Gets the list of labels for this data set.&quot;&quot;&quot; raise NotImplementedError() 参考写好的几个子类实现，我们首先继承 DataProcessor ，定义我们的领域分类任务的数据处理类 DomainCProcessor： class DomainCProcessor(DataProcessor): &quot;&quot;&quot;Processor for the DomainC corpus&quot;&quot;&quot; pass 实现 get_train_examples、get_dev_examples、get_test_examples这三个函数，分别对应训练、开发、测试的数据集的读取，输入参数 data_dir 由脚本启动时的 data_dir 参数获得，这三个函数需要把数据集每一行数据读为一个 InputExample，所有行构成 list[InputExample]。InputExample 声明如下 class InputExample(object): &quot;&quot;&quot;A single training/test example for simple sequence classification.&quot;&quot;&quot; def __init__(self, guid, text_a, text_b=None, label=None): &quot;&quot;&quot;Constructs a InputExample. Args: guid: Unique id for the example. text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified. text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks. label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples. &quot;&quot;&quot; self.guid = guid self.text_a = text_a self.text_b = text_b self.label = label InputExample 仅由唯一 id、句子 a、句子 b、类标，这四个属性构成，对于单句任务，句子 b 置 None。 所以我们要保证这三个函数 return 值是下面这样的形式即可： return [ InputExample(&apos;0&apos;, &apos;想把户口迁到外省去，大概几天可以办好？&apos;, None, &apos;户政管理&apos;), InputExample(&apos;1&apos;, &apos;校车标牌核发也需要到车管所吗？&apos;, None, &apos;道路交通&apos;), ] 一个完整的实现如下： class DomainCProcessor(DataProcessor): def get_train_examples(self, data_dir): return self._create_examples( self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;) def get_dev_examples(self, data_dir): return self._create_examples( self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;) def get_test_examples(self, data_dir): return self._create_examples( self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;) @classmethod def _read_tsv(cls, input_file, quotechar=None): &quot;&quot;&quot;读取 tsv 的工具方法&quot;&quot;&quot; with tf.gfile.Open(input_file, &quot;r&quot;) as f: reader = csv.reader(f, delimiter=&quot;\\t&quot;, quotechar=quotechar) lines = [] for line in reader: lines.append(line) return lines def _create_examples(self, lines, set_type): &quot;&quot;&quot;创建 InputExample 对象的工具方法&quot;&quot;&quot; examples = [] for (i, line) in enumerate(lines): if i == 0: continue guid = &quot;%s-%s&quot; % (set_type, i) text_a = tokenization.convert_to_unicode(line[1]) if set_type == &quot;test&quot;: label = &quot;上网服务管理&quot; else: label = tokenization.convert_to_unicode(line[0]) examples.append( InputExample(guid=guid, text_a=text_a, text_b=None, label=label)) return examples def get_labels(self): return [&quot;上网服务管理&quot;, &quot;出入境管理&quot;, &quot;刑事案件管辖&quot;, &quot;户政管理&quot;, &quot;治安管理&quot;, &quot;消防管理&quot;, &quot;禁毒管理&quot;, &quot;道路交通&quot;] 训练一旦定义好数据读取类，就可以训练模型了。启动命令如下： BERT_BASE_DIR=/bert/chinese_L-12_H-768_A-12 python run_classifier.py \\ --task_name=DomainC \\ --do_train=true \\ --do_eval=true \\ --do_predict=false \\ --data_dir=&apos;/bert/data/domainc&apos; \\ --vocab_file=$BERT_BASE_DIR/vocab.txt \\ --bert_config_file=$BERT_BASE_DIR/bert_config.json \\ --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\ --max_seq_length=128 \\ --train_batch_size=32 \\ --learning_rate=2e-5 \\ --num_train_epochs=3.0 \\ --output_dir=/tmp/domainc_output/ 参数中 do_train、do_eval 都是 true，因此会在 train.tsv 上训练，dev.tsv 上评估模型效果。至于 predict，我们下一节介绍。 其中的超参，官方表示如下设定在所有任务上都表现很好： • Batch size: 16, 32• Learning rate (Adam): 5e-5, 3e-5, 2e-5• Number of epochs: 3, 4 训练完毕，可以看到输出评估结果如下： INFO:tensorflow:***** Eval results *****INFO:tensorflow: eval_accuracy = 0.9213818INFO:tensorflow: eval_loss = 0.18872626INFO:tensorflow: global_step = 4721INFO:tensorflow: loss = 0.1886153 推断run_classifier.py 主要设计为单次运行的目的，如果把 do_predict 参数设置成 True，倒也确实可以预测，但输入样本是基于文件的，并且不支持将模型持久化在内存里进行 serving，因此需要自己改一些代码，达到两个目的： 允许将模型加载到内存里。允许一次加载，多次预测。 允许读取非文件中的样本进行预测。譬如从标准输入流读取样本输入。 将模型加载到内存里run_classifier.py 的 859 行加载了模型为 estimator 变量，但是遗憾的是 estimator 原生并不支持一次加载，多次预测。参见：https://guillaumegenthial.github.io/serving-tensorflow-estimator.html。 因此需要使用 estimator.export_saved_model() 方法把 estimator 重新导出成 tf.saved_model。 代码如下（参考了 https://github.com/bigboNed3/bert_serving）： def serving_input_fn(): label_ids = tf.placeholder(tf.int32, [None], name=&apos;label_ids&apos;) input_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=&apos;input_ids&apos;) input_mask = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=&apos;input_mask&apos;) segment_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=&apos;segment_ids&apos;) input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(&#123; &apos;label_ids&apos;: label_ids, &apos;input_ids&apos;: input_ids, &apos;input_mask&apos;: input_mask, &apos;segment_ids&apos;: segment_ids, &#125;)() return input_fnestimator._export_to_tpu = Falseestimator.export_savedmodel(&apos;/bert/my_model&apos;, serving_input_fn) 执行后，可以 $ ls /bert/my_model1553914434 会有一个时间戳命名的模型目录，因此我们最终的模型目录为 /bert/my_model/1553914434。 这样之后我们就不需要第 859 行那个 estimator 对象了，可以自行从刚刚的模型目录加载模型： predict_fn = tf.contrib.predictor.from_saved_model(&apos;/bert/my_model/1553914434&apos;) 从内存中读取样本数据并预测基于上面的 predict_fn 变量，就可以直接进行预测了。下面是一个从标准输入流读取问题样本，并预测分类的样例代码： while True: question = input(&quot;&gt; &quot;) predict_example = InputExample(&quot;id&quot;, question, None, &apos;某固定伪标记&apos;) feature = convert_single_example(100, predict_example, label_list, FLAGS.max_seq_length, tokenizer) prediction = predict_fn(&#123; &quot;input_ids&quot;:[feature.input_ids], &quot;input_mask&quot;:[feature.input_mask], &quot;segment_ids&quot;:[feature.segment_ids], &quot;label_ids&quot;:[feature.label_id], &#125;) probabilities = prediction[&quot;probabilities&quot;] label = label_list[probabilities.argmax()] print(label) 完整代码可以参见 https://gitlab.aegis-info.com/padeoe/bert/blob/dev/run_classifier.py。 bert-as-service 是一个第三方项目，Github 地址: hanxiao/bert-as-service。可以对 Bert 实现 feature extract 的用法，即将一个不定长的句子编码为一个定长的向量。该项目对 Bert 官方代码封装实现了 web 后端，以 web 接口的形式提供句子编码服务。 一些问题这个项目做的不是很完善，譬如 bert-as-service 分为服务端和客户端。官方文档中服务端部署在 GPU 上、客户端则在 CPU 上。虽然官方没有直接提到，但是事实上服务端部署在 CPU 也可以。但其官方 docker 脚本并不能在 CPU 上运行，需要自行修改基容器。 其次，文档声称 tensorflow&gt;=1.10，python&gt;=3.5 即可运行，但事实上我在 windows 运行失败，大概是有 bug。 搭建服务端搭建之前需要准备好上文 下载预训练模型 提到的 Bert 预训练模型。 安装并启动 bert-as-service。启动参数包括模型路径 PATH_MODEL、进程数 NUM_WORKER。安装方式有 pip 或 docker： pip （CPU/GPU） PATH_MODEL=/tmp/chinese_L-12_H-768_A-12NUM_WORKER=4pip install bert-serving-serverbert-serving-start -model_dir $PATH_MODEL -num_worker=$NUM_WORKER dockerGPU git clone https://github.com/hanxiao/bert-as-service.gitcd bert-as-servicedocker build -t bert-as-service -f docker/Dockerfile .PATH_MODEL=/tmp/chinese_L-12_H-768_A-12NUM_WORKER=4docker run --runtime nvidia --name bert-as-service -dit -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t bert-as-service $NUM_WORKER CPU git clone https://github.com/hanxiao/bert-as-service.gitcd bert-as-servicesed -i &apos;s/tensorflow:1.12.0-gpu-py3/tensorflow:1.12.0-py3/g&apos; docker/Dockerfiledocker build -t bert-as-service -f docker/Dockerfile .PATH_MODEL=/tmp/chinese_L-12_H-768_A-12NUM_WORKER=4docker run --name bert-as-service -dit -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t bert-as-service $NUM_WORKER 客户端调用安装客户端库。 pip install bert-serving-client 安装完就可以在代码中调用了： &gt;&gt;&gt; from bert_serving.client import BertClient&gt;&gt;&gt; bc = BertClient(ip=&apos;192.168.11.42&apos;)&gt;&gt;&gt; bc.encode([&apos;合同诈骗罪是怎样处罚的？&apos;, &apos;孩子办户口的时间限制&apos;])array([[ 0.56223714, -0.28043476, 0.15880957, ..., -0.5312789 , -0.05316753, 0.5204646 ], [ 0.3173091 , -0.39072016, 0.08520816, ..., -0.21686065, -0.25086305, -0.08226559]], dtype=float32)&gt;&gt;&gt; 如上，encode 函数接受句子 list，无论句子有多少，服务端会自动处理 batch。获得了句子的编码之后，就可以输入自己的模型做后续运算了。","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Transformer 原理与实现","slug":"Transformer","date":"2019-04-25T02:29:27.661Z","updated":"2020-08-05T09:45:24.698Z","comments":true,"path":"2019/04/25/Transformer/","link":"","permalink":"http://lucas0625.github.io/blog/2019/04/25/Transformer/","excerpt":"","text":"Transformer 模型介绍 说到自然语言处理, 语言模型, 命名实体识别, 机器翻译, 可能很多人想到的LSTM等循环神经网络, 但目前其实LSTM起码在自然语言处理领域已经过时了, 在Stanford阅读理解数据集(SQuAD2.0)榜单里, 机器的成绩已经超人类表现, 这很大程度要归功于transformer的BERT预训练模型. transformer是谷歌大脑在2017年底发表的论文attention is all you need中所提出的seq2seq模型. 现在已经取得了大范围的应用和扩展, 而BERT就是从transformer中衍生出来的预训练语言模型. 目前自然语言处理领域的现状, 目前transformer模型已经得到广泛认可和应用, 而应用的方式主要是先进行预训练语言模型, 然后把预训练的模型适配给下游任务, 以完成各种不同的任务, 如分类, 生成, 标记等等, 预训练模型非常重要, 预训练的模型的性能直接影响下游任务的性能。 transformer编码器(理论部分): $transformer$模型的直觉, 建立直观认识; $positional \\ encoding$, 即位置嵌入(或位置编码); $self \\ attention \\ mechanism$, 即自注意力机制与注意力矩阵可视化; $Layer \\ Normalization$和残差连接. $transformer \\ encoder$整体结构. 0. $transformer$模型的直觉, 建立直观认识;首先来说一下transformer和LSTM的最大区别, 就是LSTM的训练是迭代的, 是一个接一个字的来, 当前这个字过完LSTM单元, 才可以进下一个字, 而transformer的训练是并行了, 就是所有字是全部同时训练的, 这样就大大加快了计算效率, transformer使用了位置嵌入$(positional \\ encoding)$来理解语言的顺序, 使用自注意力机制和全连接层来进行计算, 这些后面都会详细讲解.transformer模型主要分为两大部分, 分别是编码器和解码器, 编码器负责把自然语言序列映射成为隐藏层(下图中第2步用九宫格比喻的部分), 含有自然语言序列的数学表达. 然后解码器把隐藏层再映射为自然语言序列, 从而使我们可以解决各种问题, 如情感分类, 命名实体识别, 语义关系抽取, 摘要生成, 机器翻译等等, 下面我们简单说一下下图的每一步都做了什么: 输入自然语言序列到编码器: Why do we work?(为什么要工作); 编码器输出的隐藏层, 再输入到解码器; 输入$$(起始)符号到解码器; 得到第一个字”为”; 将得到的第一个字”为”落下来再输入到解码器; 得到第二个字”什”; 将得到的第二字再落下来, 直到解码器输出$$(终止符), 即序列生成完成. 本节内容限于编码器部分, 即把自然语言序列映射为隐藏层的数学表达的过程, 因为理解了编码器中的结构, 理解解码器就非常简单了,最重要的是BERT预训练模型只用到了编码器的部分, 也就是先用编码器训练一个语言模型, 然后再把它适配给其他五花八门的任务.而且我们用编码器就能够完成一些自然语言处理中比较主流的任务, 如情感分类, 语义关系分析, 命名实体识别等。 Transformer Block结构图, 注意: 为方便查看, 下面的内容分别对应着上图第1, 2, 3, 4个方框的序号: 1. $positional \\ encoding$, 即位置嵌入(或位置编码);由于transformer模型没有循环神经网络的迭代操作, 所以我们必须提供每个字的位置信息给transformer, 才能识别出语言中的顺序关系.现在定义一个位置嵌入的概念, 也就是$positional \\ encoding$, 位置嵌入的维度为$[max \\ sequence \\ length, \\ embedding \\ dimension]$, 嵌入的维度同词向量的维度, $max \\ sequence \\ length$属于超参数, 指的是限定的最大单个句长.注意, 我们一般以字为单位训练transformer模型, 也就是说我们不用分词了, 首先我们要初始化字向量为$[vocab \\ size, \\ embedding \\ dimension]$, $vocab \\ size$为总共的字库数量, $embedding \\ dimension$为字向量的维度, 也是每个字的数学表达.在这里论文中使用了$sine$和$cosine$函数的线性变换来提供给模型位置信息: PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\quad PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})\\tag{eq.1}上式中$pos$指的是句中字的位置, 取值范围是$[0, \\ max \\ sequence \\ length)$, $i$指的是词向量的维度, 取值范围是$[0, \\ embedding \\ dimension)$, 上面有$sin$和$cos$一组公式, 也就是对应着$embedding \\ dimension$维度的一组奇数和偶数的序号的维度, 例如$0, 1$一组, $2, 3$一组, 分别用上面的$sin$和$cos$函数做处理, 从而产生不同的周期性变化, 而位置嵌入在$embedding \\ dimension$维度上随着维度序号增大, 周期变化会越来越慢, 而产生一种包含位置信息的纹理, 就像论文原文中第六页讲的, 位置嵌入函数的周期从$2 \\pi$到$10000 * 2 \\pi$变化, 而每一个位置在$embedding \\ dimension$维度上都会得到不同周期的$sin$和$cos$函数的取值组合, 从而产生独一的纹理位置信息, 模型从而学到位置之间的依赖关系和自然语言的时序特性.下面画一下位置嵌入, 可见纵向观察, 随着$embedding \\ dimension$增大, 位置嵌入函数呈现不同的周期变化. # 导入依赖库import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport math def get_positional_encoding(max_seq_len, embed_dim): # 初始化一个positional encoding # embed_dim: 字嵌入的维度 # max_seq_len: 最大的序列长度 positional_encoding = np.array([ [pos / np.power(10000, 2 * i / embed_dim) for i in range(embed_dim)] if pos != 0 else np.zeros(embed_dim) for pos in range(max_seq_len)]) positional_encoding[1:, 0::2] = np.sin(positional_encoding[1:, 0::2]) # dim 2i 偶数 positional_encoding[1:, 1::2] = np.cos(positional_encoding[1:, 1::2]) # dim 2i+1 奇数 # 归一化, 用位置嵌入的每一行除以它的模长 # denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True)) # position_enc = position_enc / (denominator + 1e-8) return positional_encoding positional_encoding = get_positional_encoding(max_seq_len=100, embed_dim=16)plt.figure(figsize=(10,10))sns.heatmap(positional_encoding)plt.title(\"Sinusoidal Function\")plt.xlabel(\"hidden dimension\")plt.ylabel(\"sequence length\") Text(69.0, 0.5, &#39;sequence length&#39;) plt.figure(figsize=(8, 5))plt.plot(positional_encoding[1:, 1], label=\"dimension 1\")plt.plot(positional_encoding[1:, 2], label=\"dimension 2\")plt.plot(positional_encoding[1:, 3], label=\"dimension 3\")plt.legend()plt.xlabel(\"Sequence length\")plt.ylabel(\"Period of Positional Encoding\") Text(0, 0.5, &#39;Period of Positional Encoding&#39;) X: [batch_size, len, embedding_size]W: [embedding_size, hidden_dimension]XW = [batch_size, len, hidden_dimension] 2. $self \\ attention \\ mechanism$, 自注意力机制; Attention Mask 注意, 在上面$self \\ attention$的计算过程中, 我们通常使用$mini \\ batch$来计算, 也就是一次计算多句话, 也就是$X$的维度是$[batch \\ size, \\ sequence \\ length]$, $sequence \\ length$是句长, 而一个$mini \\ batch$是由多个不等长的句子组成的, 我们就需要按照这个$mini \\ batch$中最大的句长对剩余的句子进行补齐长度, 我们一般用$0$来进行填充, 这个过程叫做$padding$.但这时在进行$softmax$的时候就会产生问题, 回顾$softmax$函数$\\sigma (\\mathbf {z} ){i}={\\frac {e^{z{i}}}{\\sum {j=1}^{K}e^{z{j}}}}$, $e^0$是1, 是有值的, 这样的话$softmax$中被$padding$的部分就参与了运算, 就等于是让无效的部分参与了运算, 会产生很大隐患, 这时就需要做一个$mask$让这些无效区域不参与运算, 我们一般给无效区域加一个很大的负数的偏置, 也就是: z_{illegal} = z_{illegal} + bias_{illegal}bias_{illegal} \\to -\\inftye^{z_{illegal}} \\to 0经过上式的$masking$我们使无效区域经过$softmax$计算之后还几乎为$0$, 这样就避免了无效区域参与计算. 3. $Layer \\ Normalization$和残差连接.1). 残差连接:我们在上一步得到了经过注意力矩阵加权之后的$V$, 也就是$Attention(Q, \\ K, \\ V)$, 我们对它进行一下转置, 使其和$X_{embedding}$的维度一致, 也就是$[batch \\ size, \\ sequence \\ length, \\ embedding \\ dimension]$, 然后把他们加起来做残差连接, 直接进行元素相加, 因为他们的维度一致: X_{embedding} + Attention(Q, \\ K, \\ V)在之后的运算里, 每经过一个模块的运算, 都要把运算之前的值和运算之后的值相加, 从而得到残差连接, 训练的时候可以使梯度直接走捷径反传到最初始层: X + SubLayer(X) \\tag{eq. 5}2). $LayerNorm$:$Layer Normalization$的作用是把神经网络中隐藏层归一为标准正态分布, 也就是$i.i.d$独立同分布, 以起到加快训练速度, 加速收敛的作用: \\mu_{i}=\\frac{1}{m} \\sum^{m}_{i=1}x_{ij}上式中以矩阵的行$(row)$为单位求均值; \\sigma^{2}_{j}=\\frac{1}{m} \\sum^{m}_{i=1} (x_{ij}-\\mu_{j})^{2}上式中以矩阵的行$(row)$为单位求方差; LayerNorm(x)=\\alpha \\odot \\frac{x_{ij}-\\mu_{i}} {\\sqrt{\\sigma^{2}_{i}+\\epsilon}} + \\beta \\tag{eq.6}然后用每一行的每一个元素减去这行的均值, 再除以这行的标准差, 从而得到归一化后的数值, $\\epsilon$是为了防止除$0$;之后引入两个可训练参数$\\alpha, \\ \\beta$来弥补归一化的过程中损失掉的信息, 注意$\\odot$表示元素相乘而不是点积, 我们一般初始化$\\alpha$为全$1$, 而$\\beta$为全$0$. 4. $transformer \\ encoder$整体结构.经过上面3个步骤, 我们已经基本了解到来$transformer$编码器的主要构成部分, 我们下面用公式把一个$transformer \\ block$的计算过程整理一下:1). 字向量与位置编码: X = EmbeddingLookup(X) + PositionalEncoding \\tag{eq.2}X \\in \\mathbb{R}^{batch \\ size \\ * \\ seq. \\ len. \\ * \\ embed. \\ dim.}2). 自注意力机制: Q = Linear(X) = XW_{Q}K = Linear(X) = XW_{K} \\tag{eq.3}V = Linear(X) = XW_{V}X_{attention} = SelfAttention(Q, \\ K, \\ V) \\tag{eq.4}3). 残差连接与$Layer \\ Normalization$ X_{attention} = X + X_{attention} \\tag{eq. 5}X_{attention} = LayerNorm(X_{attention}) \\tag{eq. 6}4). 下面进行$transformer \\ block$结构图中的第4部分, 也就是$FeedForward$, 其实就是两层线性映射并用激活函数激活, 比如说$ReLU$: X_{hidden} = Activate(Linear(Linear(X_{attention}))) \\tag{eq. 7}5). 重复3).: X_{hidden} = X_{attention} + X_{hidden}X_{hidden} = LayerNorm(X_{hidden})X_{hidden} \\in \\mathbb{R}^{batch \\ size \\ * \\ seq. \\ len. \\ * \\ embed. \\ dim.}小结:我们到现在位置已经讲完了transformer的编码器的部分, 了解到了transformer是怎样获得自然语言的位置信息的, 注意力机制是怎样的, 其实举个语言情感分类的例子, 我们已经知道, 经过自注意力机制, 一句话中的每个字都含有这句话中其他所有字的信息, 那么我们可不可以添加一个空白字符到句子最前面, 然后让句子中的所有信息向这个空白字符汇总, 然后再映射成想要分的类别呢? 这就是BERT, 我们下次会讲到.在BERT的预训练中, 我们给每句话的句头加一个特殊字符, 然后句末再加一个特殊字符, 之后模型预训练完毕之后, 我们就可以用句头的特殊字符的$hidden \\ state$完成一些分类任务了.","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Logistic 回归","slug":"Logistic回归","date":"2019-02-04T16:00:00.000Z","updated":"2020-08-05T09:45:24.692Z","comments":true,"path":"2019/02/05/Logistic回归/","link":"","permalink":"http://lucas0625.github.io/blog/2019/02/05/Logistic回归/","excerpt":"Logistic 回归理论与实战","text":"Logistic 回归理论与实战 Logistic 回归模型Sigmoid 函数Sigmoid 函数，公示如下: f(x)=\\frac{1}{1+e^{-x}}图像如下: 损失函数 \\operatorname{loss} =-(y * \\log (\\hat{y})+(1-y) * \\log (1-\\hat{y})) \\operatorname{loss}=-(\\log (1-\\hat{y})) \\operatorname{loss}=-(\\log (\\hat{y}))模型 代码import torchfrom torch.autograd import Variableimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline # 设定随机种子torch.manual_seed(2017) &lt;torch._C.Generator at 0x10d916550&gt; # 从 data.txt 中读入点with open('./Logistic_data.txt', 'r') as f: data_list = [i.split('\\n')[0].split(',') for i in f.readlines()] data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]# 标准化x0_max = max([i[0] for i in data])x1_max = max([i[1] for i in data])data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点plot_x0 = [i[0] for i in x0]plot_y0 = [i[1] for i in x0]plot_x1 = [i[0] for i in x1]plot_y1 = [i[1] for i in x1]plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x112a31f98&gt; np_data = np.array(data, dtype='float32') # 转换成 numpy arrayx_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1] 实现Sigmoid 的函数，Sigmoid 函数的公式为 f(x) = \\frac{1}{1 + e^{-x}}# 定义 sigmoid 函数def sigmoid(x): return 1 / (1 + np.exp(-x)) # 画出 sigmoid 的图像plot_x = np.arange(-10, 10.01, 0.01)plot_y = sigmoid(plot_x)plt.plot(plot_x, plot_y, 'r') [&lt;matplotlib.lines.Line2D at 0x115df7fd0&gt;] x_data = Variable(x_data)y_data = Variable(y_data) # 定义 logistic 回归模型w = Variable(torch.randn(2, 1), requires_grad=True) b = Variable(torch.zeros(1), requires_grad=True)def logistic_regression(x): return torch.sigmoid(torch.mm(x, w) + b) # 画出参数更新之前的结果w0 = w[0].data[0]w1 = w[1].data[0]b0 = b.data[0]plot_x = np.arange(0.2, 1, 0.01)plot_x = torch.from_numpy(plot_x) # 转换成Tensorplot_y = (-w0 * plot_x - b0) / w1plot_x = plot_x.numpy() # 转换成numpyplot_y = plot_y.numpy() # 转换成numpyplt.plot(plot_x, plot_y, 'g', label='cutting line')plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x11686cc18&gt; 可以看到分类效果基本是混乱的，我们来计算一下 loss，公式如下 loss = -(y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y}))# 计算lossdef binary_loss(y_pred, y): logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean() return -logits y_pred = logistic_regression(x_data)loss = binary_loss(y_pred, y_data)print(loss) tensor(0.7911, grad_fn=&lt;NegBackward&gt;) # 自动求导并更新参数loss.backward()w.data = w.data - 0.1 * w.grad.datab.data = b.data - 0.1 * b.grad.data# 算出一次更新之后的lossy_pred = logistic_regression(x_data)loss = binary_loss(y_pred, y_data)print(loss) tensor(0.7801, grad_fn=&lt;NegBackward&gt;) # 使用 torch.optim 更新参数from torch import nnw = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))optimizer = torch.optim.SGD([w, b], lr=1.) # 进行 1000 次更新import timestart = time.time()for e in range(1000): # 前向传播 y_pred = logistic_regression(x_data) loss = binary_loss(y_pred, y_data) # 计算 loss # 反向传播 optimizer.zero_grad() # 使用优化器将梯度归 0 loss.backward() optimizer.step() # 使用优化器来更新参数 # 计算正确率 mask = y_pred.ge(0.5).float() acc = (mask == y_data).sum().data.item() / y_data.shape[0] if (e + 1) % 200 == 0: print('epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;'.format(e+1, loss.data.item(), acc))during = time.time() - startprint()print('During Time: &#123;:.3f&#125; s'.format(during)) epoch: 200, Loss: 0.32431, Acc: 0.91000 epoch: 400, Loss: 0.29052, Acc: 0.91000 epoch: 600, Loss: 0.27069, Acc: 0.91000 epoch: 800, Loss: 0.25759, Acc: 0.90000 epoch: 1000, Loss: 0.24827, Acc: 0.89000 During Time: 0.304 s # 画出更新之后的结果w0 = w[0].data[0]w1 = w[1].data[0]b0 = b.data[0]plot_x = np.arange(0.2, 1, 0.01)plot_x = torch.from_numpy(plot_x) # 转换成Tensorplot_y = (-w0 * plot_x - b0) / w1plot_x = plot_x.numpy() # 转换成numpyplot_y = plot_y.numpy() # 转换成numpyplt.plot(plot_x, plot_y, 'g', label='cutting line')plt.plot(plot_x0, plot_y0, 'ro', label='x_0')plt.plot(plot_x1, plot_y1, 'bo', label='x_1')plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x116526ef0&gt; 前面我们使用了自己写的 loss，其实 PyTorch 已经为我们写好了一些常见的 loss，比如线性回归里面的 loss 是 nn.MSE()，而 Logistic 回归的二分类 loss 在 PyTorch 中是 nn.BCEWithLogitsLoss()，关于更多的 loss，可以查看文档 PyTorch 为我们实现的 loss 函数有两个好处，第一是方便我们使用，不需要重复造轮子，第二就是其实现是在底层 C++ 语言上的，所以速度上和稳定性上都要比我们自己实现的要好 另外，PyTorch 出于稳定性考虑，将模型的 Sigmoid 操作和最后的 loss 都合在了 nn.BCEWithLogitsLoss()，所以我们使用 PyTorch 自带的 loss 就不需要再加上 Sigmoid 操作了 # 使用自带的losscriterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性w = nn.Parameter(torch.randn(2, 1))b = nn.Parameter(torch.zeros(1))def logistic_reg(x): return torch.mm(x, w) + boptimizer = torch.optim.SGD([w, b], 1.) y_pred = logistic_reg(x_data)loss = criterion(y_pred, y_data)print(loss.data) tensor(0.6650) # 同样进行 1000 次更新start = time.time()for e in range(1000): # 前向传播 y_pred = logistic_reg(x_data) loss = criterion(y_pred, y_data) # 反向传播 optimizer.zero_grad() loss.backward() optimizer.step() # 计算正确率 mask = y_pred.ge(0.5).float() acc = (mask == y_data).sum().data.item() / y_data.shape[0] if (e + 1) % 200 == 0: print('epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;'.format(e+1, loss.data.item(), acc))during = time.time() - startprint()print('During Time: &#123;:.3f&#125; s'.format(during)) epoch: 200, Loss: 0.39010, Acc: 0.87000 epoch: 400, Loss: 0.32184, Acc: 0.87000 epoch: 600, Loss: 0.28917, Acc: 0.87000 epoch: 800, Loss: 0.26983, Acc: 0.87000 epoch: 1000, Loss: 0.25700, Acc: 0.88000 During Time: 0.202 s 可以看到，使用了 PyTorch 自带的 loss 之后，速度有了一定的上升，虽然看上去速度的提升并不多，但是这只是一个小网络，对于大网络，使用自带的 loss 不管对于稳定性还是速度而言，都有质的飞跃，同时也避免了重复造轮子的困扰","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"Pytorch 基本概念","slug":"基本概念","date":"2019-02-04T16:00:00.000Z","updated":"2020-08-05T09:45:24.726Z","comments":true,"path":"2019/02/05/基本概念/","link":"","permalink":"http://lucas0625.github.io/blog/2019/02/05/基本概念/","excerpt":"本文介绍Pytorch的基本概念。","text":"本文介绍Pytorch的基本概念。 Pytorch 基本概念import torchimport numpy as npimport pandas as pdfrom torch import nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoader torch.cuda.is_available() False Tensor 张量表示的是一个多维矩阵，零维就是一个点，一维就是向量，二维就是一般的矩阵，多维就相当于一个多维矩阵，Tensor可以和numpy的ndarray相互对应，Tensor可以和ndarray相互转换 a = torch.Tensor([[2, 3], [4, 8], [7, 9]])print('a is : &#123;&#125;'.format(a))print('a size is &#123;&#125;'.format(a.size())) a is : tensor([[2., 3.], [4., 8.], [7., 9.]]) a size is torch.Size([3, 2]) b = torch.LongTensor([[2, 3], [4, 8],[7, 9]])print('b is : &#123;&#125;'.format(b)) b is : tensor([[2, 3], [4, 8], [7, 9]]) c = torch.zeros((3, 2))print('zero tensor : &#123;&#125;'.format(c))d = torch.randn((3, 2))print('randn tensor is : &#123;&#125;'.format(d)) zero tensor : tensor([[0., 0.], [0., 0.], [0., 0.]]) randn tensor is : tensor([[-0.7097, -0.2020], [-0.1451, -0.3853], [-0.9366, 1.1942]]) a[0, 1] = 100print('changed a is : &#123;&#125;'.format(a)) changed a is : tensor([[ 2., 100.], [ 4., 8.], [ 7., 9.]]) numpy_b = b.numpy()print('conver to numpy is \\n &#123;&#125;'.format(numpy_b)) conver to numpy is [[2 3] [4 8] [7 9]] e = np.array([[2, 3], [4, 5]])torch_e = torch.from_numpy(e)print('from numpy to touch.Tensor is &#123;&#125;'.format(torch_e))f_torch_e = torch_e.float()print('change data type to float tensor : &#123;&#125;'.format(f_torch_e)) from numpy to touch.Tensor is tensor([[2, 3], [4, 5]]) change data type to float tensor : tensor([[2., 3.], [4., 5.]]) Variable 变量variable 提供了自动求导功能，Variable和Tensor没有本质区别，不过Variable会被放入一个计算图中，然后进行前向传播，反向传播，自动求导 Variable有三个比较重要的组成属性：data， grad， grad_fn data： 可以取出Variable中的Tensor数值 grad_fn： 表示的是得到这个Variable的操作，比如加减或者乘除 grad： 是这个Variable的反向传播梯度 # creat Variablex = Variable(torch.Tensor([1]), requires_grad=True)w = Variable(torch.Tensor([2]), requires_grad=True)b = Variable(torch.Tensor([3]), requires_grad=True)# build a computational graphy = w * x + b# compute gradientsy.backward()print(x.grad)print(w.grad)print(b.grad) tensor([2.]) tensor([1.]) tensor([1.]) # 矩阵求导x = torch.randn(3)x = Variable(x, requires_grad=True)y = x * 2print(y)y.backward(torch.FloatTensor([1, 0.1, 0.01]))print(x.grad) tensor([-0.6298, 1.7484, -1.1588], grad_fn=&lt;MulBackward0&gt;) tensor([2.0000, 0.2000, 0.0200]) Dataset 数据集torch.utils.data.Dataset 代表这一数据的抽象类吗可以自己定义数据类的继承和重写，只需要定义_len_和 _getitem_ 两个函数 class myDataset(Dataset): def __init__(self, csv_file, txt_file, root_dir, other_file): self.csv_data = pd.read_csv(csv_file) with open(txt_file, 'r') as f: data_list = f.readlines() self.txt_data = data_list self.root_dir = root_dir def __len__(self): return len(self.csv_data) def __getitem__(self, idx): data = (self.csv_file[idx], self.txt_file[idx]) return data # DataLoader 来定义一个新的迭代器# dataiter = DataLoader(myDataset, batch_size=32, shuffle=True, collate_fn=default_collate) nn.Module 模组PyTorch 编写的神经网络，所有层结构和损失函数都来自于torch.nn，所有的模型构建都从这个基类nn.Module继承 # 构建计算图 也就是模型class net_name(nn.Module): def __init__(self, other_arguments): super(net_name, self).__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size) # other network layer def forward(self, x): x = self.conv1(x) return x # 定义损失函数criterion = nn.CrossEntropyLoss()# loss = criterion(output, target) torch.optim 优化在机器学习或者深度学习中，我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略 优化算法分类： 一阶优化算法 梯度下降 二阶优化算法 使用二阶倒数，也叫hessian方法 # lr 学习率 momentum 动量# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 模型的保存和加载两种保存方式： 保存整个模型的结构信息和参数信息，保存的对象是模型model 保存模型的参数，保存的对象是模型的状态model.state_dict() 两种加载方式： 加载完整的模型结构和参数信息，网络较大的时候记载时间比较长，同时存储空间也比较大 加载模型参数信息，需要先导入模型的结构，然后再导入模型 # 保存# torch.save(model, './model.path')# torch.save(model.state_dict(), './model_state.path')# 加载# load_model = torch.load('model.path')# 先加载模型，再加载参数# model.load_state_dict(torch.load('model_state.path'))","categories":[{"name":"NLP","slug":"NLP","permalink":"http://lucas0625.github.io/blog/categories/NLP/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://lucas0625.github.io/blog/tags/PyTorch/"}]},{"title":"爬虫基础","slug":"爬虫基础","date":"2019-02-03T16:00:00.000Z","updated":"2019-02-04T09:17:45.054Z","comments":true,"path":"2019/02/04/爬虫基础/","link":"","permalink":"http://lucas0625.github.io/blog/2019/02/04/爬虫基础/","excerpt":"本文介绍爬虫的一些基础知识。","text":"本文介绍爬虫的一些基础知识。 爬虫基本原理讲解什么是爬虫自动获取网络资源机器人。请求网站并提取数据的自动化程序。获取的是html代码，所需要的数据保存在html代码中，接下来就是从html中提取出想要的信息，然后将信息存储在数据库。 爬虫的基本流程 发起请求通过HTTP库像目标站点发起请求，即发送一个Request，请求包含额外的headers等信息，等待服务器响应。 获取响应内容如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。 解析内容得到的内容可能是HTML，可以用正则表达式，网页解析库进行解析，可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。 保存数据保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。 Request中包含的内容 请求方式主要有GET，POST两种类型，另外还有HEAD，PUT，DELETE，OPTIONS等。GET请求： 请求的参数都包含在请求网址里面，即包含在Request url中；可以直接输入url然后回车直接访问。POST请求：请求的参数包含在Form Data中，不包含在请求网址中；必须构建表单，点击表单提交，才可以构造一个POST请求。 请求URLURL全称是统一资源定位符，如一个网页文档，一张图片，一个视频等都可以用URL唯一来确定。 请求头包含请求时的头部信息（配置信息），如User-Agent，Host，Cookies等信息。 请求体请求时额外携带的数据，如表单提交时的表单数据。GET请求时一般不包含，POST请求时加入Form Data信息 Response中包含的内容 响应状态Status Code，有多种响应状态，如200代表成功，301跳转，404找不到页面，502服务器错误。 响应头如内容类型，内容长度，服务器信息，设置Cookie等等。 响应体最主要的部分，包含了请求资源的内容，如网页HTML，图片，二进制数据等。 爬虫可以抓取什么样的数据 网页文本如HTML文档，Json格式文本等。 图片获取到的是二进制文件，保存为图片格式。 视频同为二进制文件，保存为视频格式即可。4.其他只要是能请求到的，都能获取。 怎么进行解析 直接处理返回最简单的字符串，可以对字符串直接进行处理。这种处理的网站比较简单。 Json解析XHR标签，解析json 正则表达式 Beautifulsoup解析 PyQuery解析 XPath解析 为什么我抓到的数据和浏览器看到的不一样抓到的只是网页源代码，浏览器看得到代码是通过JS渲染过的 解决JavaScript渲染问题 分析Ajax请求返回的结果是Json格式字符串 使用Selenium/Webdriver驱动一个浏览器 Splash 模拟JavaScript PyV8，Ghost.py进行模拟加载 怎样保存数据 文本存文本，Json，Xml等。 关系型数据库如MySQL，Oracle，SQL Server，等具有结构化表结构形式存储。 非关系型数据库如MongoDB，Redis等key-value形式存储。 环境配置所需环境python3.6, anacondaMongoDB 非关系型数据库。key,value型数据库 环境安装首先安装homebrew,然后利用homebrew安装mongodbbrew install mongodb Redis 非关系型数据库。key,value型数据库，分布式爬虫需要用到。 brew install redis MySQL 关系型数据库，体积小，使用方便，做数据存储时简便易用brew install mysql Python多版本共存 windows:环境变量：where python 可以查看各版本Pythonmac:echo $PATH 输出环境变量 爬虫常用库安装 请求库 urllib urllib.request re requests selenium 驱动浏览器，用来做自动化测试 chromedriver 驱动chrome浏览器 phantomjs 无界面浏览器 解析库 lxml 提供了xpath解析方式 beautifulsoup4 (bs4)网页解析库，依赖于lxml pyquery 网页解析库，相对于bs4更加方便 存储库 pymysql 用Python操作MySQL数据库 pymongo 用Python操作MongoDB数据库 redis 用Python操作Redis数据库 flask web库，做web代理的时候会用到 django web服务器框架，提供了完整的后台管理。做分布式爬虫管理的时候会用到 jupyter 运行在网页上的记事本Urllibk库基本使用Urllib库组成 urllib.request 请求模块,用来请求url urllib.error 异常处理模块，捕捉请求时出现的错误 urllib.parse url解析模块 urllib.robotparse robot.txt解析模块 urlopenurllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) import urllib.requestresponse = urllib.request.urlopen('http://www.baidu.com')print(response.read().decode('utf-8')) import urllib.parseimport urllib.requestdata = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')response = urllib.request.urlopen('http://httpbin.org/post', data=data)print(response.read()) b&#39;{\\n &quot;args&quot;: {}, \\n &quot;data&quot;: &quot;&quot;, \\n &quot;files&quot;: {}, \\n &quot;form&quot;: {\\n &quot;word&quot;: &quot;hello&quot;\\n }, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;1&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Content-Length&quot;: &quot;10&quot;, \\n &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;89667a57-c909-475a-9870-f01181e8c85d&quot;\\n }, \\n &quot;json&quot;: null, \\n &quot;origin&quot;: &quot;219.238.82.169&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/post&quot;\\n}\\n&#39; import urllib.requestresponse = urllib.request.urlopen('http://httpbin.org/get', timeout=1)print(response.read()) b&#39;{\\n &quot;args&quot;: {}, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;0&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;40948f0e-e4b2-4b5f-9d84-aeb77595ca52&quot;\\n }, \\n &quot;origin&quot;: &quot;219.238.82.169&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\\n}\\n&#39; import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)except urllib.error.URLError as e: if isinstance(e.reason, socket.timeout): print('TIME OUT') TIME OUT 响应响应类型import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(type(response)) &lt;class &#39;http.client.HTTPResponse&#39;&gt; 状态码、响应头import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(response.status)print(response.getheaders())print(response.getheader('Server')) 200 [(&#39;Server&#39;, &#39;nginx&#39;), (&#39;Content-Type&#39;, &#39;text/html; charset=utf-8&#39;), (&#39;X-Frame-Options&#39;, &#39;SAMEORIGIN&#39;), (&#39;X-Clacks-Overhead&#39;, &#39;GNU Terry Pratchett&#39;), (&#39;Content-Length&#39;, &#39;47436&#39;), (&#39;Accept-Ranges&#39;, &#39;bytes&#39;), (&#39;Date&#39;, &#39;Wed, 22 Mar 2017 15:40:16 GMT&#39;), (&#39;Via&#39;, &#39;1.1 varnish&#39;), (&#39;Age&#39;, &#39;3417&#39;), (&#39;Connection&#39;, &#39;close&#39;), (&#39;X-Served-By&#39;, &#39;cache-itm7426-ITM&#39;), (&#39;X-Cache&#39;, &#39;HIT&#39;), (&#39;X-Cache-Hits&#39;, &#39;16&#39;), (&#39;X-Timer&#39;, &#39;S1490197216.605863,VS0,VE0&#39;), (&#39;Vary&#39;, &#39;Cookie&#39;), (&#39;Public-Key-Pins&#39;, &#39;max-age=600; includeSubDomains; pin-sha256=&quot;WoiWRyIOVNa9ihaBciRSC7XHjliYS9VwUGOIud4PB18=&quot;; pin-sha256=&quot;5C8kvU039KouVrl52D0eZSGf4Onjo4Khs8tmyTlV3nU=&quot;; pin-sha256=&quot;5C8kvU039KouVrl52D0eZSGf4Onjo4Khs8tmyTlV3nU=&quot;; pin-sha256=&quot;lCppFqbkrlJ3EcVFAkeip0+44VaoJUymbnOaEUk7tEU=&quot;; pin-sha256=&quot;TUDnr0MEoJ3of7+YliBMBVFB4/gJsv5zO7IxD9+YoWI=&quot;; pin-sha256=&quot;x4QzPSC810K5/cMjb05Qm4k3Bw5zBn4lTdO/nEW/Td4=&quot;;&#39;), (&#39;Strict-Transport-Security&#39;, &#39;max-age=63072000; includeSubDomains&#39;)] nginx import urllib.requestresponse = urllib.request.urlopen('https://www.python.org')print(response.read().decode('utf-8')) Requestimport urllib.requestrequest = urllib.request.Request('https://python.org')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) from urllib import request, parseurl = 'http://httpbin.org/post'headers = &#123; 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) { &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: { &quot;name&quot;: &quot;Germey&quot; }, &quot;headers&quot;: { &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connect-Time&quot;: &quot;1&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Total-Route-Time&quot;: &quot;0&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;, &quot;Via&quot;: &quot;1.1 vegur&quot;, &quot;X-Request-Id&quot;: &quot;f96e736e-0b8a-4ab4-9dcc-a970fcd2fbbf&quot; }, &quot;json&quot;: null, &quot;origin&quot;: &quot;219.238.82.169&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot; } from urllib import request, parseurl = 'http://httpbin.org/post'dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, method='POST')req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')response = request.urlopen(req)print(response.read().decode('utf-8')) { &quot;args&quot;: {}, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: {}, &quot;form&quot;: { &quot;name&quot;: &quot;Germey&quot; }, &quot;headers&quot;: { &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Connect-Time&quot;: &quot;0&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Total-Route-Time&quot;: &quot;0&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;, &quot;Via&quot;: &quot;1.1 vegur&quot;, &quot;X-Request-Id&quot;: &quot;a624bcaa-3581-4b93-84b0-037940338e71&quot; }, &quot;json&quot;: null, &quot;origin&quot;: &quot;219.238.82.169&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot; } Handler代理import urllib.requestproxy_handler = urllib.request.ProxyHandler(&#123; 'http': 'http://127.0.0.1:9743', 'https': 'https://127.0.0.1:9743'&#125;)opener = urllib.request.build_opener(proxy_handler)response = opener.open('http://httpbin.org/get')print(response.read()) b&#39;{\\n &quot;args&quot;: {}, \\n &quot;headers&quot;: {\\n &quot;Accept-Encoding&quot;: &quot;identity&quot;, \\n &quot;Connect-Time&quot;: &quot;2&quot;, \\n &quot;Connection&quot;: &quot;close&quot;, \\n &quot;Host&quot;: &quot;httpbin.org&quot;, \\n &quot;Total-Route-Time&quot;: &quot;0&quot;, \\n &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;, \\n &quot;Via&quot;: &quot;1.1 vegur&quot;, \\n &quot;X-Request-Id&quot;: &quot;b0e2272d-1663-4192-ac45-eb958279afd8&quot;\\n }, \\n &quot;origin&quot;: &quot;110.10.176.224&quot;, \\n &quot;url&quot;: &quot;http://httpbin.org/get&quot;\\n}\\n&#39; Cookieimport http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+\"=\"+item.value) BAIDUID=E77BF84491E332F6F8F1D451AD0063D3:FG=1 BIDUPSID=E77BF84491E332F6F8F1D451AD0063D3 H_PS_PSSID=1466_21127_22075 PSTM=1490198051 BDSVRTM=0 BD_HOME=0 import http.cookiejar, urllib.requestfilename = \"cookie.txt\"cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) import http.cookiejar, urllib.requestfilename = 'cookie.txt'cookie = http.cookiejar.LWPCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) import http.cookiejar, urllib.requestcookie = http.cookiejar.LWPCookieJar()cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 异常处理from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.URLError as e: print(e.reason) Not Found from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.HTTPError as e: print(e.reason, e.code, e.headers, sep='\\n')except error.URLError as e: print(e.reason)else: print('Request Successfully') Not Found 404 Server: nginx/1.10.1 Date: Wed, 22 Mar 2017 15:59:55 GMT Content-Type: text/html; charset=UTF-8 Transfer-Encoding: chunked Connection: close Vary: Cookie Expires: Wed, 11 Jan 1984 05:00:00 GMT Cache-Control: no-cache, must-revalidate, max-age=0 Link: &lt;http://cuiqingcai.com/wp-json/&gt;; rel=&quot;https://api.w.org/&quot; import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)except urllib.error.URLError as e: print(type(e.reason)) if isinstance(e.reason, socket.timeout): print('TIME OUT') &lt;class &#39;socket.timeout&#39;&gt; TIME OUT URL解析urlparseurllib.parse.urlparse(urlstring, scheme=’’, allow_fragments=True) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment')print(type(result), result) &lt;class &#39;urllib.parse.ParseResult&#39;&gt; ParseResult(scheme=&#39;http&#39;, netloc=&#39;www.baidu.com&#39;, path=&#39;/index.html&#39;, params=&#39;user&#39;, query=&#39;id=5&#39;, fragment=&#39;comment&#39;) from urllib.parse import urlparseresult = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) ParseResult(scheme=&#39;https&#39;, netloc=&#39;&#39;, path=&#39;www.baidu.com/index.html&#39;, params=&#39;user&#39;, query=&#39;id=5&#39;, fragment=&#39;comment&#39;) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) ParseResult(scheme=&#39;http&#39;, netloc=&#39;www.baidu.com&#39;, path=&#39;/index.html&#39;, params=&#39;user&#39;, query=&#39;id=5&#39;, fragment=&#39;comment&#39;) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html;user?id=5#comment', allow_fragments=False)print(result) ParseResult(scheme=&#39;http&#39;, netloc=&#39;www.baidu.com&#39;, path=&#39;/index.html&#39;, params=&#39;user&#39;, query=&#39;id=5#comment&#39;, fragment=&#39;&#39;) from urllib.parse import urlparseresult = urlparse('http://www.baidu.com/index.html#comment', allow_fragments=False)print(result) ParseResult(scheme=&#39;http&#39;, netloc=&#39;www.baidu.com&#39;, path=&#39;/index.html#comment&#39;, params=&#39;&#39;, query=&#39;&#39;, fragment=&#39;&#39;) urlunparsefrom urllib.parse import urlunparsedata = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']print(urlunparse(data)) http://www.baidu.com/index.html;user?a=6#comment urljoinfrom urllib.parse import urljoinprint(urljoin('http://www.baidu.com', 'FAQ.html'))print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))print(urljoin('http://www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com#comment', '?category=2')) http://www.baidu.com/FAQ.html https://cuiqingcai.com/FAQ.html https://cuiqingcai.com/FAQ.html https://cuiqingcai.com/FAQ.html?question=2 https://cuiqingcai.com/index.php http://www.baidu.com?category=2#comment www.baidu.com?category=2#comment www.baidu.com?category=2 urlencodefrom urllib.parse import urlencodeparams = &#123; 'name': 'germey', 'age': 22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) http://www.baidu.com?name=germey&amp;age=22 request库使用requestsRequests是Python语言编写，基于urllib，采用Apache2 Licensed开源协议的HTTP库。它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。 实例引入import requestsresponse = requests.get('https://www.baidu.com/')print(type(response))print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 各种请求方式import requestsrequests.post('http://httpbin.org/post')requests.put('http://httpbin.org/put')requests.delete('http://httpbin.org/delete')requests.head('http://httpbin.org/get')requests.options('http://httpbin.org/get') 请求基本GET请求基本写法import requestsresponse = requests.get('http://httpbin.org/get')print(response.text) 带参数GET请求import requestsresponse = requests.get(\"http://httpbin.org/get?name=germey&amp;age=22\")print(response.text) import requestsdata = &#123; 'name': 'germey', 'age': 22&#125;response = requests.get(\"http://httpbin.org/get\", params=data)print(response.text) 解析jsonimport requestsimport jsonresponse = requests.get(\"http://httpbin.org/get\")print(type(response.text))print(response.json())print(json.loads(response.text))print(type(response.json())) 获取二进制数据import requestsresponse = requests.get(\"https://github.com/favicon.ico\")print(type(response.text), type(response.content))print(response.text)print(response.content) import requestsresponse = requests.get(\"https://github.com/favicon.ico\")with open('favicon.ico', 'wb') as f: f.write(response.content) f.close() 添加headersimport requestsresponse = requests.get(\"https://www.zhihu.com/explore\")print(response.text) import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;response = requests.get(\"https://www.zhihu.com/explore\", headers=headers)print(response.text) 基本POST请求import requestsdata = &#123;'name': 'germey', 'age': '22'&#125;response = requests.post(\"http://httpbin.org/post\", data=data)print(response.text) import requestsdata = &#123;'name': 'germey', 'age': '22'&#125;headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;response = requests.post(\"http://httpbin.org/post\", data=data, headers=headers)print(response.json()) 响应response属性import requestsresponse = requests.get('http://www.jianshu.com')print(type(response.status_code), response.status_code)print(type(response.headers), response.headers)print(type(response.cookies), response.cookies)print(type(response.url), response.url)print(type(response.history), response.history) 状态码判断import requestsresponse = requests.get('http://www.jianshu.com/hello.html')exit() if not response.status_code == requests.codes.not_found else print('404 Not Found') import requestsresponse = requests.get('http://www.jianshu.com')exit() if not response.status_code == 200 else print('Request Successfully') 100: ('continue',),101: ('switching_protocols',),102: ('processing',),103: ('checkpoint',),122: ('uri_too_long', 'request_uri_too_long'),200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\\\o/', '✓'),201: ('created',),202: ('accepted',),203: ('non_authoritative_info', 'non_authoritative_information'),204: ('no_content',),205: ('reset_content', 'reset'),206: ('partial_content', 'partial'),207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),208: ('already_reported',),226: ('im_used',),# Redirection.300: ('multiple_choices',),301: ('moved_permanently', 'moved', '\\\\o-'),302: ('found',),303: ('see_other', 'other'),304: ('not_modified',),305: ('use_proxy',),306: ('switch_proxy',),307: ('temporary_redirect', 'temporary_moved', 'temporary'),308: ('permanent_redirect', 'resume_incomplete', 'resume',), # These 2 to be removed in 3.0# Client Error.400: ('bad_request', 'bad'),401: ('unauthorized',),402: ('payment_required', 'payment'),403: ('forbidden',),404: ('not_found', '-o-'),405: ('method_not_allowed', 'not_allowed'),406: ('not_acceptable',),407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),408: ('request_timeout', 'timeout'),409: ('conflict',),410: ('gone',),411: ('length_required',),412: ('precondition_failed', 'precondition'),413: ('request_entity_too_large',),414: ('request_uri_too_large',),415: ('unsupported_media_type', 'unsupported_media', 'media_type'),416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),417: ('expectation_failed',),418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),421: ('misdirected_request',),422: ('unprocessable_entity', 'unprocessable'),423: ('locked',),424: ('failed_dependency', 'dependency'),425: ('unordered_collection', 'unordered'),426: ('upgrade_required', 'upgrade'),428: ('precondition_required', 'precondition'),429: ('too_many_requests', 'too_many'),431: ('header_fields_too_large', 'fields_too_large'),444: ('no_response', 'none'),449: ('retry_with', 'retry'),450: ('blocked_by_windows_parental_controls', 'parental_controls'),451: ('unavailable_for_legal_reasons', 'legal_reasons'),499: ('client_closed_request',),# Server Error.500: ('internal_server_error', 'server_error', '/o\\\\', '✗'),501: ('not_implemented',),502: ('bad_gateway',),503: ('service_unavailable', 'unavailable'),504: ('gateway_timeout',),505: ('http_version_not_supported', 'http_version'),506: ('variant_also_negotiates',),507: ('insufficient_storage',),509: ('bandwidth_limit_exceeded', 'bandwidth'),510: ('not_extended',),511: ('network_authentication_required', 'network_auth', 'network_authentication'), 高级操作文件上传import requestsfiles = &#123;'file': open('favicon.ico', 'rb')&#125;response = requests.post(\"http://httpbin.org/post\", files=files)print(response.text) 获取cookieimport requestsresponse = requests.get(\"https://www.baidu.com\")print(response.cookies)for key, value in response.cookies.items(): print(key + '=' + value) 会话维持模拟登录 import requestsrequests.get('http://httpbin.org/cookies/set/number/123456789')response = requests.get('http://httpbin.org/cookies')print(response.text) import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/number/123456789')response = s.get('http://httpbin.org/cookies')print(response.text) 证书验证import requestsresponse = requests.get('https://www.12306.cn')print(response.status_code) import requestsfrom requests.packages import urllib3urllib3.disable_warnings()response = requests.get('https://www.12306.cn', verify=False)print(response.status_code) import requestsresponse = requests.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))print(response.status_code) 代理设置import requestsproxies = &#123; \"http\": \"http://127.0.0.1:9743\", \"https\": \"https://127.0.0.1:9743\",&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) import requestsproxies = &#123; \"http\": \"http://user:password@127.0.0.1:9743/\",&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) pip3 install 'requests[socks]' import requestsproxies = &#123; 'http': 'socks5://127.0.0.1:9742', 'https': 'socks5://127.0.0.1:9742'&#125;response = requests.get(\"https://www.taobao.com\", proxies=proxies)print(response.status_code) 超时设置import requestsfrom requests.exceptions import ReadTimeouttry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout') 认证设置import requestsfrom requests.auth import HTTPBasicAuthr = requests.get('http://120.27.34.24:9001', auth=HTTPBasicAuth('user', '123'))print(r.status_code) import requestsr = requests.get('http://120.27.34.24:9001', auth=('user', '123'))print(r.status_code) 异常处理import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: response = requests.get(\"http://httpbin.org/get\", timeout = 0.5) print(response.status_code)except ReadTimeout: print('Timeout')except ConnectionError: print('Connection error')except RequestException: print('Error') Connection error selenium使用Selenium自动化测试工具，支持多种浏览器。爬虫中主要来解决JavaScript渲染问题。 基本使用from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com') input = browser.find_element_by_id('kw') input.send_keys('Python') input.send_keys(Keys.ENTER) wait = WebDriverWait(browser, 10) wait.until(EC.presence_of_element_located((By.ID, 'content_left'))) print(browser.current_url) print(browser.get_cookies()) print(browser.page_source)finally: browser.close() 声明浏览器对象from selenium import webdriverbrowser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.Edge()browser = webdriver.PhantomJS()browser = webdriver.Safari() 访问页面from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')print(browser.page_source)browser.close() 查找元素单个元素from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input_first = browser.find_element_by_id('q')input_second = browser.find_element_by_css_selector('#q')input_third = browser.find_element_by_xpath('//*[@id=\"q\"]')print(input_first, input_second, input_third)browser.close() &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;5e53d9e1c8646e44c14c1c2880d424af&quot;, element=&quot;0.5649563096161541-1&quot;)&gt; find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input_first = browser.find_element(By.ID, 'q')print(input_first)browser.close() &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;1f209c0d11551c40d9d20ad964fef244&quot;, element=&quot;0.07914603542731591-1&quot;)&gt; 多个元素from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.taobao.com')lis = browser.find_elements_by_css_selector('.service-bd li')print(lis)browser.close() [&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;c26290835d4457ebf7d96bfab3740d19&quot;, element=&quot;0.09221044033125603-16&quot;)&gt;] from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get('https://www.taobao.com')lis = browser.find_elements(By.CSS_SELECTOR, '.service-bd li')print(lis)browser.close() [&lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-1&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-2&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-3&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-4&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-5&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-6&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-7&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-8&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-9&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-10&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-11&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-12&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-13&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-14&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-15&quot;)&gt;, &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;bca1503cd36be550e8dba984b55c5d0e&quot;, element=&quot;0.7914623408963901-16&quot;)&gt;] find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 元素交互操作对获取的元素调用交互方法 from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input = browser.find_element_by_id('q')input.send_keys('iPhone')time.sleep(1)input.clear()input.send_keys('iPad')button = browser.find_element_by_class_name('btn-search')button.click() 更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement 交互动作将动作附加到动作链中串行执行 from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform() 更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains 执行JavaScriptfrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert(\"To Bottom\")') 获取元素信息获取属性from selenium import webdriverfrom selenium.webdriver import ActionChainsbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)logo = browser.find_element_by_id('zh-top-link-logo')print(logo)print(logo.get_attribute('class')) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;e08c0f28d7f44d75ccd50df6bb676104&quot;, element=&quot;0.7236390660048155-1&quot;)&gt; zu-top-link-logo 获取文本值from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.text) 提问 获取ID、位置、标签名、大小from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.id)print(input.location)print(input.tag_name)print(input.size) 0.6822924344980397-1 {&#39;y&#39;: 7, &#39;x&#39;: 774} button {&#39;height&#39;: 32, &#39;width&#39;: 66} Frameimport timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionbrowser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')print(source)try: logo = browser.find_element_by_class_name('logo')except NoSuchElementException: print('NO LOGO')browser.switch_to.parent_frame()logo = browser.find_element_by_class_name('logo')print(logo)print(logo.text) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.44746093888932004-1&quot;)&gt; NO LOGO &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;4bb8ac03ced4ecbdefef03ffdc0e4ccd&quot;, element=&quot;0.13792611320464965-2&quot;)&gt; RUNOOB.COM 等待隐式等待当使用了隐式等待执行测试的时候，如果 WebDriver没有在 DOM中找到元素，将继续等待，超出设定时间后则抛出找不到元素的异常, 换句话说，当查找元素或元素并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是0 from selenium import webdriverbrowser = webdriver.Chrome()browser.implicitly_wait(10)browser.get('https://www.zhihu.com/explore')input = browser.find_element_by_class_name('zu-top-add-question')print(input) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;b29214772d59e912f1ac52e96ed29abe&quot;, element=&quot;0.12886805191194894-1&quot;)&gt; 显式等待from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECbrowser = webdriver.Chrome()browser.get('https://www.taobao.com/')wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, 'q')))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))print(input, button) &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-1&quot;)&gt; &lt;selenium.webdriver.remote.webelement.WebElement (session=&quot;07dd2fbc2d5b1ce40e82b9754aba8fa8&quot;, element=&quot;0.5642646294074107-2&quot;)&gt; title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, ‘p’) visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现Alert 详细内容：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions 前进后退import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com/')browser.get('https://www.taobao.com/')browser.get('https://www.python.org/')browser.back()time.sleep(1)browser.forward()browser.close() Cookiesfrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')print(browser.get_cookies())browser.add_cookie(&#123;'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'&#125;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies()) [{&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;l_cap_id&#39;, &#39;expiry&#39;: 1494196091.403418}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;YWEyOGY4MmI1MzQ2NGY5MmFiMjgzZGUzZWJjYTgwYjY=|1491604091|ff946847ddb5881245bdb7a5e6401b70dc61013f&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;cap_id&#39;, &#39;expiry&#39;: 1494196091.402855}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;l_n_c&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;MjcxMDE3YzU1YjI4NDljZjljNTQ4ZDIyOWJjZTBhNmY=|1491604091|8da4722b56a1545c2020dba97394a220c0eca8d9&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;r_cap_id&#39;, &#39;expiry&#39;: 1494196091.402525}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmc&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;AADCo7e1kguPTqvEOMieRUzwkA7ZUBhV-VY=|1491604091&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;d_c0&#39;, &#39;expiry&#39;: 1586212091.344773}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.1491604091.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmz&#39;, &#39;expiry&#39;: 1507372091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;3cc99fc5-8706-43fc-90ac-3ad991bd1a25&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;_zap&#39;, &#39;expiry&#39;: 1554676091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;97cb00128ccb46659728f7c69cc191b0|1491604091000|1491604091000&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;q_c1&#39;, &#39;expiry&#39;: 1586212091.401644}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.2.10.1491604091&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmb&#39;, &#39;expiry&#39;: 1491605891}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;n_c&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.000--|3=entry_date=20170408=1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmv&#39;, &#39;expiry&#39;: 1554676091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.669300758.1491604091.1491604091.1491604091.1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utma&#39;, &#39;expiry&#39;: 1554676091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmt&#39;, &#39;expiry&#39;: 1491604691}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;AQAAALCZuwh+dgIAeu3PPHA+csDPnXvT&#39;, &#39;domain&#39;: &#39;www.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: True, &#39;name&#39;: &#39;aliyungf_tc&#39;}] [{&#39;secure&#39;: False, &#39;value&#39;: &#39;germey&#39;, &#39;domain&#39;: &#39;.www.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;name&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;l_cap_id&#39;, &#39;expiry&#39;: 1494196091.403418}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;YWEyOGY4MmI1MzQ2NGY5MmFiMjgzZGUzZWJjYTgwYjY=|1491604091|ff946847ddb5881245bdb7a5e6401b70dc61013f&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;cap_id&#39;, &#39;expiry&#39;: 1494196091.402855}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;l_n_c&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;MjcxMDE3YzU1YjI4NDljZjljNTQ4ZDIyOWJjZTBhNmY=|1491604091|8da4722b56a1545c2020dba97394a220c0eca8d9&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;r_cap_id&#39;, &#39;expiry&#39;: 1494196091.402525}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmc&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;&quot;AADCo7e1kguPTqvEOMieRUzwkA7ZUBhV-VY=|1491604091&quot;&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;d_c0&#39;, &#39;expiry&#39;: 1586212091.344773}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.1491604091.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmz&#39;, &#39;expiry&#39;: 1507372091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;3cc99fc5-8706-43fc-90ac-3ad991bd1a25&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;_zap&#39;, &#39;expiry&#39;: 1554676091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;97cb00128ccb46659728f7c69cc191b0|1491604091000|1491604091000&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;q_c1&#39;, &#39;expiry&#39;: 1586212091.401644}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.2.10.1491604091&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmb&#39;, &#39;expiry&#39;: 1491605891}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;n_c&#39;}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.000--|3=entry_date=20170408=1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmv&#39;, &#39;expiry&#39;: 1554676091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;51854390.669300758.1491604091.1491604091.1491604091.1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utma&#39;, &#39;expiry&#39;: 1554676091}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;1&#39;, &#39;domain&#39;: &#39;.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__utmt&#39;, &#39;expiry&#39;: 1491604691}, {&#39;secure&#39;: False, &#39;value&#39;: &#39;AQAAALCZuwh+dgIAeu3PPHA+csDPnXvT&#39;, &#39;domain&#39;: &#39;www.zhihu.com&#39;, &#39;path&#39;: &#39;/&#39;, &#39;httpOnly&#39;: True, &#39;name&#39;: &#39;aliyungf_tc&#39;}] [] 选项卡管理import timefrom selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.execute_script('window.open()')print(browser.window_handles)browser.switch_to_window(browser.window_handles[1])browser.get('https://www.taobao.com')time.sleep(1)browser.switch_to_window(browser.window_handles[0])browser.get('https://python.org') [&#39;CDwindow-4f58e3a7-7167-4587-bedf-9cd8c867f435&#39;, &#39;CDwindow-6e05f076-6d77-453a-a36c-32baacc447df&#39;] 异常处理from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.find_element_by_id('hello') --------------------------------------------------------------------------- NoSuchElementException Traceback (most recent call last) &lt;ipython-input-23-978945848a1b&gt; in &lt;module&gt;() 3 browser = webdriver.Chrome() 4 browser.get(&#39;https://www.baidu.com&#39;) ----&gt; 5 browser.find_element_by_id(&#39;hello&#39;) /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in find_element_by_id(self, id_) 267 driver.find_element_by_id(&#39;foo&#39;) 268 &quot;&quot;&quot; --&gt; 269 return self.find_element(by=By.ID, value=id_) 270 271 def find_elements_by_id(self, id_): /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in find_element(self, by, value) 750 return self.execute(Command.FIND_ELEMENT, { 751 &#39;using&#39;: by, --&gt; 752 &#39;value&#39;: value})[&#39;value&#39;] 753 754 def find_elements(self, by=By.ID, value=None): /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/webdriver.py in execute(self, driver_command, params) 234 response = self.command_executor.execute(driver_command, params) 235 if response: --&gt; 236 self.error_handler.check_response(response) 237 response[&#39;value&#39;] = self._unwrap_value( 238 response.get(&#39;value&#39;, None)) /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/selenium/webdriver/remote/errorhandler.py in check_response(self, response) 190 elif exception_class == UnexpectedAlertPresentException and &#39;alert&#39; in value: 191 raise exception_class(message, screen, stacktrace, value[&#39;alert&#39;].get(&#39;text&#39;)) --&gt; 192 raise exception_class(message, screen, stacktrace) 193 194 def _value_or_default(self, obj, key, default): NoSuchElementException: Message: no such element: Unable to locate element: {&quot;method&quot;:&quot;id&quot;,&quot;selector&quot;:&quot;hello&quot;} (Session info: chrome=57.0.2987.133) (Driver info: chromedriver=2.27.440174 (e97a722caafc2d3a8b807ee115bfb307f7d2cfd9),platform=Mac OS X 10.12.3 x86_64) from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: print('No Element')finally: browser.close() No Element 详细文档：http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions JsonJson 是一种轻量级的数据交换格式字符串是Json的表现形式符合Json格式的字符串叫Json字符串 Json相比与XML的优点 易于阅读 易于解析 网络传输效率高同时也可以跨语言交换数据 反序列化json.loads() # 将Json字符串格式转换成对应的Python相对应数据格式 序列化json.dumps() #将Python数据格式转换成Json相对应的格式 数据类型转换对应关系 Json Python object dict array kist string str number int number float true True false False null None 相关名词辨析 Json对象：是Javascript中的一中叫法，相对于其他语言其实是没有的。 Json：数据交换的标准格式。 Json字符串：符合Json格式的字符串。 正则表达式正则表达式正则表达式是对字符串操作的一中逻辑公示，就是用事先定义好的一些特定字符，以及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一中过滤逻辑。 常见匹配模式 模式 描述 \\w 匹配字母数字及下划线 \\W 匹配非字母数字下划线 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f]. \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0-9] \\D 匹配任意非数字 \\A 匹配字符串开始 \\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结束 \\G 匹配最后匹配完成的位置 \\n 匹配一个换行符 \\t 匹配一个制表符 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ ... 不在[]中的字符：abc 匹配除了a,b,c之外的字符。 * 匹配0个或多个的表达式。 + 匹配1个或多个的表达式。 ? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 {n} 精确匹配n个前面表达式。 {n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a&#124;b 匹配a或b ( ) 匹配括号内的表达式，也表示一个组 re.matchre.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。re.match(pattern, string, flags=0) 最常规的匹配import recontent = 'Hello 123 4567 World_This is a Regex Demo'print(len(content))result = re.match('^Hello\\s\\d\\d\\d\\s\\d&#123;4&#125;\\s\\w&#123;10&#125;.*Demo$', content)print(result)print(result.group())print(result.span()) 41 &lt;_sre.SRE_Match object; span=(0, 41), match=&#39;Hello 123 4567 World_This is a Regex Demo&#39;&gt; Hello 123 4567 World_This is a Regex Demo (0, 41) 泛匹配import recontent = 'Hello 123 4567 World_This is a Regex Demo'result = re.match('^Hello.*Demo$', content)print(result)print(result.group())print(result.span()) &lt;_sre.SRE_Match object; span=(0, 41), match=&#39;Hello 123 4567 World_This is a Regex Demo&#39;&gt; Hello 123 4567 World_This is a Regex Demo (0, 41) 匹配目标import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^Hello\\s(\\d+)\\sWorld.*Demo$', content)print(result)print(result.group(1))print(result.span()) &lt;_sre.SRE_Match object; span=(0, 40), match=&#39;Hello 1234567 World_This is a Regex Demo&#39;&gt; 1234567 (0, 40) 贪婪匹配import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^He.*(\\d+).*Demo$', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(0, 40), match=&#39;Hello 1234567 World_This is a Regex Demo&#39;&gt; 7 非贪婪匹配import recontent = 'Hello 1234567 World_This is a Regex Demo'result = re.match('^He.*?(\\d+).*Demo$', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(0, 40), match=&#39;Hello 1234567 World_This is a Regex Demo&#39;&gt; 1234567 匹配模式import recontent = '''Hello 1234567 World_Thisis a Regex Demo'''result = re.match('^He.*?(\\d+).*?Demo$', content, re.S)print(result.group(1)) 1234567 转义import recontent = 'price is $5.00'result = re.match('price is $5.00', content)print(result) None import recontent = 'price is $5.00'result = re.match('price is \\$5\\.00', content)print(result) &lt;_sre.SRE_Match object; span=(0, 14), match=&#39;price is $5.00&#39;&gt; 总结：尽量使用泛匹配、使用括号得到匹配目标、尽量使用非贪婪模式、有换行符就用re.S re.searchre.search 扫描整个字符串并返回第一个成功的匹配。 import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'result = re.match('Hello.*?(\\d+).*?Demo', content)print(result) None import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'result = re.search('Hello.*?(\\d+).*?Demo', content)print(result)print(result.group(1)) &lt;_sre.SRE_Match object; span=(13, 53), match=&#39;Hello 1234567 World_This is a Regex Demo&#39;&gt; 1234567 总结：为匹配方便，能用search就不用match 匹配演练import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;&lt;i class=\"fa fa-user\"&gt;&lt;/i&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?active.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)if result: print(result.group(1), result.group(2)) 齐秦 往事随风 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)if result: print(result.group(1), result.group(2)) 任贤齐 沧海一声笑 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''result = re.search('&lt;li.*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html)if result: print(result.group(1), result.group(2)) beyond 光辉岁月 re.findall搜索字符串，以列表形式返回全部能匹配的子串。 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''results = re.findall('&lt;li.*?href=\"(.*?)\".*?singer=\"(.*?)\"&gt;(.*?)&lt;/a&gt;', html, re.S)print(results)print(type(results))for result in results: print(result) print(result[0], result[1], result[2]) [(&#39;/2.mp3&#39;, &#39;任贤齐&#39;, &#39;沧海一声笑&#39;), (&#39;/3.mp3&#39;, &#39;齐秦&#39;, &#39;往事随风&#39;), (&#39;/4.mp3&#39;, &#39;beyond&#39;, &#39;光辉岁月&#39;), (&#39;/5.mp3&#39;, &#39;陈慧琳&#39;, &#39;记事本&#39;), (&#39;/6.mp3&#39;, &#39;邓丽君&#39;, &#39;但愿人长久&#39;)] &lt;class &#39;list&#39;&gt; (&#39;/2.mp3&#39;, &#39;任贤齐&#39;, &#39;沧海一声笑&#39;) /2.mp3 任贤齐 沧海一声笑 (&#39;/3.mp3&#39;, &#39;齐秦&#39;, &#39;往事随风&#39;) /3.mp3 齐秦 往事随风 (&#39;/4.mp3&#39;, &#39;beyond&#39;, &#39;光辉岁月&#39;) /4.mp3 beyond 光辉岁月 (&#39;/5.mp3&#39;, &#39;陈慧琳&#39;, &#39;记事本&#39;) /5.mp3 陈慧琳 记事本 (&#39;/6.mp3&#39;, &#39;邓丽君&#39;, &#39;但愿人长久&#39;) /6.mp3 邓丽君 但愿人长久 import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''results = re.findall('&lt;li.*?&gt;\\s*?(&lt;a.*?&gt;)?(\\w+)(&lt;/a&gt;)?\\s*?&lt;/li&gt;', html, re.S)print(results)for result in results: print(result[1]) [(&#39;&#39;, &#39;一路上有你&#39;, &#39;&#39;), (&#39;&lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;&#39;, &#39;沧海一声笑&#39;, &#39;&lt;/a&gt;&#39;), (&#39;&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;&#39;, &#39;往事随风&#39;, &#39;&lt;/a&gt;&#39;), (&#39;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;&#39;, &#39;光辉岁月&#39;, &#39;&lt;/a&gt;&#39;), (&#39;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;&#39;, &#39;记事本&#39;, &#39;&lt;/a&gt;&#39;), (&#39;&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&#39;, &#39;但愿人长久&#39;, &#39;&lt;/a&gt;&#39;)] 一路上有你 沧海一声笑 往事随风 光辉岁月 记事本 但愿人长久 re.sub替换字符串中每一个匹配的子串后返回替换后的字符串。注意：sub函数第二个参数可以传入一个函数。 import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('\\d+', '', content)print(content) Extra stings Hello World_This is a Regex Demo Extra stings import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('\\d+', 'Replacement', content)print(content) Extra stings Hello Replacement World_This is a Regex Demo Extra stings import recontent = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'content = re.sub('(\\d+)', r'\\1 8910', content)print(content) Extra stings Hello 1234567 8910 World_This is a Regex Demo Extra stings import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;''' import rehtml = '''&lt;div id=\"songs-list\"&gt; &lt;h2 class=\"title\"&gt;经典老歌&lt;/h2&gt; &lt;p class=\"introduction\"&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=\"list\" class=\"list-group\"&gt; &lt;li data-view=\"2\"&gt;一路上有你&lt;/li&gt; &lt;li data-view=\"7\"&gt; &lt;a href=\"/2.mp3\" singer=\"任贤齐\"&gt;沧海一声笑&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"4\" class=\"active\"&gt; &lt;a href=\"/3.mp3\" singer=\"齐秦\"&gt;往事随风&lt;/a&gt; &lt;/li&gt; &lt;li data-view=\"6\"&gt;&lt;a href=\"/4.mp3\" singer=\"beyond\"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt;&lt;a href=\"/5.mp3\" singer=\"陈慧琳\"&gt;记事本&lt;/a&gt;&lt;/li&gt; &lt;li data-view=\"5\"&gt; &lt;a href=\"/6.mp3\" singer=\"邓丽君\"&gt;但愿人长久&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;'''html = re.sub('&lt;a.*?&gt;|&lt;/a&gt;', '', html)print(html)results = re.findall('&lt;li.*?&gt;(.*?)&lt;/li&gt;', html, re.S)print(results)for result in results: print(result.strip()) &lt;div id=&quot;songs-list&quot;&gt; &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt; &lt;p class=&quot;introduction&quot;&gt; 经典老歌列表 &lt;/p&gt; &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt; &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt; &lt;li data-view=&quot;7&quot;&gt; 沧海一声笑 &lt;/li&gt; &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; 往事随风 &lt;/li&gt; &lt;li data-view=&quot;6&quot;&gt;光辉岁月&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt;记事本&lt;/li&gt; &lt;li data-view=&quot;5&quot;&gt; 但愿人长久 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; [&#39;一路上有你&#39;, &#39;\\n 沧海一声笑\\n &#39;, &#39;\\n 往事随风\\n &#39;, &#39;光辉岁月&#39;, &#39;记事本&#39;, &#39;\\n 但愿人长久\\n &#39;] 一路上有你 沧海一声笑 往事随风 光辉岁月 记事本 但愿人长久 re.compile将正则字符串编译成正则表达式对象 将一个正则表达式串编译成正则对象，以便于复用该匹配模式 import recontent = '''Hello 1234567 World_Thisis a Regex Demo'''pattern = re.compile('Hello.*Demo', re.S)result = re.match(pattern, content)#result = re.match('Hello.*Demo', content, re.S)print(result) &lt;_sre.SRE_Match object; span=(0, 40), match=&#39;Hello 1234567 World_This\\nis a Regex Demo&#39;&gt; 实战练习import requestsimport recontent = requests.get('https://book.douban.com/').textpattern = re.compile('&lt;li.*?cover.*?href=\"(.*?)\".*?title=\"(.*?)\".*?more-meta.*?author\"&gt;(.*?)&lt;/span&gt;.*?year\"&gt;(.*?)&lt;/span&gt;.*?&lt;/li&gt;', re.S)results = re.findall(pattern, content)for result in results: url, name, author, date = result author = re.sub('\\s', '', author) date = re.sub('\\s', '', date) print(url, name, author, date) https://book.douban.com/subject/26925834/?icn=index-editionrecommend 别走出这一步 [英]S.J.沃森 2017-1 https://book.douban.com/subject/26953532/?icn=index-editionrecommend 白先勇细说红楼梦 白先勇 2017-2-1 https://book.douban.com/subject/26959159/?icn=index-editionrecommend 岁月凶猛 冯仑 2017-2 https://book.douban.com/subject/26949210/?icn=index-editionrecommend 如果没有今天，明天会不会有昨天？ [瑞士]伊夫·博萨尔特（YvesBossart） 2017-1 https://book.douban.com/subject/27001447/?icn=index-editionrecommend 人类这100年 阿夏 2017-2 https://book.douban.com/subject/26864566/?icn=index-latestbook-subject 眼泪的化学 [澳]彼得·凯里 2017-2 https://book.douban.com/subject/26991064/?icn=index-latestbook-subject 青年斯大林 [英]西蒙·蒙蒂菲奥里 2017-3 https://book.douban.com/subject/26938056/?icn=index-latestbook-subject 带艾伯特回家 [美]霍默·希卡姆 2017-3 https://book.douban.com/subject/26954757/?icn=index-latestbook-subject 乳房 [美]弗洛伦斯·威廉姆斯 2017-2 https://book.douban.com/subject/26956479/?icn=index-latestbook-subject 草原动物园 马伯庸 2017-3 https://book.douban.com/subject/26956018/?icn=index-latestbook-subject 贩卖音乐 [美]大卫·伊斯曼 2017-3-1 https://book.douban.com/subject/26703649/?icn=index-latestbook-subject 被占的宅子 [阿根廷]胡利奥·科塔萨尔 2017-3 https://book.douban.com/subject/26578402/?icn=index-latestbook-subject 信仰与观看 [法]罗兰·雷希特(RolandRecht) 2017-2-17 https://book.douban.com/subject/26939171/?icn=index-latestbook-subject 妹妹的坟墓 [美]罗伯特·杜格尼(RobertDugoni) 2017-3-1 https://book.douban.com/subject/26972465/?icn=index-latestbook-subject 全栈市场人 Lydia 2017-2-1 https://book.douban.com/subject/26986928/?icn=index-latestbook-subject 终极X战警2 [英]马克·米勒&amp;nbsp;/&amp;nbsp;[美]亚当·库伯特 2017-3-15 https://book.douban.com/subject/26948144/?icn=index-latestbook-subject 格调（修订第3版） [美]保罗·福塞尔（PaulFussell） 2017-2 https://book.douban.com/subject/26945792/?icn=index-latestbook-subject 原谅石 [美]洛里·斯皮尔曼 2017-2 https://book.douban.com/subject/26974207/?icn=index-latestbook-subject 庇护二世闻见录 [意]皮科洛米尼 2017-2 https://book.douban.com/subject/26983143/?icn=index-latestbook-subject 遇见野兔的那一年 [芬]阿托·帕西林纳 2017-3-1 https://book.douban.com/subject/26976429/?icn=index-latestbook-subject 鲍勃·迪伦：诗人之歌 [法]让-多米尼克·布里埃 2017-4 https://book.douban.com/subject/26962860/?icn=index-latestbook-subject 牙医谋杀案 [英]阿加莎·克里斯蒂 2017-3 https://book.douban.com/subject/26923022/?icn=index-latestbook-subject 石挥谈艺录：把生命交给舞台 石挥 2017-2 https://book.douban.com/subject/26897190/?icn=index-latestbook-subject 理想 [美]安·兰德 2017-2 https://book.douban.com/subject/26985981/?icn=index-latestbook-subject 青苔不会消失 袁凌 2017-4 https://book.douban.com/subject/26984949/?icn=index-latestbook-subject 地下铁道 [美]科尔森·怀特黑德（ColsonWhitehead） 2017-3 https://book.douban.com/subject/26944012/?icn=index-latestbook-subject 极简进步史 [英]罗纳德·赖特 2017-4-1 https://book.douban.com/subject/26969002/?icn=index-latestbook-subject 驻马店伤心故事集 郑在欢 2017-2 https://book.douban.com/subject/26854223/?icn=index-latestbook-subject 致薇拉 [美]弗拉基米尔·纳博科夫 2017-3 https://book.douban.com/subject/26841616/?icn=index-latestbook-subject 北方档案 [法]玛格丽特·尤瑟纳尔 2017-2 https://book.douban.com/subject/26980391/?icn=index-latestbook-subject 食帖15：便当灵感集 林江 2017-2 https://book.douban.com/subject/26958882/?icn=index-latestbook-subject 生火 [法]克里斯多夫·夏布特（ChristopheChabouté）编绘 2017-3 https://book.douban.com/subject/26989163/?icn=index-latestbook-subject 文明之光（第四册） 吴军 2017-3-1 https://book.douban.com/subject/26878906/?icn=index-latestbook-subject 公牛山 [美]布赖恩·帕诺威奇 2017-2 https://book.douban.com/subject/26989534/?icn=index-latestbook-subject 几乎消失的偷闲艺术 [加拿大]达尼·拉费里埃 2017-4 https://book.douban.com/subject/26939973/?icn=index-latestbook-subject 散步去 [日]谷口治郎 2017-3 https://book.douban.com/subject/26865333/?icn=index-latestbook-subject 中国1945 [美]理查德·伯恩斯坦(RichardBernstein) 2017-3-1 https://book.douban.com/subject/26989242/?icn=index-latestbook-subject 有匪2：离恨楼 Priest 2017-3 https://book.douban.com/subject/26985790/?icn=index-latestbook-subject 女人、火与危险事物 [美]乔治·莱考夫 2017-3 https://book.douban.com/subject/26972277/?icn=index-latestbook-subject 寻找时间的人 [爱尔兰]凯特·汤普森 2017-3 https://www.douban.com/note/610758170/ 白先勇细说红楼梦【全二册】 白先勇 2017-2-1 https://read.douban.com/ebook/31540864/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 奇爱博士 [英]彼得·乔治 2016-8-1 https://read.douban.com/ebook/31433872/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 在时光中盛开的女子 李筱懿 2017-3 https://read.douban.com/ebook/31178635/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 如何高效记忆（原书第2版） [美]肯尼思•希格比（KennethL.Higbee） 2017-3-5 https://read.douban.com/ebook/31358183/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 愿无岁月可回头 回忆专用小马甲 2016-9 https://read.douban.com/ebook/31341636/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 走神的艺术与科学 [新西兰]迈克尔·C.科尔巴里斯 2017-3-1 https://read.douban.com/ebook/27621094/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 神秘的量子生命 [英]吉姆•艾尔－哈利利/约翰乔•麦克法登 2016-8 https://read.douban.com/ebook/31221966/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 寻找时间的人 [爱尔兰]凯特·汤普森 2017-3 https://read.douban.com/ebook/31481323/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 山之四季 [日]高村光太郎 2017-1 https://read.douban.com/ebook/31154855/?dcs=book-hot&amp;amp;dcm=douban&amp;amp;dct=read-subject 东北游记 [美]迈克尔·麦尔 2017-1 BeautifulSoup 解析库BeautifulSoup灵活又方便的网页解析库，处理高效，支持多种解析器，利用它不用编写正则表达式即可方便的实现网页信息的提取。 解析库 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, “html.parser”) Python的内置标准库、执行速度适中 、文档容错能力强 Python 2.7.3 or 3.2.2)前的版本中文容错能力差 lxml HTML 解析器 BeautifulSoup(markup, “lxml”) 速度快、文档容错能力强 需要安装C语言库 lxml XML 解析器 BeautifulSoup(markup, “xml”) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, “html5lib”) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢、不依赖外部扩展 基本使用html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string) &lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#39;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt; &lt;b&gt; The Dormouse&#39;s story &lt;/b&gt; &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;!-- Elsie --&gt; &lt;/a&gt; , &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt; Lacie &lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt; Tillie &lt;/a&gt; ; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt; ... &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; The Dormouse&#39;s story 标签选择器选择元素html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.head)print(soup.p) &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt; &lt;class &#39;bs4.element.Tag&#39;&gt; &lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt; 获取名称html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title.name) title 获取属性html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.attrs['name'])print(soup.p['name']) dromouse dromouse 获取内容html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p clss=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.string) The Dormouse&#39;s story 嵌套选择html = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.head.title.string) The Dormouse&#39;s story 子节点和子孙节点html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.contents) [&#39;\\n Once upon a time there were three little sisters; and their names were\\n &#39;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt;, &#39;\\n&#39;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &#39; \\n and\\n &#39;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;, &#39;\\n and they lived at the bottom of a well.\\n &#39;] html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.children)for i, child in enumerate(soup.p.children): print(i, child) &lt;list_iterator object at 0x1064f7dd8&gt; 0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; 2 3 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; 4 and 5 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; 6 and they lived at the bottom of a well. html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.descendants)for i, child in enumerate(soup.p.descendants): print(i, child) &lt;generator object descendants at 0x10650e678&gt; 0 Once upon a time there were three little sisters; and their names were 1 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; 2 3 &lt;span&gt;Elsie&lt;/span&gt; 4 Elsie 5 6 7 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; 8 Lacie 9 and 10 &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; 11 Tillie 12 and they lived at the bottom of a well. 父节点和祖先节点html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.a.parent) &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(list(enumerate(soup.a.parents))) [(0, &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;), (1, &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;), (2, &lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;), (3, &lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;)] 兄弟节点html = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"story\"&gt; Once upon a time there were three little sisters; and their names were &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and &lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\"from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(list(enumerate(soup.a.next_siblings)))print(list(enumerate(soup.a.previous_siblings))) [(0, &#39;\\n&#39;), (1, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;), (2, &#39; \\n and\\n &#39;), (3, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;), (4, &#39;\\n and they lived at the bottom of a well.\\n &#39;)] [(0, &#39;\\n Once upon a time there were three little sisters; and their names were\\n &#39;)] 标准选择器find_all( name , attrs , recursive , text , **kwargs )可根据标签名、属性、内容查找文档 namehtml='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all('ul'))print(type(soup.find_all('ul')[0])) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;, &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;/ul&gt;] &lt;class &#39;bs4.element.Tag&#39;&gt; html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.find_all('ul'): print(ul.find_all('li')) [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] attrshtml='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\" name=\"elements\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(attrs=&#123;'id': 'list-1'&#125;))print(soup.find_all(attrs=&#123;'name': 'elements'&#125;)) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot; name=&quot;elements&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(id='list-1'))print(soup.find_all(class_='element')) [&lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] texthtml='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(text='Foo')) [&#39;Foo&#39;, &#39;Foo&#39;] find( name , attrs , recursive , text , **kwargs )find返回单个元素，find_all返回所有元素 html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find('ul'))print(type(soup.find('ul')))print(soup.find('page')) &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt; &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt; &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;class &#39;bs4.element.Tag&#39;&gt; None find_parents() find_parent()find_parents()返回所有祖先节点，find_parent()返回直接父节点。 find_next_siblings() find_next_sibling()find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。 find_previous_siblings() find_previous_sibling()find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。 find_all_next() find_next()find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点 find_all_previous() 和 find_previous()find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点 CSS选择器通过select()直接传入CSS选择器即可完成选择CSS选择器语法： html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.select('.panel .panel-heading'))print(soup.select('ul li'))print(soup.select('#list-2 .element'))print(type(soup.select('ul')[0])) [&lt;div class=&quot;panel-heading&quot;&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] &lt;class &#39;bs4.element.Tag&#39;&gt; html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul.select('li')) [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;] [&lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;, &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;] 获取属性html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul['id']) print(ul.attrs['id']) list-1 list-1 list-2 list-2 获取内容html='''&lt;div class=\"panel\"&gt; &lt;div class=\"panel-heading\"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class=\"panel-body\"&gt; &lt;ul class=\"list\" id=\"list-1\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;li class=\"element\"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class=\"list list-small\" id=\"list-2\"&gt; &lt;li class=\"element\"&gt;Foo&lt;/li&gt; &lt;li class=\"element\"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for li in soup.select('li'): print(li.get_text()) Foo Bar Jay Foo Bar 总结 推荐使用lxml解析库，必要时使用html.parser 标签选择筛选功能弱但是速度快 建议使用find()、find_all() 查询匹配单个结果或者多个结果 如果对CSS选择器熟悉建议使用select() 记住常用的获取属性和文本值的方法 pyquerypyquery强大又灵活的网页解析库。如果你觉得正则写起来太麻烦，如果你觉得BeautifulSoup语法太难记，如果你熟悉jQuery的语法，那么PyQuery就是你的绝佳选择。主要是使用CSS选择器进行解析。比BeautifulSoup中的select()方法更加强大。 初始化 字符串初始化，直接传入html即可 URL初始化，将URL以参数url传入即可。 文件初始化，将demo.html传入filename即可。 字符串初始化html = '''&lt;div&gt; &lt;ul&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; URL初始化from pyquery import PyQuery as pqdoc = pq(url='http://www.baidu.com')print(doc('head')) &lt;head&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;/&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&quot;/&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; 文件初始化from pyquery import PyQuery as pqdoc = pq(filename='demo.html')print(doc('li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 基本CSS选择器 find() 返回符合条件的所有节点。 children() 返回所有子节点。 parent() 返回某个节点的父节点 parents() 返回某个节点的祖先节点。 siblings() 返回某个节点的兄弟节点。 获取到单个节点，可以直接打印或者转换成字符串。获取到多个节点，需要调用items()方法，将结果变成生成器，然后对其遍历，去除所有结果。 attr() 获取属性。 text() 获取文本。 html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('#container .list li')) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 查找元素子元素html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis) &lt;class &#39;pyquery.pyquery.PyQuery&#39;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;class &#39;pyquery.pyquery.PyQuery&#39;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; lis = items.children()print(type(lis))print(lis) &lt;class &#39;pyquery.pyquery.PyQuery&#39;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; lis = items.children('.active')print(lis) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 父元素html = '''&lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container) &lt;class &#39;pyquery.pyquery.PyQuery&#39;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')parents = items.parents()print(type(parents))print(parents) &lt;class &#39;pyquery.pyquery.PyQuery&#39;&gt; &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; parent = items.parents('.wrap')print(parent) &lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 兄弟元素html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings()) &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings('.active')) &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; 遍历单个元素html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li) &lt;class &#39;generator&#39;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 获取信息获取属性html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.attr('href'))print(a.attr.href) &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; link3.html link3.html 获取文本html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text()) &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; third item 获取HTMLhtml = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(li.html()) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt; DOM操作addClass、removeClasshtml = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.removeClass('active')print(li)li.addClass('active')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; attr、csshtml = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.attr('name', 'link')print(li)li.css('font-size', '14px')print(li) &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot; name=&quot;link&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot; name=&quot;link&quot; style=&quot;font-size: 14px&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; removehtml = '''&lt;div class=\"wrap\"&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)wrap = doc('.wrap')print(wrap.text())wrap.find('p').remove()print(wrap.text()) Hello, World This is a paragraph. Hello, World 其他DOM方法http://pyquery.readthedocs.io/en/latest/api.html 伪类选择器html = '''&lt;div class=\"wrap\"&gt; &lt;div id=\"container\"&gt; &lt;ul class=\"list\"&gt; &lt;li class=\"item-0\"&gt;first item&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0 active\"&gt;&lt;a href=\"link3.html\"&gt;&lt;span class=\"bold\"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1 active\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('li:first-child')print(li)li = doc('li:last-child')print(li)li = doc('li:nth-child(2)')print(li)li = doc('li:gt(2)')print(li)li = doc('li:nth-child(2n)')print(li)li = doc('li:contains(second)')print(li) &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; 更多CSS选择器可以查看http://www.w3school.com.cn/css/index.asp 官方文档http://pyquery.readthedocs.io/ 数据库Python操作三大主流数据库数据库分类 关系型数据库e.g: MariaDB, SQLite, SQLServer, MySQL, PostgreSQL, ORACLE 非关系型数据库e.g: mongoDB, HBASE, redis, CouchDB, Cassandre, Neo4j分类： 文档型 key-value型 列式数据库 图形数据库 MySQL 数据库","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://lucas0625.github.io/blog/categories/爬虫/"}],"tags":[{"name":"基础知识","slug":"基础知识","permalink":"http://lucas0625.github.io/blog/tags/基础知识/"}]},{"title":"常见排序算法","slug":"常见排序算法","date":"2018-12-10T16:00:00.000Z","updated":"2020-08-05T09:44:38.906Z","comments":true,"path":"2018/12/11/常见排序算法/","link":"","permalink":"http://lucas0625.github.io/blog/2018/12/11/常见排序算法/","excerpt":"本文介绍常见的排序算法。包括冒泡排序，插入排序，选择排序。","text":"本文介绍常见的排序算法。包括冒泡排序，插入排序，选择排序。 冒泡排序from typing import List# 冒泡排序 稳定排序算法def bubble_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(len(a)): made_swap = False for j in range(len(a) - i - 1): if a[j] &gt; a[j + 1]: a[j], a[j + 1] = a[j + 1], a[j] made_swap = True if not made_swap: break 注： 空间复杂度O(1)，原地排序算法。 稳定排序算法。 最优时间复杂度O(n), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 插入排序from typing import List# 插入排序 稳定排序算法def insertion_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(1, len(a)): value = a[i] j = i - 1 while j &gt;= 0 and a[j] &gt; value: a[j + 1] = a[j] j -= 1 a[j + 1] = value 注： 空间复杂度O(1)，原地排序算法。 稳定排序算法。 最优时间复杂度O(n), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 选择排序from typing import List# 选择排序 非稳定排序算法def selection_sort(a: List[int]): if len(a) &lt;= 1: return for i in range(len(a)): min_index = i min_val = a[i] for j in range(i, len(a)): if a[j] &lt; min_val: min_val = a[j] min_index = j a[i], a[min_index] = a[min_index], a[i] 注： 空间复杂度O(1)，原地排序算法。 非稳定排序算法。 最优时间复杂度O(n^2), 最坏时间复杂度O(n^2), 平均时间复杂度O(n^2) 归并排序from typing import List# 归并排序 稳定排序算法def merge_sort(a: List[int]): _merge_sort_between(a, 0, len(a) - 1)def _merge_sort_between(a: List[int], low: int, high: int): # The indices are inclusive for both low and high. if low &lt; high: mid = low + (high - low) // 2 _merge_sort_between(a, low, mid) _merge_sort_between(a, mid + 1, high) _merge(a, low, mid, high)def _merge(a: List[int], low: int, mid: int, high: int): # a[low:mid], a[mid+1, high] are sorted. i, j = low, mid + 1 tmp = [] while i &lt;= mid and j &lt;= high: if a[i] &lt;= a[j]: tmp.append(a[i]) i += 1 else: tmp.append(a[j]) j += 1 start = i if i &lt;= mid else j end = mid if i &lt;= mid else high tmp.extend(a[start:end + 1]) a[low:high + 1] = tmp 注： 空间复杂度O(N), 非原地排序算法。 稳定排序算法。 最优时间复杂度O(NlogN), 最坏时间复杂度O(NlogN), 平均时间复杂度O(NlogN)","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"http://lucas0625.github.io/blog/categories/算法与数据结构/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]},{"title":"Python 编码规范","slug":"Python 编码规范","date":"2018-10-08T16:00:00.000Z","updated":"2020-08-05T09:45:24.623Z","comments":true,"path":"2018/10/09/Python 编码规范/","link":"","permalink":"http://lucas0625.github.io/blog/2018/10/09/Python 编码规范/","excerpt":"遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。在团队开发中，使用（尽量）统一的编码风格，还可以降低沟通成本。","text":"遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。在团队开发中，使用（尽量）统一的编码风格，还可以降低沟通成本。 缩紧 不要使用 tab 缩进 使用任何编辑器写 Python，请把一个 tab 展开为 4 个空格 绝对不要混用 tab 和空格，否则容易出现 IndentationError 空格 在 list, dict, tuple, set, 参数列表的 , 后面加一个空格 在 dict 的 : 后面加一个空格 在注释符号 # 后面加一个空格，但是 #!/usr/bin/python 的 # 后不能有空格 操作符两端加一个空格，如 +, -, *, /, |, &amp;, = 接上一条，在参数列表里的 =两端不需要空格 括号((), {}, [])内的两端不需要空格 空行 function 和 class 顶上两个空行 class 的 method 之间一个空行 函数内逻辑无关的段落之间空一行，不要过度使用空行 不要把多个语句写在一行，然后用 ; 隔开 if/for/while 语句中，即使执行语句只有一句，也要另起一行 换行 每一行代码控制在80字符以内 使用\\或()控制换行，举例： def foo(first, second, third, fourth, fifth, sixth, and_some_other_very_long_param): user = User.objects.filter_by(first=first, second=second, third=third) \\ .skip(100).limit(100) \\ .all()text = ('Long strings can be made up ' 'of several shorter strings.') 命名 使用有意义的，英文单词或词组，绝对不要使用汉语拼音 package/module 名中不要出现 - 各种类型的命名规范： import 所有 import 尽量放在文件开头，在 docstring 下面，其他变量定义的上面 不要使用 from foo imort * import 需要分组，每组之间一个空行，每个分组内的顺序尽量采用字典序，分组顺序是： 1.标准库 2.第三方库 3.本项目的 package 和 module 不要使用隐式的相对导入（implicit relative imports），可是使用显示的相对导入（explicit relative imports），如 from ..utils import validator，最好使用全路径导入（absolute imports） 对于不同的 package，一个 import 单独一行，同一个 package/module 下的内容可以写一起 为了避免可能出现的命名冲突，可以使用 as 或导入上一级命名空间 不要出现循环导入(cyclic import) 注释 文档字符串 docstring, 是 package, module, class, method, function 级别的注释，可以通过 __doc__ 成员访问到，注释内容在一对 &quot;&quot;&quot; 符号之间 function, method 的文档字符串应当描述其功能、输入参数、返回值，如果有复杂的算法和实现，也需要写清楚 不要写错误的注释，不要无谓的注释 优先使用英文写注释，英文不好全部写中文，否则更加看不懂 异常 不要轻易使用 try/except except 后面需要指定捕捉的异常，裸露的 except 会捕捉所有异常，意味着会隐藏潜在的问题 可以有多个 except 语句，捕捉多种异常，分别做异常处理 使用 finally 子句来处理一些收尾操作 try/except 里的内容不要太多，只在可能抛出异常的地方使用 从 Exception 而不是 BaseException 继承自定义的异常类 Class(类) 显示的写明父类，如果不是继承自别的类，就继承自 object 类 使用 super 调用父类的方法 支持多继承，即同时有多个父类，建议使用 Mixin 编码建议字符串 使用字符串的 join 方法拼接字符串 使用字符串类型的方法，而不是 string 模块的方法 使用 startswith 和 endswith 方法比较前缀和后缀 使用 format 方法格式化字符串 比较 空的 list, str, tuple, set, dict 和 0, 0.0, None 都是 False 使用 if some_list 而不是 if len(some_list) 判断某个 list 是否为空，其他类型同理 使用 is 和 is not 与单例（如 None）进行比较，而不是用 == 和 != 使用 if a is not None 而不是 if not a is None 用 isinstance 而不是 type 判断类型 不要用 == 和 != 与 True 和 False 比较（除非有特殊情况，如在 sqlalchemy 中可能用到） 使用 in 操作: 用 key in dict 而不是 dict.has_key() 用 set 加速 “存在性” 检查，list 的查找是线性的，复杂度 O(n)，set 底层是 hash table, 复杂度 O(1)，但用 set需要比 list 更多内存空间 其他 使用列表表达式（list comprehension），字典表达式(dict comprehension, Python 2.7+) 和生成器(generator) dict 的 get 方法可以指定默认值，但有些时候应该用 [] 操作，使得可以抛出 KeyError 使用 for item in list 迭代 list, for index, item in enumerate(list) 迭代 list 并获取下标 使用内建函数 sorted 和 list.sort 进行排序 适量使用map, reduce, filter 和 lambda，使用内建的 all, any 处理多个条件的判断 使用 defaultdict (Python 2.5+), Counter(Python 2.7+) 等 “冷门” 但好用的标准库算法和数据结构 使用装饰器(decorator) 使用 with 语句处理上下文 有些时候不要对类型做太过严格的限制，利用 Python 的鸭子类型（Duck Type）特性 使用 logging 记录日志，配置好格式和级别 了解 Python 的 Magic Method：A Guide to Python’s Magic Methods, Python 魔术方法指南 阅读优秀的开源代码，如 Flask 框架, Requests for Humans 不要重复造轮子，查看标准库、PyPi、Github、Google 等使用现有的优秀的解决方案","categories":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://lucas0625.github.io/blog/categories/PYTHON/"}],"tags":[{"name":"经验","slug":"经验","permalink":"http://lucas0625.github.io/blog/tags/经验/"}]},{"title":"深入理解 Python package","slug":"深入理解Python package","date":"2018-10-07T16:00:00.000Z","updated":"2020-08-05T09:45:24.663Z","comments":true,"path":"2018/10/08/深入理解Python package/","link":"","permalink":"http://lucas0625.github.io/blog/2018/10/08/深入理解Python package/","excerpt":"Python 是通过 module 组织代码的，module 即一个 py 文件，module 又是通过 package 来组织的，package 是一个包含 __init__.py 的文件夹，代码，module，package 它们三者的关系就是：module 包含代码，package 至少包含一个为 __init__.py 的 module。 package├── __init__.py├── submodule.py└── subpackage └── __init__.py","text":"Python 是通过 module 组织代码的，module 即一个 py 文件，module 又是通过 package 来组织的，package 是一个包含 __init__.py 的文件夹，代码，module，package 它们三者的关系就是：module 包含代码，package 至少包含一个为 __init__.py 的 module。 package├── __init__.py├── submodule.py└── subpackage └── __init__.py 空的 __init__.py不包含任何代码的 __init__.py 只用来标识一个文件夹是一个 package，而 package 是可以被导出的。 from package import item 此处的 item 可以是 package 中包含的 submodule 或 subpackage。 from package import submodulefrom package import subpackage 不为空 __init__.py如果 __init__.py 不为空，其中包含的任何变量，包括 function、class、variable 以及 任何被导入的 module 都可以通过 package 导出。 from package import item 此处的 item 可以是 __init__.py中的任何变量 package的初始化工作一个 package 被导入，不管在什么时候 __init__.py 中的代码只执行一次。 &gt;&gt;&gt; import packagehello world&gt;&gt;&gt; import package&gt;&gt;&gt; import package&gt;&gt;&gt; 由于 package 被导入时__init__.py 中的可执行代码会被执行，所以小心在 package 中放置你的代码，尽可能消除它们产生的副作用，比如把代码尽可能的进行封装成函数或类。 从package中倒入变量的顺序from package import item import 语句首先检查 item 是否是 __init__.py 中定义的变量，然后检查其是不是一个 subpackage，如果不是再去检查其是不是一个 module，都不是将抛出 ImportError。 在 import item.subitem.subsubitem 语句时，除了最后一个 subsubitem 之外其他 item 都必须是 package，而最后一个 subsubitem 必须是一个 package 或者 module，不能是他前一个 item 定义的 function、class、variable。 使用*导入在 from package import *语句中，如果 __init__.py 中定义了 __all__ 变量，一个 list，仅仅只有这个 list 中定义的 submodule 或者变量将会被导出。 如果__init__.py中没有__all__变量，导出将按照一下规则执行： 此 package 被导入，并且执行 __init__.py 中可被执行的代码 __init__.py 中定义的 variable 被导入 __init__.py 中被显式导入的 module 被导入","categories":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://lucas0625.github.io/blog/categories/PYTHON/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://lucas0625.github.io/blog/tags/基础/"}]},{"title":"python 常用技巧","slug":"json","date":"2018-09-16T08:30:18.409Z","updated":"2020-08-05T09:45:24.617Z","comments":true,"path":"2018/09/16/json/","link":"","permalink":"http://lucas0625.github.io/blog/2018/09/16/json/","excerpt":"记录自己常用的一些代码技巧","text":"记录自己常用的一些代码技巧 格式化显示字符串import jsonjson.dumps(dict, indent=4) and, or 的特殊用法 a and b and c 返回第一个False 对象, 反之返回 c a or b or c 返回第一个True 对象, 反之返回 c 删除特殊字符str.replace(u'\\u200b', '') 将列表每个元素作为参数传入函数a = [1, 2, 3, 4]# 将列表a作为整体传入函数funcdef func(a): ...# 将列表每一个元素作为参数传入函数funcdef func(*a): ... 以字符分割字符串，长度不超过n的数字作为一个字符import redef splice_string(string: str, nums: int = 3) -&gt; str: \"\"\" 按字符分割字符串，不超过 nums 个的数字不切分 :param string: :return: \"\"\" pattern = re.compile('((?&lt;!\\d)(\\d&#123;1,%d&#125;)(?!(\\d+))|(\\S))' % (nums)) result = re.sub(pattern, r'\\1\\t', string) return ' '.join(result.strip().split())res = splice_string('人一天要睡20-24小时，一天24小时有1小时可以娱乐，一年365天这样做，30000块钱就到手了')print(res) &#39;人 一 天 要 睡 20 - 24 小 时 ， 一 天 24 小 时 有 1 小 时 可 以 娱 乐 ， 一 年 365 天 这 样 做 ， 3 0 0 0 0 块 钱 就 到 手 了&#39; 判断奇偶# 方法一if nums % 2 == 1: # 奇数else: # 偶数# 方法二if nums &amp; 1 == 1: # 奇数else: # 偶数 不使用第三个变量交换两个数# 方法一# n ^ n = 0, n ^ 0 = nx = x ^ yy = x ^ yx = x ^ y# 方法二# 采用四则运算a = a + bb = a - ba = a - b 四舍五入技巧a = int(b + 0.5) 输出化多维列表# 初始化长度为 n 的一维列表nums = [0] * n# 初始化 n * m 的二维列表nums = [[0 for _in range(m)] for _ in range(n)]# 初始化 n * m * z 的三维列表nums = [[[0 for _in range(z)] for _ in range(m)] for _ in range(n)] 第三方包的使用bisect-快速给列表插入元素，不改变原来的顺序example： # 基于二分插入数据import bisect # 查看 bisect 结构dir(bisect) [&#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;bisect&#39;, &#39;bisect_left&#39;, &#39;bisect_right&#39;, &#39;insort&#39;, &#39;insort_left&#39;, &#39;insort_right&#39;] # 使用 bisect 模块，必须保证要操作的数组是有序的data = [1, 4, 6] # 给数组插入一个元素，插入后不改变原来顺序bisect.insort(data, 3) data [1, 3, 4, 6] # 这两个函数用来处理遇到重复元素，是选择左插入，还是右插入bisect.insort_left(data, 4)bisect.insort_right(data, 4) data [1, 3, 4, 4, 4, 6] # 用来查找元素应该插入的位置，返回对应位置，不执行插入操作bisect.bisect(data, 2) # 同样是用来处理重复元素print(bisect.bisect_left(data, 4))print(bisect.bisect_right(data, 4)) 2 5","categories":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://lucas0625.github.io/blog/categories/PYTHON/"}],"tags":[{"name":"技巧","slug":"技巧","permalink":"http://lucas0625.github.io/blog/tags/技巧/"}]},{"title":"算法和数据结构","slug":"Algorithms and Data Structures","date":"2018-09-11T13:17:13.800Z","updated":"2020-08-05T09:45:24.732Z","comments":true,"path":"2018/09/11/Algorithms and Data Structures/","link":"","permalink":"http://lucas0625.github.io/blog/2018/09/11/Algorithms and Data Structures/","excerpt":"数据结构数据结构就是关系，数据元素相互之间存在的一种或多种特定关系的集合","text":"数据结构数据结构就是关系，数据元素相互之间存在的一种或多种特定关系的集合 逻辑结构和物理结构逻辑结构： 指数据对象中数据元素之间的相互关系，也是今后最需要关注和讨论的问题 逻辑结构指数据的逻辑结构在计算机中的存储形式 四大逻辑结构： 集合结构： 同属一个集合 线性结构： 一对一 树形结构： 一对多 图形结构： 多对多 物理结构 存储器： 主要针对内存而言，想赢盘，光盘 数据存储结构形式： 顺序存储，链式存储 顺序存储： 数据元素存放在地址连续的存储单元里，其数据的逻辑关系和物理关系是一致的链式存储： 把数据元素放在任意的存储单元里，这组存储单元可以是连续的，也可以是不连续的 算法算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作 算法的五个基本特征： 输入：零个或多个输入 输出： 至少有一个或多个输出 有穷行： 算法在执行有限的步骤之后，自动结束而不会出现无限循环 确定性： 每一个步骤都具有确定的含义 可行性： 每一步都必须是可行的 算法设计的要求 正确性 可读性 健壮性 时间效率高和存储量低 算法效率的度量方法运行时间主要取决因素 算法采用的策略，方案 编译产生的代码质量 问题的输入规模 机器执行指令的速度 分析算法的运行时间时，重要的是把基本操作的数量和输入模式关联起来可以忽略的项 常数可以忽略 与最高次项相乘的常数可以忽略 其他次项（除去最高项） 时间复杂度定义：在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随n的变化情况并确定T(n)的数量级。算法的时间复杂度，也就是算法的时间度量，记作：T(n)=O(f(n))。它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐进时间复杂度，简称为时间复杂度。其中f(n)是问题规模n的某个函数。记法：O()，大O记法推导大O阶： 用常数1取代运行时间中的所有假发常数 在修改后的运行次数函数中，只保留最高阶项 如果最高阶存在且不是1，则去除与这个项相乘的常数 得到的最后结果就是大O阶最常用的大O阶1.O(1) 常数阶2.O(n) 线形阶3.O(n^2 ) 平方阶4.O(logn) 对数阶5.O(nlogn) nlogn阶6.O(n^3 ) 立方阶7.O(2^n ) 指数阶 空间复杂度算法的空间复杂度通过计算算法所需的存储空间实现，算法的空间复杂度的计算公式：S(n)=O(f(n))，其中，n为问题规模，f(n)为语句关于n所占存储空间的函数通常，我们都是用时间复杂度来指运行时间的需求，用空间复杂度来指空间需求 四大算法思想贪心思想核心针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。所做的下一步选择一定是当前的最优解。但是当该步选择对下一步选择有影响的话，往往贪心算法不能找到全局最优解。 典型问题 背包问题 分糖果问题 我们有 m 个糖果和 n 个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&lt;n），所以糖果只能分配给一部分孩子。 每个糖果的大小不等，这 m 个糖果的大小分别是 s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这 n 个孩子对糖果大小的需求分别是 g1，g2，g3，……，gn。 问题是，如何分配糖果，能尽可能满足最多数量的孩子？ 钱币找零问题 假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢？ 分治思想回溯思想动态规划思想图搜索深度优先搜索(DFS) 定义：深度优先搜索（Depth First Search，DFS），是最常见的图搜索方法之一。深度优先搜索沿着一条路径一直走下去，无法行进时，回退回退到刚刚访问的结点，似不撞南墙不回头，不到黄河不死心。深度优先遍历是按照深度优先搜索的方式对图进行遍历。 秘籍:后被访问的顶点，其邻接点先被访问。根据深度优先遍历秘籍，后来先服务，可以借助于栈实现。递归本身就是使用栈实现的，因此使用递归方法更方便。 算法步骤: 初始化图中所有顶点未被访问。 从图中的某个顶点v出发，访问v并标记已访问； 依次检查v的所有邻接点w，如果w未被访问，则从w出发进行深度优先遍历（递归调用，重复2—3步）。 算法图解:生成树:包含所有节点的树生成：包含所有节点 。 树：连通m=n-1，即连通无回路。 代码实现: def process(node): ''' 对节点进行一系列操作 :param node: 要操作的节点 :return: ''' passdef generate_related_nodes(node): ''' 取出当前节点的所有后继节点 :param node: 当前节点 :return: 返回所有后继节点 ''' passvisited = set()def DFS(node, visited): visited.add(node) process(node) ... for next_node in generate_related_nodes(node): if not next_node in visited: DFS(node, visited) 广度优先搜索(BFS) 定义: 依次访问每个节点的后继节点 算法图解: 代码实现: def process(node): ''' 对节点进行一系列操作 :param node: 要操作的节点 :return: ''' passdef generate_related_nodes(node): ''' 取出当前节点的全部后继节点，并判断是否被访问过。 :param node: 当前节点 :return: 返回所有没访问过的后继节点 ''' passdef BFS(graph, start, end): queue = [] visited = set() # 存储访问过的元素 queue.append(start) visited.add(start) while queue: node = queue.pop() visited.add(node) process(node) nodes = generate_related_nodes(node) queue.append(nodes) 哈希表两种实现方式 哈希函数，查找的时间复杂度为O(1)，解决哈希冲突的时候可以采取在冲突位置内嵌链表。 优点：查找速度快。 缺点：数据存放完全无序 二叉树，查找时间复杂度O(logN)。 优点: 数据存放相对有序。 缺点: 查找相对较慢 二叉树概念 二叉树：每个节点只有两个孩子，左孩子，右孩子。 完全二叉树: 每个节点都都含有两个子节点。 二叉搜索树：空树或者左子树每个节点都小于根节点，右子树每个节点都大于根节点 代码class TreeNode(): def __init__(self, val): self.val = val self.left, self.right = None, None 二分查找概念满足二分查找的条件： 序列单调递增或者递减 存在上下届 能够通过索引访问 代码# 时间复杂度O(logN)# 伪代码，数组默认递增left, right = 0, len(array) -1while left &lt;= right: mid = (left + right) // 2 if array[mid] == target: //找到目标 break or return result elif array[mid] &lt; target: left = mid + 1 else: right = mid -1 Trie 树(字典树)概念是一种树形结构，是一种哈希树的变种。典型的应用是用于统计和排序大量的字符串(但不仅限与字符串)，所以经常被搜索引擎系统用于文本词频统计。优点：最大限度地减少无谓的字符串比较，查询效率比哈希表高。特点：用边来存储字符，走过的路径代表一个单词，叶子结点代表一个单词。核心思想是：空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 代码class Trie: def __init__(self): \"\"\" Initialize your data structure here. \"\"\" self.root = dict() self.end_of_word = '#' def insert(self, word: str) -&gt; None: \"\"\" Inserts a word into the trie. \"\"\" node = self.root for char in word: node = node.setdefault(char, &#123;&#125;) node[self.end_of_word] = self.end_of_word def search(self, word: str) -&gt; bool: \"\"\" Returns if the word is in the trie. \"\"\" node = self.root for char in word: if char not in node: return False node = node[char] return self.end_of_word in node def startsWith(self, prefix: str) -&gt; bool: \"\"\" Returns if there is any word in the trie that starts with the given prefix. \"\"\" node = self.root for char in prefix: if char not in node: return False node = node[char] return True # Your Trie object will be instantiated and called as such:# obj = Trie()# obj.insert(word)# param_2 = obj.search(word)# param_3 = obj.startsWith(prefix) Bloom Filter 布隆过滤器定义一个很长的二进制向量和一个映射函数。布隆过滤器用于检索一个元素是否在一个集合中。如果判断结果不在集合中，则一定不在集合中。若判断结果在集合中，则有一定概率判断错误。优点：空间效率和查询时间都远远超过一般算法。缺点：有一定的误识别率和删除困难。与缓存有异曲同工之妙，可以作为预处理模块。 代码from bitarray import bitarrayimport mmh3class BloomFilter(set): def __init__(self,size,hash_count): #size:the num of the bitarray #hash_count:the num of hash function super(BloomFilter,self).__init__() self.bit_array = bitarray(size) self.bit_array.setall(0) #初始化为0 self.size = size self.hash_count = hash_count def __len__(self): return self.size def __iter__(self): return iter(self.bit_array) def add(self,item): for i in range(self.hash_count): index = mmh3.hash(item,i) % self.size self.bit_array[index] = 1 return self def __contains__(self,item): out = True for i in range(self.hash_count): index = mmh3.hash(item,i)%self.size if bit_array[index] == 0: out = False return out 堆定义堆是一种基本的数据结构。在这里我用数组来形容，在一个二叉堆的数组中，每一个元素都要保证大于等于另外两个特定位置的元素。同时相应的，这些元素又要大于等于另外两个相应位置的元素，整个数据结构以此类推。如果我们将整个数据结构画成树状结构，就能够清晰地看出整个结构的样子。下图代表大顶堆和小顶堆： 代码实现python 中内置包heapq实现了小顶堆import heapqnums = [4, 5, 2, 4, 5, 3, 9, 8]heapq.heapify(nums) # 将列表变为堆heapq.heappush(nums, 10) # 往堆中添加元素nums_min = heapq.heappop(nums) # 返回堆中的最小元素, 并将该元素删除nums_top_N = heapq.nlargest(3, nums) # 返回堆中最大的3个数nums_small_N = heapq.nsmallest(3, nums) # 返回堆中最小的3个数 动态规划解决类似问题：求路径 递推 (递归 + 记忆化) 状态定义：opt[n], dp[n], fib[n] 状态转移方程: opt[n] = best_of(opt[n-1], opt[n-2], …) 最优子结构.指的是解决最终问题的过程中，解决了包含的子问题。 DP VS 回溯 VS 贪心 比较 回溯（递归） — 重复计算 贪心 — 永远局部最优解 DP — 记录局部最优子结构/多种记录值","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"http://lucas0625.github.io/blog/categories/算法与数据结构/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://lucas0625.github.io/blog/tags/算法/"}]}]}